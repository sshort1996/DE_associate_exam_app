Question: 2
The data engineering team noticed that one of the job fails randomly as a result of using spot instances, what feature in
Jobs/Tasks can be used to address this issue so the job is more stable when using spot instances?
Use Databrick REST API to monitor and restart the job
A.
Use Jobs runs, active runs UI section to monitor and restart the job
B.
Add second task and add a check condition to rerun the first task if it fails
C.
Restart the job cluster, job automatically restarts
D.
Add a retry policy to the task
E.
https://www.dumps4less.com/
Answer: E
Explanation/Reference:
The answer is, Add a retry policy to the task
Tasks in Jobs support Retry Policy, which can be used to retry a failed tasks, especially when using spot instance it is
common to have failed executors or driver.
Question: 3
What is the main difference between AUTO LOADER and COPY INTO?
COPY INTO supports schema evolution.
A.
AUTO LOADER supports schema evolution.
B.
COPY INTO supports file notification when performing incremental loads.
C.
https://www.dumps4less.com/
AUTO LOADER supports directory listing when performing incremental loads.
D.
AUTO LOADER Supports file notification when performing incremental loads.
E.
Answer: E
Explanation/Reference:
Auto loader supports both directory listing and file notification but COPY INTO only supports directory listing.
Auto loader file notification will automatically set up a notification service and queue service that subscribe to file events
from the input directory in cloud object storage like Azure blob storage or S3. File notification mode is more performant
and scalable for large input directories or a high volume of files.
Auto Loader and Cloud Storage Integration
Auto Loader supports a couple of ways to ingest data incrementally
Directory listing - List Directory and maintain the state in RocksDB, supports incremental file listing
File notification - Uses a trigger+queue to store the file notification which can be later used to retrieve the file, unlike
Directory listing File notification can scale up to millions of files per day.
[OPTIONAL]
Auto Loader vs COPY INTO?
Auto Loader
Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional
setup. Auto Loader provides a new Structured Streaming source called cloudFiles. Given an input directory path on the
cloud file storage, the cloudFiles source automatically processes new files as they arrive, with the option of also
processing existing files in that directory.
When to use Auto Loader instead of the COPY INTO?
You want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover
files more efficiently than the COPY INTO SQL command and can split file processing into multiple batches.
You do not plan to load subsets of previously uploaded files. With Auto Loader, it can be more difficult to reprocess
subsets of files. However, you can use the COPY INTO SQL command to reload subsets of files while an Auto Loader
stream is simultaneously running.
Auto loader file notification will automatically set up a notification service and queue service that subscribe to file events
from the input directory in cloud object storage like Azure blob storage or S3. File notification mode is more performant
https://www.dumps4less.com/
and scalable for large input directories or a high volume of files.
Here are some additional notes on when to use COPY INTO vs Auto Loader
When to use COPY INTO
https://docs.databricks.com/delta/delta-ingest.html#copy-into-sql-command
When to use Auto Loader
https://docs.databricks.com/delta/delta-ingest.html#auto-loader

Question: 4
Why does AUTO LOADER require schema location?
Schema location is used to store user provided schema
A.
Schema location is used to identify the schema of target table
B.
AUTO LOADER does not require schema location, because its supports Schema evolution
C.
Schema location is used to store schema inferred by AUTO LOADER
D.
Schema location is used to identify the schema of target table and source table
E.
Answer: D
Explanation/Reference:
The answer is, Schema location is used to store schema inferred by AUTO LOADER
Auto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first. To avoid incurring this
inference cost at every stream start up, and to be able to provide a stable schema across stream restarts, you must set
the option cloudFiles.schemaLocation. Auto Loader creates a hidden directory _schemas at this location to track schema
changes to the input data over time
The below link contains detailed documentation on different options
Auto Loader options | Databricks on AWS

Question: 5
Which of the following statements are incorrect about the lakehouse
Support end-to-end streaming and batch workloads
A.
Supports ACID
B.
Support for diverse data types that can store both structured and unstructured
C.
Supports BI and Machine learning
D.
Storage is coupled with Compute
E.
Answer: E
https://www.dumps4less.com/
Explanation/Reference:
The answer is, Storage is coupled with Compute.
The question was asking what is the incorrect option, in Lakehouse Storage is decoupled with compute so both can scale
independently.
What Is a Lakehouse? - The Databricks Blog
Question: 6
You are designing a data model that works for both machine learning using images and Batch ETL/ELT workloads. Which
of the following features of data lakehouse can help you meet the needs of both workloads?
Data lakehouse requires very little data modeling.
A.
Data lakehouse combines compute and storage for simple governance
B.
Data lakehouse provides autoscaling for compute clusters.
C.
Data lakehouse can store unstructured data and support ACID transactions.
D.
https://www.dumps4less.com/
Data lakehouse fully exists in the cloud.
E.
Answer: D
Explanation/Reference:
The answer is A data lakehouse stores unstructured data and is ACID-compliant,
Question: 7
Which of the following locations in Databricks product architecture hosts jobs/pipelines and queries?
Data plane
A.
https://www.dumps4less.com/
Control plane
B.
Databricks Filesystem
C.
JDBC data source
D.
Databricks web application
E.
Answer: B
Explanation/Reference:
The answer is Control Plane,
Databricks operates most of its services out of a control plane and a data plane, please note serverless features like SQL
Endpoint and DLT compute use shared compute in Control pane.
Control Plane: Stored in Databricks Cloud Account
The control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands
and many other workspace configurations are stored in the control plane and encrypted at rest.
Data Plane: Stored in Customer Cloud Account
The data plane is managed by your Azure account and is where your data resides. This is also where data is processed.
You can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure
account to ingest data or for storage.
Here is the product architecture diagram highlighted where
https://www.dumps4less.com/
Question: 8
You are currently working on a notebook that will populate a reporting table for downstream process consumption, this
process needs to run on a schedule every hour. what type of cluster are you going to use to set up this job?
Since it’s just a single job and we need to run every hour, we can use an all-purpose cluster
A.
The job cluster is best suited for this purpose.
B.
Use Azure VM to read and write delta tables in Python
C.
Use delta live table pipeline to run in continuous mode
D.
Answer: B
Explanation/Reference:
The answer is, The Job cluster is best suited for this purpose.
Since you don't need to interact with the notebook during the execution especially when it's a scheduled job, job cluster
makes sense. Using an all-purpose cluster can be twice as expensive as a job cluster.
FYI,
When you run a job scheduler with option of creating a new cluster when the job is complete it terminates the cluster.
You cannot restart a job cluster.
Question: 9
Which of the following developer operations in CI/CD flow can be implemented in Databricks Repos?
Merge when code is committed
A.
Pull request and review process
B.
Trigger Databricks Repos pull API to update the latest version
C.
Create and edit code.
D.
Delete a branch
E.
Answer: C
Explanation/Reference:
See the below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workflow.
All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git
provider like Github or Azure DevOps
https://www.dumps4less.com/
Question: 10
You are currently working with the second team and both teams are looking to modify the same notebook, you noticed
that the second member is copying the notebooks to the personal folder to edit and replace the collaboration notebook,
which notebook feature do you recommend to make the process easier to collaborate.
Databricks notebooks should be copied to a local machine and setup source control locally to version the notebooks
A.
Databricks notebooks support automatic change tracking and versioning
B.
Databricks Notebooks support real-time coauthoring on a single notebook
C.
Databricks notebooks can be exported into dbc archive files and stored in data lake
D.
Databricks notebook can be exported as HTML and imported at a later time
E.
Answer: C
Explanation/Reference:
Answer is Databricks Notebooks support real-time coauthoring on a single notebook
Every change is saved, and a notebook can be changed my multiple users.
https://www.dumps4less.com/
Question: 11
You are currently working on a project that requires the use of SQL and Python in a given notebook, what would be your
approach
Create two separate notebooks, one for SQL and the second for Python
A.
A single notebook can support multiple languages, use the magic command to switch between the two.
B.
Use an All-purpose cluster for python, SQL endpoint for SQL
C.
Use job cluster to run python and SQL Endpoint for SQL
D.
Answer: B
Explanation/Reference:
The answer is, A single notebook can support multiple languages, use the magic command to switch between the two.
Use %sql and %python magic commands within the same notebook.
Question: 12
Which of the following statements are correct on how Delta Lake implements a lake house?
Delta lake uses a proprietary format to write data, optimized for cloud storage
A.
Using Apache Hadoop on cloud object storage
B.
Delta lake always stores meta data in memory vs storage
C.
https://www.dumps4less.com/
Delta lake uses open source, open format, optimized cloud storage and scalable meta data
D.
Answer: D

Question: 14
if you run the command VACUUM transactions retain 0 hours? What is the outcome of this command?
Command will be successful, but no data is removed
A.
Command will fail if you have an active transaction running
B.
Command will fail, you cannot run the command with retentionDurationcheck enabled
C.
Command will be successful, but historical data will be removed
D.
Command runs successful and compacts all of the data in the table
E.
Answer: C
Explanation/Reference:
The answer is,
Command will fail, you cannot run the command with retentionDurationcheck enabled.
VACUUM [ [db_name.]table_name | path] [RETAIN num HOURS] [DRY RUN]
Recursively vacuum directories associated with the Delta table and remove data files that are no longer in the latest
state of the transaction log for the table and are older than a retention threshold. Default is 7 Days.
The reason this check is enabled is because, DELTA is trying to prevent unintentional deletion of history, and also one
important thing to point out is with 0 hours of retention there is a possibility of data loss(see below kb)
Documentation in VACUUM https://docs.delta.io/latest/delta-utility.html
https://kb.databricks.com/delta/data-missing-vacuum-parallel-write.html
Question: 15
You noticed a colleague is manually copying the data to the backup folder prior to running an update command, incase if
the update command did not provide the expected outcome so he can use the backup copy to replace table, which Delta
Lake feature would you recommend simplifying the process?
Use time travel feature to refer old data instead of manually copying
A.
Use DEEP CLONE to clone the table prior to update to make a backup copy
B.
Use SHADOW copy of the table as preferred backup choice
C.
Cloud object storage retains previous version of the file
D.
Cloud object storage automatically backups the data
E.
Answer: A
https://www.dumps4less.com/
Explanation/Reference:
The answer is, Use time travel feature to refer old data instead of manually copying.
https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html
SELECT count(*) FROM my_table TIMESTAMP AS OF "2019-01-01"
SELECT count(*) FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)
SELECT count(*) FROM my_table TIMESTAMP AS OF "2019-01-01 01:30:00.000"
Question: 16
Which one of the following is not a Databricks lakehouse object?
Tables
A.
Views
B.
Database/Schemas
C.
Catalog
D.
Functions
E.
Stored Procedures
F.
Answer: F
Explanation/Reference:
The answer is, Stored Procedures.
Databricks lakehouse does not support stored procedures.
Question: 17
What type of table is created when you create delta table with below command?
CREATE TABLE transactions USING DELTA LOCATION "DBFS:/mnt/bronze/transactions"
Managed delta table
A.
External table
B.
Managed table
C.
Temp table
D.
Delta Lake table
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
Anytime a table is created using the LOCATION keyword it is considered an external table, below is the current syntax.
Syntax
CREATE TABLE table_name ( column column_data_type…) USING format LOCATION "dbfs:/"
format -> DELTA, JSON, CSV, PARQUET, TEXT
Question: 18
Which of the following command can be used to drop a managed delta table and the underlying files in the storage?
DROP TABLE table_name CASCADE
A.
DROP TABLE table_name
B.
Use DROP TABLE table_name command and manually delete files using command dbutils.fs.rm("/path",True)
C.
DROP TABLE table_name INCLUDE_FILES
D.
DROP TABLE table and run VACUUM command
E.
Answer: B
Explanation/Reference:
The answer is DROP TABLE table_name,
When a managed table is dropped, the table definition is dropped from metastore and everything including data,
metadata, and history are also dropped from storage.
Question: 19
Which of the following is the correct statement for a session scoped temporary view?
Temporary views are lost once the notebook is detached and re-attached
A.
Temporary views stored in memory
B.
Temporary views can be still accessed even if the notebook is detached and attached
C.
Temporary views can be still accessed even if cluster is restarted
D.
Temporary views are created in local_temp database
E.
Answer: A
https://www.dumps4less.com/
Explanation/Reference:
The answer is Temporary views are lost once the notebook is detached and attached
There are two types of temporary views that can be created, Session scoped and Global
A local/session scoped temporary view is only available with a spark session, so another notebook in the same cluster can
not access it. if a notebook is detached and reattached local temporary view is lost.
A global temporary view is available to all the notebooks in the cluster, if a cluster restarts global temporary view is lost.
Question: 20
Which of the following is correct for the global temporary view?
global temporary views cannot be accessed once the notebook is detached and attached
A.
global temporary views can be accessed across many clusters
B.
global temporary views can be still accessed even if the notebook is detached and attached
C.
global temporary views can be still accessed even if the cluster is restarted
D.
global temporary views are created in a database called temp database
E.
Answer: C
Explanation/Reference:
The answer is global temporary views can be still accessed even if the notebook is detached and attached
There are two types of temporary views that can be created Local and Global
· A local temporary view is only available with a spark session, so another notebook in the same cluster can not access it.
if a notebook is detached and reattached local temporary view is lost.
· A global temporary view is available to all the notebooks in the cluster, even if the notebook is detached and reattached
it can still be accessible but if a cluster is restarted the global temporary view is lost.
Question: 21
You are currently working on reloading customer_sales tables using the below query
INSERT OVERWRITE customer_sales
SELECT * FROM customers c
INNER JOIN sales_monthly s on s.customer_id = c.customer_id
After you ran the above command, the Marketing team quickly wanted to review the old data that was in the table. How
does INSERT OVERWRITE impact the data in the customer_sales table if you want to see the previous version of the data
prior to running the above statement?
Overwrites the data in the table, all historical versions of the data, you can not time travel to previous versions
A.
Overwrites the data in the table but preserves all historical versions of the data, you can time travel to previous
B.
versions
Overwrites the current version of the data but clears all historical versions of the data, so you can not time travel to
C.
previous versions.
https://www.dumps4less.com/
Appends the data to the current version, you can time travel to previous versions
D.
By default, overwrites the data and schema, you cannot perform time travel
E.
Answer: B
Explanation/Reference:
The answer is, INSERT OVERWRITE Overwrites the current version of the data but preserves all historical versions of the
data, you can time travel to previous versions.
INSERT OVERWRITE customer_sales
SELECT * FROM customers c
INNER JOIN sales s on s.customer_id = c.customer_id
Let's just assume that this is the second time you are running the above statement, you can still query the prior version
of the data using time travel, and any DML/DDL except DROP TABLE creates new PARQUET files so you can still access
the previous versions of data.
SQL Syntax for Time travel
SELECT * FROM table_name as of [version number]
with customer_sales example
SELECT * FROM customer_sales as of 1 -- previous version
SELECT * FROM customer_sales as of 2 -- current version
You see all historical changes on the table using DESCRIBE HISTORY table_name
Note: the main difference between INSERT OVERWRITE and CREATE OR REPLACE TABLE(CRAS) is that CRAS can modify
the schema of the table, i.e it can add new columns or change data types of existing columns. By default INSERT
OVERWRITE only overwrites the data.
INSERT OVERWRITE can also be used to update the schema when spark.databricks.delta.schema.autoMerge.enabled is
set true if this option is not enabled and if there is a schema mismatch command INSERT OVERWRITEwill fail.
Any DML/DDL operation(except DROP TABLE) on the Delta table preserves the historical version of the data.
Question: 22
Which of the following SQL statement can be used to query a table by eliminating duplicate rows from the query results?
SELECT DISTINCT * FROM table_name
A.
SELECT DISTINCT * FROM table_name HAVING COUNT(*) > 1
B.
SELECT DISTINCT_ROWS (*) FROM table_name
C.
SELECT * FROM table_name GROUP BY * HAVING COUNT(*) < 1
D.
SELECT * FROM table_name GROUP BY * HAVING COUNT(*) > 1
E.
Answer: A
Explanation/Reference:
The answer is SELECT DISTINCT * FROM table_name
https://www.dumps4less.com/
Question: 23
Which of the below SQL Statements can be used to create a SQL UDF to convert Celsius to Fahrenheit and vice versa, you
need to pass two parameters to this function one, actual temperature, and the second that identifies if its needs to be
converted to Fahrenheit or Celcius with a one-word letter F or C?
select udf_convert(60,'C') will result in 15.5
select udf_convert(10,'F') will result in 50
CREATE UDF FUNCTION udf_convert(temp DOUBLE, measure STRING) RETURNS DOUBLE RETURN CASE WHEN
A.
measure == 'F' then (temp * 9/5) + 32 ELSE (temp – 33 ) * 5/9 END
CREATE UDF FUNCTION udf_convert(temp DOUBLE, measure STRING) RETURN CASE WHEN measure == 'F' then
B.
(temp * 9/5) + 32 ELSE (temp – 33 ) * 5/9 END
CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)RETURN CASE WHEN measure == 'F' then (temp *
C.
9/5) + 32 ELSE (temp – 33 ) * 5/9 END
CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)RETURNS DOUBLERETURN CASE WHEN measure ==
D.
'F' then (temp * 9/5) + 32 ELSE (temp – 33 ) * 5/9 END
CREATE USER FUNCTION udf_convert(temp DOUBLE, measure STRING)RETURNS DOUBLERETURN CASE WHEN
E.
measure == 'F' then (temp * 9/5) + 32 ELSE (temp – 33 ) * 5/9 END
Answer: D
Explanation/Reference:
The answer is
CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)
RETURNS DOUBLE
RETURN CASE WHEN measure == ‘F’ then (temp * 9/5) + 32
ELSE (temp – 33 ) * 5/9
END
Question: 24
You are trying to parse a complex JSON object below, and calculate total sales made by all the employees, how would you
approach this in SQL
id INT, performance ARRAY, insertDate TIMESTAMP
Sample data of performance column
Sample data of performance column
[
{ "employeeId":1234
"sales" : 10000.00 },
{ "employeeId":3232
"sales" : 30000.00 }
]
WITH CTE as (SELECT EXPLODE (performance) FROM table_name)SELECT SUM (performance.sales) FROM CTE
A.
WITH CTE as (SELECT FLATTEN (performance) FROM table_name)SELECT SUM (sales) FROM CTE
B.
SELECT SUM (performance.sales) FROM employee
C.
https://www.dumps4less.com/
SELECT SUM (SLICE (performance, sales)) FROM employee
D.
SELECT SUM (ELEMENT_AT (performance, sales)) FROM employee
E.
Answer: C
Explanation/Reference:
The answer is SELECT SUM (performance.sales) FROM employee
Nested JSON can be queried using the . notation performance.sales will give you access to all the sales values in the JSON
structure.
Question: 25
Which of the following statements can be used to test the functionality of code to test number of rows in the table equal
to 10 in python?
row_count = spark.sql("select count(*) from table").collect()[0][0]
assert (row_count = 10, "Row count did not match")
A.
assert if (row_count = 10, "Row count did not match")
B.
assert row_count == 10, "Row count did not match"
C.
assert if row_count == 10, "Row count did not match"
D.
assert row_count = 10, "Row count did not match"
E.
Answer: C
Explanation/Reference:
The answer is assert row_count == 10, "Row count did not match"
Review below documentation
Assert Python
Question: 26
How do you handle failures gracefully when writing code in Pyspark, fill in the blanks to complete the below statement
_____
Spark.read.table("table_name").select("column").write.mode("append").SaveAsTable("new_table_name")
_____
print(f"query failed")
try: failure:
A.
try: catch:
B.
https://www.dumps4less.com/
try: except:
C.
try: fail:
D.
try: error:
E.
Answer: C
Explanation/Reference:
The answer is try: and except:
Question: 27
You are working on a process to query the table based on batch date, and batch date is an input parameter and expected
to change every time the program runs, what is the best way to we can parameterize the query to run without manually
changing the batch date?
Create a notebook parameter for batch date and assign the value to a python variable and use a spark data frame to
A.
filter the data based on the python variable
Create a dynamic view that can calculate the batch date automatically and use the view to query the data
B.
There is no way we can combine python variable and spark code
C.
Manually edit code every time to change the batch date
D.
Store the batch date in the spark configuration and use a spark data frame to filter the data based on the spark
E.
configuration
Answer: A
Explanation/Reference:
The answer is, Create a notebook parameter for batch date and assign the value to a python variable and use a spark
data frame to filter the data based on the python variable
Question: 28
Which of the following commands results in the successful creation of a view on top of the streaming data frame?
Spark.read.table("sales").createOrReplaceTempView("streaming_vw")
A.
Spark.readStream.table("sales").createOrReplaceTempView("streaming_vw")
B.
Spark.read.table("sales").mode("stream").createOrReplaceTempView("streaming_vw")
C.
https://www.dumps4less.com/
Spark.read.table("sales").trigger("stream").createOrReplaceTempView("streaming_vw")
D.
Spark.read.stream("sales").createOrReplaceTempView("streaming_vw")
E.
Answer: B
Explanation/Reference:
The answer is
Spark.readStream.table("sales").createOrReplaceTempView("streaming_vw")
Question: 29
Which of the following techniques structured streaming uses to create an end-to-end fault tolerance?
Checkpointing and Water marking
A.
Checkpointing and Water marking
B.
Checkpointing and idempotent sinks
C.
Write ahead logging and idempotent sinks
D.
Stream will failover to available nodes in the cluste
E.
Answer: C
Explanation/Reference:
The answer is Checkpointing and idempotent sinks
How does structured streaming achieves end to end fault tolerance:
First, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed
during each trigger interval.
Next, the streaming sinks are designed to be _idempotent_—that is, multiple writes of the same data (as identified by the
offset) do not result in duplicates being written to the sink.
Taken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure end-to-end, exactly-
once semantics under any failure condition.
Question: 30
Which of the following two options are supported in identifying the arrival of new files, and incremental data from Cloud
object storage using Auto Loader?
Directory listing, File notification
A.
Checking pointing, watermarking
B.
https://www.dumps4less.com/
Writing ahead logging, read head logging
C.
File hashing, Dynamic file lookup
D.
Checkpointing and Write ahead logging
E.
Answer: A
Explanation/Reference:
The answer is A, Directory listing, File notifications
Directory listing: Auto Loader identifies new files by listing the input directory.
File notification: Auto Loader can automatically set up a notification service and queue service that subscribe to file
events from the input directory.
Choosing between file notification and directory listing modes | Databricks on AWS
Question: 31
Which of the following data workloads will utilize a Bronze table as its destination?
A job that aggregates cleaned data to create standard summary statistics
A.
A job that queries aggregated data to publish key insights into a dashboard
B.
A job that ingests raw data from a streaming source into the Lakehouse
C.
A job that develops a feature set for a machine learning application
D.
A job that enriches data by parsing its timestamps into a human-readable format
E.
Answer: C
Explanation/Reference:
The answer is A job that ingests raw data from a streaming source into the Lakehouse.
The ingested data from the raw streaming data source like Kafka is first stored in the Bronze layer as first destination
before it is further optimized and stored in Silver.
Medallion Architecture – Databricks
Bronze Layer:
1. Raw copy of ingested data
2. Replaces traditional data lake
3. Provides efficient storage and querying of full, unprocessed history of data
4. No schema is applied at this layer
https://www.dumps4less.com/
Question: 32
Which of the following data workloads will utilize a silver table as its source?
A job that enriches data by parsing its timestamps into a human-readable format
A.
A job that queries aggregated data that already feeds into a dashboard
B.
A job that ingests raw data from a streaming source into the Lakehouse
C.
A job that aggregates cleaned data to create standard summary statistics
D.
A job that cleans data by removing malformatted records
E.
Answer: D
Explanation/Reference:
The answer is, A job that aggregates cleaned data to create standard summary statistics
Silver zone maintains the grain of the original data, in this scenario a job is taking data from the silver zone as the source
and aggregating and storing them in the gold zone.
Medallion Architecture – Databricks
Silver Layer:
1. Reduces data storage complexity, latency, and redundency
2. Optimizes ETL throughput and analytic query performance
3. Preserves grain of original data (without aggregation)
4. Eliminates duplicate records
5. production schema enforced
6. Data quality checks, quarantine corrupt data
Question: 33
Which of the following data workloads will utilize a gold table as its source?
A job that enriches data by parsing its timestamps into a human-readable format
A.
A job that queries aggregated data that already feeds into a dashboard
B.
A job that ingests raw data from a streaming source into the Lakehouse
C.
A job that aggregates cleaned data to create standard summary statistics
D.
A job that cleans data by removing malformatted records
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
The answer is, A job that queries aggregated data that already feeds into a dashboard
The gold layer is used to store aggregated data, which are typically used for dashboards and reporting.
Review the below link for more info
Medallion Architecture – Databricks
Gold Layer:
1. Powers Ml applications, reporting, dashboards, ad hoc analytics
2. Refined views of data, typically with aggregations
3. Reduces strain on production systems
4. Optimizes query performance for business-critical data
Question: 34
You are currently asked to work on building a data pipeline, you have noticed that you are currently working with data
source with a lot of data quality issues and you need to monitor data quality and enforce it as part of the data ingestion
process, which of the following tools can be used to address this problem?
AUTO LOADER
A.
DELTA LIVE TABLES
B.
JOBS and TASKS
C.
UNITY Catalog and Data Governance
D.
STRUCTURED STREAMING with MULTI HOP
E.
Answer: B
Explanation/Reference:
The answer is, DELTA LIVE TABLES
Delta live tables expectations can be used to identify and quarantine bad data, all of the data quality metrics are stored
in the event logs which can be used to later analyze and monitor.
DELTA LIVE Tables expectations
Below are three types of expectations, make sure to pay attention differences between these three.
Retain invalid records:
Use the expect operator when you want to keep records that violate the expectation. Records that violate the
expectation are added to the target dataset along with valid records:
Python
@dlt.expect("valid timestamp", "col(“timestamp”) > '2012-01-01'")
SQL
CONSTRAINT valid_timestamp EXPECT (timestamp > '2012-01-01')
Drop invalid records:
Use the expect or drop operator to prevent the processing of invalid records. Records that violate the expectation are
dropped from the target dataset:
Python
@dlt.expect_or_drop("valid_current_page", "current_page_id IS NOT NULL AND current_page_title IS NOT NULL")
SQL
CONSTRAINT valid_current_page EXPECT (current_page_id IS NOT NULL and current_page_title IS NOT NULL) ON
VIOLATION DROP ROW
Fail on invalid records:
https://www.dumps4less.com/
When invalid records are unacceptable, use the expect or fail operator to halt execution immediately when a record fails
validation. If the operation is a table update, the system atomically rolls back the transaction:
Python
@dlt.expect_or_fail("valid_count", "count > 0")
SQL
CONSTRAINT valid_count EXPECT (count > 0) ON VIOLATION FAIL UPDATE
Question: 35
What is the main difference between CREATE STREAMING LIVE TABLE vs CREATE LIVE TABLE?
CREATE STREAMING LIVE table is used in MULTI HOP Architecture
A.
CREATE LIVE TABLE is used when working with Streaming data sources and Incremental data
B.
CREATE STREAMING LIVE TABLE is used when working with Streaming data sources and Incremental data
C.
There is no difference both are the same, CREATE STRAMING LIVE will be deprecated soon
D.
CREATE LIVE TABLE is used in DELTA LIVE TABLES, CREATE STREAMING LIVE can only used in Structured Streaming
E.
applications
Answer: C
Explanation/Reference:
The answer is, CREATE STREAMING LIVE TABLE is used when working with Streaming data sources and Incremental data
Question: 36
A particular job seems to be performing slower and slower over time, the team thinks this started to happen when a
recent production change was implemented, you were asked to take look at the job history and see if we can identify
trends and root cause, where in the workspace UI can you perform this analysis?
Under jobs UI select the job you are interested, under runs we can see current active runs and last 60 days historical
A.
run
Under jobs UI select the job cluster, under spark UI select the application job logs, then you can access last 60 day
B.
historical runs
Under Workspace logs, select job logs and select the job you want to monitor to view the last 60 day historical runs
C.
Under Compute UI, select Job cluster and select the job cluster to see last 60 day historical runs
D.
Historical job runs can only be accessed by REST API
E.
Answer: A
https://www.dumps4less.com/
Explanation/Reference:
The answer is,
Under jobs UI select the job you are interested, under runs we can see current active runs and last 60 days historical run
Question: 37
What are the different ways you can schedule a job in Databricks workspace?
Continuous, Incremental
A.
On-Demand runs, File notification from Cloud object storage
B.
Cron, On Demand runs
C.
Cron, File notification from Cloud object storage
D.
Once, Continuous
E.
Answer: C
Explanation/Reference:
The answer is, Cron, On-Demand runs
Supports running job immediately or using can be scheduled using CRON syntax
Jobs in Databricks
https://www.dumps4less.com/
Question: 38
You have noticed that Databricks SQL queries are running slow, you are asked to look reason why queries are running
slow and identify steps to improve the performance, when you looked at the issue you noticed all the queries are running
in parallel and using a SQL endpoint cluster. Which of the following steps can be taken to resolve the issue?
They can turn on the Serverless feature for the SQL endpoint.
A.
They can increase the maximum bound of the SQL endpoint’s scaling range.
B.
They can increase the cluster size of the SQL endpoint.
C.
They can turn on the Auto Stop feature for the SQL endpoint.
D.
They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to “Reliability
E.
Optimized.”
Answer: B
Explanation/Reference:
The answer is, They can increase the maximum bound of the SQL endpoint’s scaling range.
SQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.
Scale-out -> to add more clusters for a SQL endpoint, change max number of clusters
If you are trying to improve the throughput, being able to run as many queries as possible then having an additional
cluster(s) will improve the performance.
Scale-up-> Increase the size of the SQL endpoint, change cluster size from x-small to small, to medium, X Large....
If you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the
cluster will improve the performance.
Question: 39
You currently working with marketing team to setup a dashboard on ad campaign analysis, since the team is not sure
how often the dashboard should refreshed they have decided to do a manual refresh on as needed basis. Which of the
following steps can be taken to reduce the overall cost of the compute?
They can turn on the Serverless feature for the SQL endpoint.
A.
They can decrease the maximum bound of the SQL endpoint’s scaling range.
B.
They can decrease the cluster size of the SQL endpoint.
C.
They can turn on the Auto Stop feature for the SQL endpoint.
D.
They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from “Reliability
E.
Optimized” to “Cost optimized”
Answer: D
https://www.dumps4less.com/
Explanation/Reference:
The answer is, They can turn on the Auto Stop feature for the SQL endpoint.
Use auto stop to automatically terminate the cluster when you are not using it.
Question: 40
You had worked with the Data analysts team to set up a SQL Endpoint point so they can easily query and analze data in
the gold layer, but once they started consuming the SQL Endpoint you noticed that during the peak hours as the number
of users increase you are seeing a degradation in the query performance, which of the following steps can be taken to
resolve the issue?
They can turn on the Serverless feature for the SQL endpoint.
A.
They can increase the maximum bound of the SQL endpoint’s scaling range.
B.
They can increase the cluster size of the SQL endpoint.
C.
They can turn on the Auto Stop feature for the SQL endpoint.
D.
They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from “Cost
E.
optimized” to “Reliability Optimized.”
Answer: B
Explanation/Reference:
the answer is,
They can increase the maximum bound of the SQL endpoint’s scaling range.
SQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.
Scale-out -> to add more clusters for a SQL endpoint, change max number of clusters
If you are trying to improve the throughput, being able to run as many queries as possible then having an additional
cluster(s) will improve the performance.
Scale-up-> Increase the size of the SQL endpoint, change cluster size from x-small to small, to medium, X Large.
If you are trying to improve the performance of a single query having additional memory, additional nodes, and cpu in
the cluster will improve the performance
Question: 41
The research team has put together a funnel analysis query to monitor the customer traffic on the e-commerce platform,
the query takes about 30 mins to run on a small SQL endpoint cluster with max scaling set to 1 cluster.
They can turn on the Serverless feature for the SQL endpoint.
A.
They can increase the maximum bound of the SQL endpoint’s scaling range anywhere from between 1 to 100 to
B.
review the performance and select the size that meets the required SLA.
They can increase the cluster size anywhere from X small to 3XL to review the performance and select the size that
C.
meets the required SLA.
https://www.dumps4less.com/
They can turn off the Auto Stop feature for the SQL endpoint to more than 30 mins.
D.
They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from “Cost
E.
optimized” to “Reliability Optimized.”
Answer: C
Explanation/Reference:
The answer is, They can increase the cluster size anywhere from small to 3XL to review the performance and select the
size that meets your SLA.
SQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.
Scale-out -> to add more clusters for a SQL endpoint, change max number of clusters
If you are trying to improve the throughput, being able to run as many queries as possible then having an additional
cluster(s) will improve the performance.
Scale-up-> Increase the size of the SQL endpoint, change cluster size from x-small to small, to medium, X Large....
If you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the
cluster will improve the performance.
Question: 42
Unity catalog simplifies managing multiple workspaces, by storing and managing permissions and ACL at _______ level
Workspace
A.
Account
B.
Storage
C.
Data pane
D.
Control pane
E.
Answer: B
Explanation/Reference:
The answer is, Account Level
The classic access control list (tables, workspace, cluster) is at the workspace level, Unity catalog is at the account level
and can manage all the workspaces in an Account.
Question: 43
Which of the following section in the UI can be used to manage permissions and grants to tables?
User Settings
A.
https://www.dumps4less.com/
Admin UI
B.
Workspace admin settings
C.
User access control lists
D.
Data Explorer
E.
Answer: E
Explanation/Reference:
The answer is Data Explorer
Data explorer
Question: 44
Which of the following is not a privilege in the Unity catalog?
SELECT
A.
MODIFY
B.
EXECUTE
C.
CREATE
D.
USAGE
E.
Answer: C
Explanation/Reference:
The Answer is EXECUTE,
Here are the list of all privileges
Privileges
SELECT: gives read access to an object.
CREATE: gives ability to create an object (for example, a table in a schema).
MODIFY: gives ability to add, delete, and modify data to or from an object.
USAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.
READ_METADATA: gives ability to view an object and its metadata.
CREATE_NAMED_FUNCTION: gives ability to create a named UDF in an existing catalog or schema.
MODIFY_CLASSPATH: gives ability to add files to the Spark class path.
ALL PRIVILEGES: gives all privileges (is translated into all the above privileges).
Privileges
https://www.dumps4less.com/
Question: 45
A team member is leaving the team and he/she is currently the owner of the few tables, instead of transfering the
ownership to a user you have decided to transfer the ownership to a group so in the future anyone in the group can
manage the permissions rather than a single individual, which of the following commands help you accomplish this?
ALTER TABLE table_name OWNER to 'group'
A.
TRANSFER OWNER table_name to 'group'
B.
GRANT OWNER table_name to 'group'
C.
ALTER OWNER ON table_name to 'group'
D.
GRANT OWNER On table_name to 'group'
E.
Answer: A
Explanation/Reference:
The answer is ALTER TABLE table_name OWNER to ‘group’
Assign owner to object
Question: 46
What is the best way to describe a data lakehouse compared to a data warehouse?
A data lakehouse provides a relational system of data management
A.
A data lakehouse captures snapshots of data for version control purposes.
B.
A data lakehouse couples storage and compute for complete control.
C.
A data lakehouse utilizes proprietary storage formats for data.
D.
A data lakehouse enables both batch and streaming analytics.
E.
Answer: E
Explanation/Reference:
Anser is A data lakehouse enables both batch and streaming analytics
A lakehouse has the following key features:
Transaction support: In an enterprise lakehouse many data pipelines will often be reading and writing data concurrently.
Support for ACID transactions ensures consistency as multiple parties concurrently read or write data, typically using
SQL.
Schema enforcement and governance: The Lakehouse should have a way to support schema enforcement and evolution,
supporting DW schema architectures such as star/snowflake-schemas. The system should be able to reason about data
https://www.dumps4less.com/
integrity, and it should have robust governance and auditing mechanisms.
BI support: Lakehouses enable using BI tools directly on the source data. This reduces staleness and improves recency,
reduces latency, and lowers the cost of having to operationalize two copies of the data in both a data lake and a
warehouse.
Storage is decoupled from compute: In practice this means storage and compute use separate clusters, thus these
systems are able to scale to many more concurrent users and larger data sizes. Some modern data warehouses also
have this property.
Openness: The storage formats they use are open and standardized, such as Parquet, and they provide an API so a
variety of tools and engines, including machine learning and Python/R libraries, can efficiently access the data directly.
Support for diverse data types ranging from unstructured to structured data: The lakehouse can be used to store, refine,
analyze, and access data types needed for many new data applications, including images, video, audio, semi-structured
data, and text.
Support for diverse workloads: including data science, machine learning, and SQL and analytics. Multiple tools might be
needed to support all these workloads but they all rely on the same data repository.
End-to-end streaming: Real-time reports are the norm in many enterprises. Support for streaming eliminates the need for
separate systems dedicated to serving real-time data applications.
Question: 47
You are designing an analytical to store structured data from your e-commerce platform and unstructured data from
website traffic and app store, how would you approach where you store this data?
Use traditional data warehouse for structured data and use data lakehouse for unstructured data.
A.
Data lakehouse can only store unstructured data but cannot enforce a schema
B.
Data lakehouse can store structured and unstructured data and can enforce schema
C.
Traditional data warehouses are good for storing structured data and enforcing schema
D.
Answer: C
Explanation/Reference:
The answer is, Data lakehouse can store structured and unstructured data and can enforce schema
What Is a Lakehouse? - The Databricks Blog
https://www.dumps4less.com/
Question: 48
You are currently working on a production job failure with a job set up in job clusters due to a data issue, what cluster do
you need to start to investigate and analyze the data?
A Job cluster can be used to analyze the problem
A.
All-purpose cluster/ interactive cluster is the recommended way to run commands and view the data.
B.
Existing job cluster can be used to investigate the issue
C.
Databricks SQL Endpoint can be used to investigate the issue
D.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
Answer is All-purpose cluster/ interactive cluster is the recommended way to run commands and view the data.
A job cluster can not provide a way for a user to interact with a notebook once the job is submitted, but an Interactive
cluster allows to you display data, view visualizations write or edit quries, which makes it a perfect fit to investigate and
analyze the data.
Question: 49
Which of the following describes how Databricks Repos can help facilitate CI/CD workflows on the Databricks Lakehouse
Platform?
Databricks Repos can facilitate the pull request, review, and approval process before merging branches
A.
Databricks Repos can merge changes from a secondary Git branch into a main Git branch
B.
Databricks Repos can be used to design, develop, and trigger Git automation pipelines
C.
Databricks Repos can store the single-source-of-truth Git repository
D.
Databricks Repos can commit or push code changes to trigger a CI/CD process
E.
Answer: E
Explanation/Reference:
Answer is Databricks Repos can commit or push code changes to trigger a CI/CD process
See below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workdlow.
All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git
provider like Github or Azure Devops.
https://www.dumps4less.com/
Question: 50
You noticed that colleague is manually copying the notebook with _bkp to store the previous versions, which of the
following feature would you recommend instead.
Databricks notebooks support change tracking and versioning
A.
Databricks notebooks should be copied to a local machine and setup source control locally to version the notebooks
B.
Databricks notebooks can be exported into dbc archive files and stored in data lake
C.
Databricks notebook can be exported as HTML and imported at a later time
D.
Answer: A
Explanation/Reference:
Answer is Databricks notebooks support automatic change tracking and versioning.
When you are editing the notebook on the right side check version history to view all the changes, every change you are
making is captured and saved.
https://www.dumps4less.com/
Question: 51
Newly joined data analyst requested read-only access to tables, assuming you are owner/admin which section of
Databricks platform is going to facilitate granting select access to the user
Admin console
A.
User settings
B.
Data explorer
C.
Azure Databricks control pane IAM
D.
Azure RBAC
E.
Answer: C
Explanation/Reference:
Anser is Data Explorer
https://docs.databricks.com/sql/user/data/index.html
Data explorer lets you easily explore and manage permissions on databases and tables. Users can view schema details,
preview sample data, and see table details and properties. Administrators can view and change owners, and admins and
data object owners can grant and revoke permissions.
To open data explorer, click Data in the sidebar.
Question: 52
How does a Delta Lake differ from a traditional data lake?
https://www.dumps4less.com/
Delta lake is Datawarehouse service on top of data lake that can provide reliability, security, and performance
A.
Delta lake is a caching layer on top of data lake that can provide reliability, security, and performance
B.
Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and
C.
performance
Delta lake is an open storage format designed to replace flat files with additional capabilities that can provide
D.
reliability, security, and performance
Delta lake is proprietary software designed by Databricks that can provide reliability, security, and performance
E.
Answer: C
Explanation/Reference:
Answer is, Delta lake is an open storage format like parquet with additional capabilities that can provide reliability,
security, and performance
Delta lake is
· Open source
· Builds up on standard data format
· Optimized for cloud object storage
· Built for scalable metadata handling
Delta lake is not
· Proprietary technology
· Storage format
· Storage medium
· Database service or data warehouse

Question: 54
Which of the following is a correct statement on how the data is organized in the storage when when managing a DELTA
table?
All of the data is broken down into one or many parquet files, log files are broken down into one or many JSON files,
A.
and each transaction creates a new data file(s) and log file.
All of the data and log are stored in a single parquet file
B.
All of the data is broken down into one or many parquet files, but the log file is stored as a single json file, and every
C.
transaction creates a new data file(s) and log file gets appended.
All of the data is broken down into one or many parquet files, log file is removed once the transaction is committed.
D.
All of the data is stored into one parquet file, log files are broken down into one or many json files.
E.
Answer: A
Explanation/Reference:
Answer is
All of the data is broken down into one or many parquet files, log files are broken down into one or many json files, and
each transaction creates a new data file(s) and log file.
here is sample layout of how DELTA table might look,
https://www.dumps4less.com/
Question: 55
You are still noticing slowness in query after performing optimize which helped you to resolve the small files problem, the
column(transactionId) you are using to filter the data has high cardinality and auto incrementing number. Which delta
optimization can you enable to filter data effectively based on this column?
Create BLOOM FLTER index on the transactionId
A.
Perform Optimize with Zorder on transactionId
B.
transactionId has high cardinality, you cannot enable any optimization.
C.
Increase the cluster size and enable delta optimization
D.
Increase the driver size and enable delta optimization
E.
Answer: B
Explanation/Reference:
The answer is, perform Optimize with Z-order by transactionid
Here is a simple explanation of how Z-order works, once the data is naturally ordered, when a flle is scanned it only
brings the data it needs into spark's memory
Based on the column min and max it knows which data files needs to be scanned.
https://www.dumps4less.com/
Question: 56
If you create a database sample_db with the statement CREATE DATABASE sample_db what will be the location of the
database in DBFS?
Default location, DBFS:/user/
A.
The location assigned to parameter warehouse.dir
B.
Storage account
C.
Statement fails “Unable to create database without location”
D.
Default Location, DBFS:/user/databases/
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
Answer is The location assigned to parameter warehouse.dir
Question: 57
Which of the following results in the creation of an external table?
CREATE TABLE transactions (id int, desc string) USING DELTA LOCATION EXTERNAL
A.
CREATE TABLE transactions (id int, desc string)
B.
CREATE EXTERNAL TABLE transactions (id int, desc string)
C.
CREATE TABLE transactions (id int, desc string) TYPE EXTERNAL
D.
CREATE TABLE transactions (id int, desc string) LOCATION '/mnt/delta/transactions'
E.
Answer: E
Explanation/Reference:
Answer is CREATE TABLE transactions (id int, desc string) USING DELTA LOCATION ‘/mnt/delta/transactions’
Anytime a table is created using Location it is considered an external table, below is the current syntax.
Syntax
CREATE TABLE table_name ( column column_data_type…) USING format LOCATION “dbfs:/”
Question: 58
When you drop an external DELTA table using DROP TABLE table_name, how does it impact metadata, data stored in the
storage?
Drops table from metastore, metadata and data in storage
A.
Drops table from metastore, data but keeps metadata in storage
B.
Drops table from metastore, metadata but keeps the data in storage
C.
Drops table from metastore, but keeps metadata and data in storage
D.
Drops table from metastore and data in storage but keeps meta data
E.
Answer: D
https://www.dumps4less.com/
Explanation/Reference:
The answer is Drops table from metastore, but keeps metadata and data in storage.
When an external table is dropped, only the table definition is dropped from metastore everything including data and
metadata remains in the storage
Question: 59
Which of the following is a true statement about the global temporary view?
A global temporary view is available only on the cluster it was created, when the cluster restarts global temporary
A.
view is automatically dropped.
A global temporary view is available on all clusters for a given workspace
B.
A global temporary view persists even if the cluster is restarted
C.
A global temporary view is stored in a user database
D.
A global temporary view is automatically dropped after 7 days
E.
Answer: A
Explanation/Reference:
Answer is, A global temporary view is available only on the cluster it was created.
Two types of temporary views can be created Local and Global
A local temporary view is only available with a spark session, so another notebook in the same cluster can not access it. if
a notebook is detached and re attached local temporary view is lost.
A global temporary view is available to all the notebooks in the cluster, if a cluster restarts global temporary view is lost.
Question: 60
You are trying to create an object by joining two tables that and it is accessible to data scientist’s team, so it does not get
dropped if the cluster restarts or if the notebook is detached. What type of object are you trying to create?
Temporary view
A.
Global Temporary view
B.
Global Temporary view with cache option
C.
External view
D.
View
E.
Answer: E
https://www.dumps4less.com/
Explanation/Reference:
Answer is View, A view can be used to join multiple tables but also persist into meta stores so others can accesses it
Question: 61
What is the best way to query external csv files located on DBFS Storage to inspect the data using SQL?
SELECT * FROM 'dbfs:/location/csv_files/' FORMAT = 'CSV'
A.
SELECT CSV. * from 'dbfs:/location/csv_files/'
B.
SELECT * FROM CSV. 'dbfs:/location/csv_files/'
C.
You can not query external files directly, us COPY INTO to load the data into a table first
D.
SELECT * FROM 'dbfs:/location/csv_files/' USING CSV
E.
Answer: C
Explanation/Reference:
Answer is, SELECT * FROM CSV. ‘dbfs:/location/csv_files/’
you can query external files stored on the storage using below syntax
SELECT * FROM format.`/Location`
format - CSV, JSON, PARQUET, TEXT
Question: 62
Direct query on external files limited options, create external tables for CSV files with header and pipe delimited CSV
files, fill in the blanks to complete the create table statement
CREATE TABLE sales (id int, unitsSold int, price FLOAT, items STRING)
________
________
LOCATION “dbfs:/mnt/sales/*.csv”
FORMAT CSVOPTIONS ( “true”,”|”)
A.
USING CSVTYPE ( “true”,”|”)
B.
USING CSVOPTIONS ( header =“true”, delimiter = ”|”)
C.
FORMAT CSVFORMAT TYPE ( header =“true”, delimiter = ”|”)
D.
FORMAT CSVTYPE ( header =“true”, delimiter = ”|”)
E.
Answer: C
https://www.dumps4less.com/
Explanation/Reference:
Answer is
USING CSV
OPTIONS ( header =“true”, delimiter = ”|”)
Here is the syntax to create an external table with additional options
CREATE TABLE table_name (col_name1 col_typ1,..)
USING data_source
OPTIONS (key=’value’, key2=vla2)
LOCATION = “/location“
Question: 64
You are working on a table called orders which contains data for 2021 and you have the second table called
orders_archive which contains data for 2020, you need to combine the data from two tables and there could be a
possibility of the same rows between both the tables, you are looking to combine the results from both the tables and
eliminate the duplicate rows, which of the following SQL statements helps you accomplish this?
SELECT * FROM orders UNION SELECT * FROM orders_archive
A.
SELECT * FROM orders INTERSECT SELECT * FROM orders_archive
B.
SELECT * FROM orders UNION ALL SELECT * FROM orders_archive
C.
SELECT * FROM orders_archive MINUS SELECT * FROM orders
D.
https://www.dumps4less.com/
SELECT distinct * FROM orders JOIN orders_archive on order.id = orders_archive.id
E.
Answer: A
Explanation/Reference:
Answer is SELECT * FROM orders UNION SELECT * FROM orders_archive
UNION and UNION ALL are set operators,
UNION combines the output from both queries but also eliminates the duplicates.
UNION ALL combines the output from both queries.
Question: 65
Which of the following python statement can be used to replace the schema name and table name in the query
statement?
table_name = "sales"schema_name = "bronze"query = f”select * from schema_name.table_name”
A.
table_name = "sales"schema_name = "bronze"query = "select * from {schema_name}.{table_name}"
B.
table_name = "sales"schema_name = "bronze"query = f"select * from { schema_name}.{table_name}"
C.
table_name = "sales"schema_name = "bronze"query = f"select * from + schema_name +"."+table_name"
D.
Answer: C
Explanation/Reference:
Answer is
table_name = “sales”
query = f”select * from {schema_name}.{table_name}”
f strings can be used to format a string. f" This is string {python variable}"
https://realpython.com/python-f-strings/
Question: 66
Which of the following SQL statements can replace python variables in Databricks SQL code, when the notebook is set in
SQL mode?
%python
table_name = "sales"
schema_name = "bronze"
%sql
SELECT * FROM ____________________
SELECT * FROM f{schema_name.table_name}
A.
SELECT * FROM {schem_name.table_name}
B.
https://www.dumps4less.com/
SELECT * FROM ${schema_name}.${table_name}
C.
SELECT * FROM schema_name.table_name
D.
Answer: C
Explanation/Reference:
The answer is, SELECT * FROM ${schema_name}.${table_name}
%python
table_name = "sales"
schema_name = "bronze"
%sql
SELECT * FROM ${schema_name}.${table_name}
${python variable} -> Python variables in Databricks SQL code
Question: 67
Your notebook accepts an input parameter called department and you are looking to control the flow of the code using
department, if the department is passed then execute code and if no department is passed skip code execution.
if department is not None: #Execute codeelse: pass
A.
if (department is not None) #Execute codeelse pass
B.
if department is not None: #Execute codeend: pass
C.
if department is not None: #Execute codethen: pass
D.
if department is None: #Execute codeelse: pass
E.
Answer: A
Explanation/Reference:
The answer is,
if department is not None:
#Execute code
else:
pass
Question: 68
Which of the following operations are not supported on a streaming dataset view ?
spark.readStream.table("sales").createOrReplaceTempView("streaming_view")
SELECT sum(unitssold) FROM streaming_view
A.
https://www.dumps4less.com/
SELECT max(unitssold) FROM streaming_view
B.
SELECT id, sum(unitssold) FROM streaming_view GROUP BY id ORDER BY id
C.
SELECT id, count(*) FROM streaming_view GROUP BY id
D.
SELECT * FROM streadming_view ORDER BY id
E.
Answer: E
Explanation/Reference:
The answer isSELECT * FROM streadming_view order by id Please Note: Sorting with Group by will work without any
issues
see below explanation for each option of the options,
Certain operations are not allowed on streaming data, please see highlighted in bold.
https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations
Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming
Datasets.
Limit and take the first N rows are not supported on streaming Datasets.
Distinct operations on streaming Datasets are not supported.
Deduplication operation is not supported after aggregation on a streaming Datasets.
Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.
Note: Sorting without aggregation function is not supported.
Here is the sample code to prove this,
Setup test stream
https://www.dumps4less.com/
Sum aggregation function has no issues on stream
Max aggregation function has no issues on stream
Group by with Order by has no issues on stream
https://www.dumps4less.com/
Group by has no issues on stream
Order by without group by fails.
Question: 69
Which of the following techniques structured streaming uses to ensure recovery of failures during stream processing?
Checkpointing and Watermarking
A.
Write ahead logging and watermarking
B.
Checkpointing and write-ahead logging
C.
Delta time travel
D.
The stream will failover to available nodes in the cluster
E.
Checkpointing and Idempotent sinks
F.
Answer: C
https://www.dumps4less.com/
Explanation/Reference:
The answer is Checkpointing and write-ahead logging.
Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during
each trigger interval.
Question: 70
What is the underlying process that makes the Auto Loader work?
Loader
A.
Delta Live Tables
B.
Structured Streaming
C.
DataFrames
D.
Live DataFames
E.
Answer: C
Explanation/Reference:
The answer is Structured Streaming
Auto Loader is built on top of Structured Streaming, Auto Loader provides a Structured Streaming source called
cloudFiles. Given an input directory path on the cloud file storage, the cloudFiles source automatically processes new files
as they arrive, with the option of also processing existing files in that directory
Question: 71
Thousands of files get uploaded to the cloud object storage for consumption, you are asked to build a process to ingest
data which of the following method can be used to ingest the data incrementally, the schema of the file is expected to
change overtime ingestion process should be able to handle these changes automatically.
AUTO APPEND
A.
AUTO LOADER
B.
COPY INTO
C.
Structured Streaming
D.
Checkpoint
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
The answer is AUTO LOADER,
Use Auto Loader instead of the COPY INTO SQL command when:
You want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover
files more efficiently than the COPY INTO SQL command and can split file processing into multiple batches.
Your data schema evolves frequently. Auto Loader provides better support for schema inference and evolution. See
Configuring schema inference and evolution in Auto Loader.
Question: 72
At the end of the inventory process, a file gets uploaded to the cloud object storage, you are asked to build a process to
ingest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to
change overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to
command to load the data, fill in the blanks for successful execution of below code.
spark.readStream
.format("cloudfiles")
.option("_______",”csv)
.option("_______", ‘dbfs:/location/checkpoint/’)
.load(data_source)
.writeStream
.option("_______",’ dbfs:/location/checkpoint/’)
.option("_______", "true")
.table(table_name))
format, checkpointlocation, schemalocation, overwrite
A.
cloudfiles.format, checkpointlocation, cloudfiles.schemalocation, overwrite
B.
cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema
C.
cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, overwrite
D.
cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, append
E.
Answer: C
Explanation/Reference:
The answer is cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema.
Here is the end to end syntax of streaming ELT, below link contains complete options Auto Loader options | Databricks on
AWS
spark.readStream
.format("cloudfiles") # Returns a stream data source, reads data as it arrives based on the trigger.
.option("cloudfiles.format","csv") # Format of the incoming files
.option("cloudfiles.schemalocation", "dbfs:/location/checkpoint/") The location to store the inferred schema and
subsequent changes
.load(data_source)
.writeStream
.option("checkpointlocation","dbfs:/location/checkpoint/") # The location of the stream’s checkpoint
.option("mergeSchema", "true") # Infer the schema across multiple files and to merge the schema of each file. Enabled
by default for Auto Loader when inferring the schema.
.table(table_name)) # target table
https://www.dumps4less.com/
Question: 73
What is the purpose of the bronze layer in a Multi-hop architecture?
Eliminates duplicate records
A.
Powers ML applications
B.
Data quality checks, corrupt data quarantined
C.
Contain aggregated data that is to be consumed into Silver
D.
Efficient storage and querying of full, unprocessed history
E.
Answer: E
Explanation/Reference:
The answer is Efficient storage and querying of full, unprocessed history.
Medallion Architecture – Databricks
Bronze Layer:
Raw copy of ingested data
Replaces traditional data lake
Provides efficient storage and querying of full, unprocessed history of data
No schema is applied at this layer
Question: 74
What is the purpose of a silver layer in Multi hop architecture?
Replaces a traditional data lake
A.
Efficient storage and querying of full, unprocessed history of data
B.
A schema is enforced, with data quality checks.
C.
Refined views with aggregated data
D.
Optimized query performance for business-critical data
E.
Answer: C
Explanation/Reference:
The answer is, A schema is enforced, with data quality checks.
https://www.dumps4less.com/
Medallion Architecture – Databricks
Silver Layer:
Reduces data storage complexity, latency, and redundency
Optimizes ETL throughput and analytic query performance
Preserves grain of original data (without aggregation)
Eliminates duplicate record
production schema enforced
Data quality checks, quarantine corrupt data
Question: 75
What is the purpose of a gold layer in Multi-hop architecture?
Optimizes ETL throughput and analytic query performance
A.
Eliminate duplicate records
B.
Preserves grain of original data, without any aggregations
C.
Data quality checks and schema enforcement
D.
Powers ML applications, reporting, dashboards and adhoc reports.
E.
Answer: E
Explanation/Reference:
The answer is Powers ML applications, reporting, dashboards and adhoc reports.
Review the below link for more info,
Medallion Architecture – Databricks
Gold Layer:
Powers Ml applications, reporting, dashboards, ad hoc analytics
Refined views of data, typically with aggregations
Reduces strain on production systems
Optimizes query performance for business-critical data
Question: 76
You are currently asked to work on building a data pipeline, you have noticed that you are currently working on a very
large scale ETL many data dependencies, which of the following tools can be used to address this problem?
AUTO LOADER
A.
JOBS and TASKS
B.
SQL Endpoints
C.
DELTA LIVE TABLES
D.
https://www.dumps4less.com/
STRUCTURED STREAMING with MULTI HOP
E.
Answer: D
Explanation/Reference:
The answer is, DELTA LIVE TABLES
DLT simplifies data dependencies by building DAG-based joins between live tables. Here is a view of how the dag looks
with data dependencies without additional meta data,
create or replace live view customers
select * from customers;
create or replace live view sales_orders_raw
select * from sales_orders;
create or replace live view sales_orders_cleaned
as
select sales.* from
live.sales_orders_raw s
join live.customers c
on c.customer_id = s.customer_id
where c.city = 'LA';
create or replace live table sales_orders_in_la
select * from sales_orders_cleaned;
Above code creates below dag
Documentation on DELTA LIVE TABLES,
https://databricks.com/product/delta-live-tables
https://databricks.com/blog/2022/04/05/announcing-generally-availability-of-databricks-delta-live-tables-dlt.html
https://www.dumps4less.com/
DELTA LIVE TABLES, addresses below challenges when building ETL processes
Complexities of large scale ETL
Hard to build and maintain dependencies
Difficult to switch between batch and stream
Data quality and governance
Difficult to monitor and enforce data quality
Impossible to trace data lineage
Difficult pipeline operations
Poor observability at granular data level
Error handling and recovery is laborious
Question: 77
How do you create a delta live tables pipeline and deploy using DLT UI?
Within the Workspace UI, click on Workflows, select Delta Live tables and create a pipeline and select the notebook
A.
with DLT code.
Under Cluster UI, select SPARK UI and select Structured Streaming and click create pipeline and select the notebook
B.
with DLT code.
There is no UI, you can only setup DELTA LIVE TABLES using Python and SQL API and select the notebook with DLT
C.
code.
Use VS Code and download DBX plugin, once the plugin is loaded you can build DLT pipelines and select the notebook
D.
with DLT code.
Within the Workspace UI, click on SQL Endpoint, select Delta Live tables and create pipelinea and select the notebook
E.
with DLT code.
Answer: A
Explanation/Reference:
The answer is, Within the Workspace UI, click on Workflows, select Delta Live tables and create a pipeline and select the
notebook with DLT code.
https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-quickstart.html
Question: 78
You are noticing job cluster is taking 6 to 8 mins to start which is delaying your job to finish on time, what steps you can
take to reduce the amount of time cluster startup time
Setup a second job ahead of first job to start the cluster, so the cluster is ready with resources when the job starts
A.
Use All purpose cluster instead to reduce cluster start up time
B.
Reduce the size of the cluster, smaller the cluster size shorter it takes to start the cluster
C.
https://www.dumps4less.com/
Use cluster pools to reduce the startup time of the jobs
D.
Use SQL endpoints to reduce the startup time
E.
Answer: D
Explanation/Reference:
The answer is, Use cluster pools to reduce the startup time of the jobs.
Cluster pools allow us to reserve VM's ahead of time, when a new job cluster is created VM are grabbed from the pool.
Note: when the VM's are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only
billed once VM is allocated to a cluster.
Here is a demo of how to setup and follow some best practices,
https://www.youtube.com/watch?v=FVtITxOabxg&ab_channel=DatabricksAcademy
Question: 79
Data engineering team has a job currently setup to run a task load data into a reporting table every day at 8: 00 AM
takes about 20 mins, Operations teams are planning to use that data to run a second job, so they access latest complete
set of data. What is the best to way to orchestrate this job setup?
Add Operation reporting task in the same job and set the Data Engineering task to depend on Operations reporting
A.
task
Setup a second job to run at 8:20 AM in the same workspace
B.
Add Operation reporting task in the same job and set the operations reporting task to depend on Data Engineering
C.
task
Use Auto Loader to run every 20 mins to read the initial table and set the trigger to once and create a second job
D.
Setup a Delta live to table based on the first table, set the job to run in continuous mode
E.
Answer: C
Explanation/Reference:
The answer is Add Operation reporting task in the same job and set the operations reporting task to depend on Data
Engineering task.
https://www.dumps4less.com/
Job View
https://www.dumps4less.com/
Question: 80
The data engineering team noticed that one of the job normally finishes in 15 mins but gets stuck randomly when reading
remote databases due to network packet drops and the job takes really long time to finish, which of the following option
can be used to fix the problem?
Use Databrick REST API to monitor long running jobs and issue a kill command
A.
Use Jobs runs, active runs UI section to monitor and kill long running job
B.
Modify the task, to include a timeout to kill the job if it runs more than 15 mins.
C.
Use Spark job time out setting in the Spark UI
D.
Use Cluster timeout setting in the Job cluster UI
E.
Answer: C
Explanation/Reference:
The answer is, Modify the task, to include time out to kill the job if it runs more than 15 mins.
https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs#timeout
Question: 81
Which of the following programming languages can be used to build a Databricks SQL dashboard?
https://www.dumps4less.com/
Python
A.
Scala
B.
SQL
C.
R
D.
All of the above
E.
Answer: C
Explanation/Reference:
The answer is SQL
Question: 82
You have noticed that Databricks SQL queries are running slow, you were asked to look reason why queries are running
slow and identify steps to improve the performance, when you looked at the code you noticed all the queries are running
sequentially and using a SQL endpoint cluster. Which of the following steps can be taken to resolve the issue?
Turn on the Serverless feature for the SQL endpoint.
A.
Increase the maximum bound of the SQL endpoint’s scaling range.
B.
Increase the cluster size of the SQL endpoint.
C.
Turn on the Auto Stop feature for the SQL endpoint.
D.
Turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to “Reliability Optimized.”
E.
Answer: C
Explanation/Reference:
The answer is increase the cluster size of the SQL Endoint,
SQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.
Scale-out -> Add more clusters, change max number of clusters
If you are trying to improve the throughput, being able to run as many queries as possible then having an additional
cluster(s) will improve the performance.
Scale-up-> Increase the size of the cluster from x-small to small, to medium, X Large....
If you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the
cluster will improve the performance.
SQL endpoint
https://www.dumps4less.com/
Question: 83
The operations team is interested in monitoring the recently launched product, team wants to set up an email alert when
the number of units sold increases by more than 10,000 units. They want to monitor this every 5 mins.
Fill in the below blanks to finish the steps we need to take
· Create ___ query that calculates total units sold
· Setup ____ with query on trigger condition Units Sold > 10,000
· Setup ____ to run every 5 mins
· Add destination ______
Python, Job, SQL Cluster, email address
A.
SQL, Alert, Refresh, email address
B.
SQL, Job, SQL Cluster, email address
C.
SQL, Job, Refresh, email address
D.
Python, Job, Refresh, email address
E.
Answer: B
Explanation/Reference:
The answer is SQL, Alert, Refresh, email address
Here the steps from Databricks documentation,
Create an alert
Follow these steps to create an alert on a single column of a query.
Do one of the following:
https://www.dumps4less.com/
Click Create in the sidebar and select Alert.
Click Alerts in the sidebar and click the + New Alert button.
Search for a target query.
To alert on multiple columns, you need to modify your query. See Alert on multiple columns
In the Trigger when field, configure the alert.
The Value column drop-down controls which field of your query result is evaluated.
The Condition drop-down controls the logical operation to be applied.
The Threshold text input is compared against the Value column using the Condition you specify.
Note
If a target query returns multiple records, Databricks SQL alerts act on the first one. As you change the Value column
setting, the current value of that field in the top row is shown beneath it.
In the When triggered, send notification field, select how many notifications are sent when your alert is triggered:
Just once: Send a notification when the alert status changes from OK to TRIGGERED.
Each time alert is evaluated: Send a notification whenever the alert status is TRIGGERED regardless of its status at the
previous evaluation.
At most every: Send a notification whenever the alert status is TRIGGERED at a specific interval. This choice lets you
avoid notification spam for alerts that trigger often.
Regardless of which notification setting you choose, you receive a notification whenever the status goes from OK to
TRIGGERED or from TRIGGERED to OK. The schedule settings affect how many notifications you will receive if the status
remains TRIGGERED from one execution to the next. For details, see Notification frequency.
In the Template drop-down, choose a template:
https://www.dumps4less.com/
Use default template: Alert notification is a message with links to the Alert configuration screen and the Query screen.
Use custom template: Alert notification includes more specific information about the alert.
A box displays, consisting of input fields for subject and body. Any static content is valid, and you can incorporate built-in
template variables:
ALERT_STATUS: The evaluated alert status (string).
ALERT_CONDITION: The alert condition operator (string).
ALERT_THRESHOLD: The alert threshold (string or number).
ALERT_NAME: The alert name (string).
ALERT_URL: The alert page URL (string).
QUERY_NAME: The associated query name (string).
QUERY_URL: The associated query page URL (string).
QUERY_RESULT_VALUE: The query result value (string or number).
QUERY_RESULT_ROWS: The query result rows (value array).
QUERY_RESULT_COLS: The query result columns (string array).
An example subject, for instance, could be: Alert "{{ALERT_NAME}}" changed status to {{ALERT_STATUS}}.
Click the Preview toggle button to preview the rendered result.
Important
The preview is useful for verifying that template variables are rendered correctly. It is not an accurate representation of
the eventual notification content, as each alert destination can display notifications differently.
Click the Save Changes button.
In Refresh, set a refresh schedule. An alert’s refresh schedule is independent of the query’s refresh schedule.
If the query is a Run as owner query, the query runs using the query owner’s credential on the alert’s refresh schedule.
If the query is a Run as viewer query, the query runs using the alert creator’s credential on the alert’s refresh schedule.
Click Create Alert.
Choose an alert destination.
Important
If you skip this step you will not be notified when the alert is triggered.
Question: 84
The marketing team is launching a new campaign to monitor the performance of the new campaign for the first two
weeks, they would like to set up a dashboard with a refresh schedule to run every 5 minutes, which of the below steps
can be taken to reduce of the cost of this refresh over time?
Reduce the size of the SQL Cluster size
A.
https://www.dumps4less.com/
Reduce the max size of auto scaling from 10 to 5
B.
Setup the dashboard refresh schedule to end in two weeks
C.
Change the spot instance policy from reliability optimized to cost optimized
D.
Always use X-small cluster
E.
Answer: C
Explanation/Reference:
The answer is Setup the dashboard refresh schedule to end in two weeks
Question: 85
Which of the following tool provides Data Access control, Access Audit, Data Lineage, and Data discovery?
DELTA LIVE Pipelines
A.
Unity Catalog
B.
Data Governance
C.
DELTA lake
D.
Lakehouse
E.
Answer: B
Explanation/Reference:
The answer is Unity Catalog
Question: 86
Data engineering team is required to share the data across with Data science team, both the teams are using different
workspaces which of the following techniques can be used to simplify sharing data across?
Data Sharing
A.
Unity Catalog
B.
DELTA lake
C.
https://www.dumps4less.com/
Use single storage location
D.
DELTA LIVE Pipelines
E.
Answer: B
Explanation/Reference:
The answer is Unity catalog.
Review product features
https://databricks.com/product/unity-catalog
Question: 87
A newly joined team member John Smith in the Marketing team who currently does not have any access to the data
requires read access to customers table, which of the following statements can be used to grant access.
GRANT SELECT, USAGE TO john.smith@marketing.com ON TABLE customers
A.
GRANT READ, USAGE TO john.smith@marketing.com ON TABLE customers
B.
GRANT SELECT, USAGE ON TABLE customers TO john.smith@marketing.com
C.
GRANT READ, USAGE ON TABLE customers TO john.smith@marketing.com
D.
GRANT READ, USAGE ON customers TO john.smith@marketing.com
E.
Answer: C
Explanation/Reference:
The answer is GRANT SELECT, USAGE ON TABLE customers TO john.smith@marketing.com
Data object privileges - Azure Databricks | Microsoft Docs
https://www.dumps4less.com/
Question: 88
Grant full privileges to new marketing user Kevin Smith to table sales
GRANT FULL PRIVILEGES TO kevin.smith@marketing.com ON TABLE sales
A.
GRANT ALL PRIVILEGES TO kevin.smith@marketing.com ON TABLE sales
B.
GRANT FULL PRIVILEGES ON TABLE sales TO kevin.smith@marketing.com
C.
GRANT ALL PRIVILEGES ON TABLE sales TO kevin.smith@marketing.com
D.
GRANT ANY PRIVILEGE ON TABLE sales TO kevin.smith@marketing.com
E.
Answer: D
Explanation/Reference:
The answer is GRANT ALL PRIVILEGE ON TABLE sales TO kevin.smith@marketing.com
GRANT < privilege > ON < securable_type > < securable_name > TO < principal >
Here are the available privileges and ALL Privileges gives full access to an object.
Privileges
SELECT: gives read access to an object.
CREATE: gives ability to create an object (for example, a table in a schema).
MODIFY: gives ability to add, delete, and modify data to or from an object.
USAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.
READ_METADATA: gives ability to view an object and its metadata.
CREATE_NAMED_FUNCTION: gives ability to create a named UDF in an existing catalog or schema.
MODIFY_CLASSPATH: gives ability to add files to the Spark class path.
ALL PRIVILEGES: gives all privileges (is translated into all the above privileges).
Question: 89
Which of the following locations in the Databricks product architecture hosts the notebooks and jobs?
Data plane
A.
Control plane
B.
Databricks Filesystem
C.
JDBC data source
D.
Databricks web application
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
The answer is Control Pane,
Databricks operates most of its services out of a control plane and a data plane, please note serverless features like SQL
Endpoint and DLT compute use shared compute in Control pane.
Control Plane: Stored in Databricks Cloud Account
The control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands
and many other workspace configurations are stored in the control plane and encrypted at rest.
Data Plane: Stored in Customer Cloud Account
The data plane is managed by your Azure account and is where your data resides. This is also where data is processed.
You can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure
account to ingest data or for storage.
Question: 90
A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp
EXPECT (timestamp > '2020-01-01') ON VIOLATION FAIL UPDATE
What is the expected behavior when a batch of data containing data that violates these constraints is processed?
Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.
A.
Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.
B.
Records that violate the expectation cause the job to fail
C.
Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the
D.
https://www.dumps4less.com/
target dataset.
Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table.
E.
Answer: C
Explanation/Reference:
The answer is Records that violate the expectation cause the job to fail
See below notes for additional notes,
There are three types of DLT expectations
Invalid records:
Use the expect operator when you want to keep records that violate the expectation. Records that violate the
expectation are added to the target dataset along with valid records:
SQL
CONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01')
Drop invalid records:
Use the expect or drop operator to prevent the processing of invalid records. Records that violate the expectation are
dropped from the target dataset:
SQL
CONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01') ON VIOLATION DROP ROW
Fail on invalid records:
When invalid records are unacceptable, use the expect or fail operator to halt execution immediately when a record fails
validation. If the operation is a table update, the system atomically rolls back the transaction
SQL
CONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01') ON VIOLATION FAIL UPDATE
Question: 91
Which of the statements are incorrect when choosing between lakehouse and Datawarehouse?
Lakehouse can have special indexes and caching which are optimized for Machine learning
A.
Lakehouse cannot serve low query latency with high reliability for BI workloads, only suitable for batch workloads.
B.
Lakehouse can be accessed through various API’s including but not limited to Python/R/SQL
C.
Traditional Data warehouses have storage and compute are coupled.
D.
Lakehouse uses standard data formats like Parquet.
E.
Answer: B
Explanation/Reference:
The answer is Lakehouse cannot serve low query latency with high reliability for BI workloads, only suitable for batch
workloads.
Lakehouse can replace traditional warehouses by leveraging storage and compute optimizations like caching to serve
them with low query latency with high reliability.
https://www.dumps4less.com/
Focus on comparisons between Spark Cache vs Delta Cache.
https://docs.databricks.com/delta/optimizations/delta-cache.html
What Is a Lakehouse? - The Databricks Blog
Question: 92
Which of the statements are correct about lakehouse?
Lakehouse only supports Machine learning workloads and Data warehouses support BI workloads
A.
Lakehouse only supports end-to-end streaming workloads and Data warehouses support Batch workloads
B.
Lakehouse does not support ACID
C.
In Lakehouse Storage and compute are coupled
D.
Lakehouse supports schema enforcement and evolution
E.
https://www.dumps4less.com/
Answer: E
Explanation/Reference:
The answer is Lakehouse supports schema enforcement and evolution,
Lakehouse using Delta lake can not only enforce a schema on write which is contrary to traditional big data systems that
can only enforce a schema on read, it also supports evolving schema over time with the ability to control the evolution.
For example below is the Dataframe writer API and it supports three modes of enforcement and evolution,
Default: Only enforcement, no changes are allowed and any schema drift/evolution will result in failure.
Merge: Flexible, supports enforcement and evolution
New columns are added
Evolves nested columns
Supports evolving data types, like Byte to Short to Integer to Bigin
How to enable:
DF.write.format("delta").option("mergeSchema", "true").saveAsTable("table_name")
or
spark.databricks.delta.schema.autoMerge = True ## Spark session
Overwrite: No enforcement
Dropping columns
Change string to integer
Rename columns
How to enable:
DF.write.format("delta").option("overwriteSchema", "True").saveAsTable("table_name")
What Is a Lakehouse? - The Databricks Blog
https://www.dumps4less.com/
Question: 93
Which of the following are stored in the control pane of Databricks Architecture?
Job Clusters
A.
All Purpose Clusters
B.
Databricks Filesystem
C.
Databricks Web Application
D.
Delta tables
E.
Answer: D
https://www.dumps4less.com/
Explanation/Reference:
The answer is Databricks Web Application
Azure Databricks architecture overview - Azure Databricks | Microsoft Docs
Databricks operates most of its services out of a control plane and a data plane, please note serverless features like SQL
Endpoint and DLT compute use shared compute in Control pane.
Control Plane: Stored in Databricks Cloud Account
The control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands
and many other workspace configurations are stored in the control plane and encrypted at rest.
Data Plane: Stored in Customer Cloud Account
The data plane is managed by your Azure account and is where your data resides. This is also where data is processed.
You can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure
account to ingest data or for storage.
Question: 94
You have written a notebook to generate a summary data set for reporting, Notebook was scheduled using the job
cluster, but you realized it takes 8 minutes to start the cluster, what feature can be to start the cluster in a timely fashion
Setup an additional job to run ahead of the actual job so the cluster is running second job starts
A.
Use the Databricks cluster pools feature to reduce the startup time
B.
Use Databricks Premium edition instead of Databricks standard edition
C.
Pin the cluster in the cluster UI page so it is always available to the jobs
D.
https://www.dumps4less.com/
Disable auto termination so the cluster is always running
E.
Answer: B
Explanation/Reference:
Cluster pools allow us to reserve VM's ahead of time, when a new job cluster is created VM are grabbed from the pool.
Note: when the VM's are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only
billed once VM is allocated to a cluster.
Here is a demo of how to setup a pool and follow some best practices,
Question: 95
Which of the following developer operations in the CI/CD can only be implemented through a GIT provider when using
Databricks Repos.
Trigger Databricks Repos pull API to update the latest version
A.
Commit and push code
B.
Create and edit code
C.
Create a new branch
D.
Pull request and review process
E.
Answer: E
https://www.dumps4less.com/
Explanation/Reference:
The answer is Pull request and review process, please note: the question is asking for steps that are being implemented
in GIT provider not Databricks Repos.
See below diagram to understand the role of Databricks Repos and Git provider plays when building a CI/CD workdlow.
All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git
provider like Github or Azure Devops.
Question: 96
You have noticed the Data scientist team is using the notebook versioning feature with git integration, you have
recommended them to switch to using Databricks Repos, which of the below reasons could be the reason the why the
team needs to switch to Databricks Repos.
Databricks Repos allows multiple users to make changes
A.
Databricks Repos allows merge and conflict resolution
B.
Databricks Repos has a built-in version control system
C.
Databricks Repos automatically saves changes
D.
Databricks Repos allow you to add comments and select the changes you want to commit.
E.
Answer: E
Explanation/Reference:
https://www.dumps4less.com/
The answer is Databricks Repos allow you to add comments and select the changes you want to commit.
Question: 97
Data science team members are using a single cluster to perform data analysis, although cluster size was chosen to
handle multiple users and auto-scaling was enabled, the team realized queries are still running slow, what would be the
suggested fix for this?
Setup multiple clusters so each team member has their own cluster
A.
Disable the auto-scaling feature
B.
Use High concurrency mode instead of the standard mode
C.
Increase the size of the driver node
D.
Answer: C
Explanation/Reference:
The answer is Use High concurrency mode instead of the standard mode,
https://docs.databricks.com/clusters/cluster-config-best-practices.html#cluster-mode
High Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs. Administrators
usually create High Concurrency clusters. Databricks recommends enabling autoscaling for High Concurrency clusters.
Question: 98
Which of the following SQL commands are used to append rows to an existing delta table?
APPEND INTO DELTA table_name
A.
APPEND INTO table_name
B.
COPY DELTA INTO table_name
C.
INSERT INTO table_name
D.
UPDATE table_name
E.
Answer: D
Explanation/Reference:
The answer is INSERT INTO table_name
Insert adds rows to existing table
https://www.dumps4less.com/
Question: 99
How are Delt tables stored?
A Directory where parquet data files are stored, a sub directory _delta_log where meta data, and the transaction log is
A.
stored as JSON files
A Directory where parquet data files are stored, all of the meta data is stored in memory
B.
A Directory where parquet data files are stored in Data plane, a sub directory _delta_log where meta data, history and
C.
log is stored in control pane.
A Directory where parquet data files are stored, all of the metadata is stored in parquet files
D.
Data is stored in Data plane and Metadata and delta log are stored in control pane
E.
Answer: A
Explanation/Reference:
The answer is A Directory where parquet data files are stored, a sub directory _delta_log where meta data, and the
transaction log is stored as JSON files.
Question: 100
While investigating a data issue in a Delta table, you wanted to review when and who updated the table, what is the best
way to review this data?
Review event logs in the Workspace
A.
https://www.dumps4less.com/
Run SQL SHOW HISTORY table_name
B.
Check Databricks SQL Audit logs
C.
Run SQL command DESCRIBE HISTORY table_name
D.
Review workspace audit logs
E.
Answer: D
Explanation/Reference:
The answer is Run SQL command DESCRIBE HISTORY table_name.
here is the sample data of how DESCRIBE HISTORY table_name looks
Question: 101
While investigating a performance issue, you realized that you have too many small files for a given table, which
command are you going to run to fix this issue
COMPACT table_name
A.
VACUUM table_name
B.
MERGE table_name
C.
SHRINK table_name
D.
OPTIMIZE table_name
E.
Answer: E
Explanation/Reference:
The answer is OPTIMIZE table_name,
Optimize compacts small parquet files into a bigger file, by default the size of the files are determined based on the table
size at the time of OPTIMIZE, the file size can also be set manually or adjusted based on the workload.
https://docs.databricks.com/delta/optimizations/file-mgmt.html
Tune file size based on Table size
To minimize the need for manual tuning, Databricks automatically tunes the file size of Delta tables based on the size of
the table. Databricks will use smaller file sizes for smaller tables and larger file sizes for larger tables so that the number
of files in the table does not grow too large.
https://www.dumps4less.com/
Question: 102
Create a sales database using the DBFS location 'dbfs:/mnt/delta/databases/sales.db/'
CREATE DATABASE sales FORMAT DELTA LOCATION 'dbfs:/mnt/delta/databases/sales.db/'’
A.
CREATE DATABASE sales USING LOCATION 'dbfs:/mnt/delta/databases/sales.db/'
B.
CREATE DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'
C.
The sales database can only be created in Delta lake
D.
CREATE DELTA DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'
E.
Answer: C
Explanation/Reference:
The answer is
CREATE DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'
Note: with the introduction of the Unity catalog and three-layer namespace usage of SCHEMA and DATABASE is
interchangeable
https://www.dumps4less.com/
Question: 103
What is the type of table created when you issue SQL DDL command CREATE TABLE sales (id int, units int)
Query fails due to missing location
A.
Query fails due to missing format
B.
Managed Delta table
C.
External Table
D.
Managed Parquet table
E.
Answer: C
Explanation/Reference:
Answer is Managed Delta table
Anytime a table is created without the Location keyword it is considered a managed table, by default all managed tables
DELTA tables
Syntax
CREATE TABLE table_name ( column column_data_type…)
Question: 104
How to determine if a table is a managed table vs external table?
Run IS_MANAGED(‘table_name’) function
A.
All external tables are stored in data lake, managed tables are stored in DELTA lake
B.
All managed tables are stored in unity catalog
C.
Run SQL command DESCRIBE EXTENDED table_name and check type
D.
A. Run SQL command SHOW TABLES to see the type of the table
E.
Answer: D
Explanation/Reference:
The answer is Run SQL command DESCRIBE EXTENDED table_name and check type
Example of External table
https://www.dumps4less.com/
Example of managed table
Question: 105
Which of the below SQL commands creates a session scoped temporary view?
CREATE OR REPLACE TEMPORARY VIEW view_nameAS SELECT * FROM table_name
A.
CREATE OR REPLACE LOCAL TEMPORARY VIEW view_nameAS SELECT * FROM table_name
B.
CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_nameAS SELECT * FROM table_name
C.
CREATE OR REPLACE VIEW view_nameAS SELECT * FROM table_name
D.
CREATE OR REPLACE LOCAL VIEW view_nameAS SELECT * FROM table_name
E.
Answer: A
Explanation/Reference:
The answer is
https://www.dumps4less.com/
CREATE OR REPLACE TEMPORARY VIEW view_name
AS SELECT * FROM table_name
The default temporary view is session scoped, as soon as the session ends or if a notebook is detached session scoped
temporary view is dropped.
Question: 106
Drop the customers database and associated tables and data, all of the tables inside the database are managed tables.
DROP DATABASE customers FORCE
A.
DROP DATABASE customers CASCADE
B.
DROP DATABASE customers INCLUDE
C.
All the tables must be dropped first before dropping database
D.
DROP DELTA DATABSE customers
E.
Answer: B
Explanation/Reference:
The answer is DROP DATABASE customers CASCADE
Drop database with cascade option drops all the tables, since all of the tables inside the database are managed tables we
do not need to perform any additional steps to clean the data in the storage.
Question: 107
Define an external SQL table by connecting to a local instance of an SQLite database using JDBC
CREATE TABLE users_jdbcUSING SQLITEOPTIONS ( url = "jdbc:/sqmple_db", dbtable = "users")
A.
CREATE TABLE users_jdbcUSING SQLURL = {server:"jdbc:/sqmple_db",dbtable: “users”}
B.
CREATE TABLE users_jdbcUSING SQLOPTIONS ( url = "jdbc:sqlite:/sqmple_db", dbtable = "users")
C.
CREATE TABLE users_jdbcUSING org.apache.spark.sql.jdbc.sqliteOPTIONS ( url = "jdbc:/sqmple_db", dbtable =
D.
"users")
CREATE TABLE users_jdbcUSING org.apache.spark.sql.jdbcOPTIONS ( url = "jdbc:sqlite:/sqmple_db", dbtable =
E.
"users")
Answer: E
https://www.dumps4less.com/
Explanation/Reference:
The answer is,
CREATE TABLE users_jdbc
USING org.apache.spark.sql.jdbc
OPTIONS (
url = "jdbc:sqlite:/sqmple_db",
dbtable = "users"
)
Databricks runtime currently supports connecting to a few flavors of SQL Database including SQL Server, My SQL, SQL
Lite and Snowflake using JDBC.
CREATE TABLE
USING org.apache.spark.sql.jdbc or JDBC
OPTIONS (
url = "jdbc:://:",
dbtable " = .atable",
user = "",
password = ""
)
More detailed documentation
SQL databases using JDBC - Azure Databricks | Microsoft Docs
Question: 108
When defining external tables using formats CSV, JSON, TEXT, BINARY any query on the external tables caches the data
and location for performance reasons, so within a given spark session any new files that may have arrived will not be
available after the initial query. How can we address this limitation?
UNCACHE TABLE table_name
A.
CACHE TABLE table_name
B.
REFRESH TABLE table_name
C.
BROADCAST TABLE table_name
D.
CLEAR CACH table_name
E.
Answer: C
Explanation/Reference:
The answer is REFRESH TABLE table_name
REFRESH TABLE table_name will force Spark to refresh the availability of external files and any changes.
Question: 109
Which of the following table constraints are supported in Delta tables?
https://www.dumps4less.com/
Primary key, foreign key, Not Null, Check Constraints
A.
Primary key, Not Null, Check Constraints
B.
Default, Not Null, Check Constraints
C.
Not Null, Check Constraints
D.
Unique, Not Null, Check Constraints
E.
Answer: D
Explanation/Reference:
The answer is Not Null, Check Constraints
https://docs.microsoft.com/en-us/azure/databricks/delta/delta-constraints
CREATE TABLE events( id LONG,
date STRING,
location STRING,
description STRING
) USING DELTA
ALTER TABLE events CHANGE COLUMN id SET NOT NULL;
ALTER TABLE events ADD CONSTRAINT dateWithinRange CHECK (date > '1900-01-01');
Question: 110
The data engineering team is looking to add a new column to the table, but the QA team would like to test the change
which of the below options allow you to quickly copy the table from Prod to QA environment, modify and run the tests
DEEP CLONE
A.
SHADOW CLONE
B.
ZERO COPY CLONE
C.
SHALLOW CLONE
D.
METADATA CLONE
E.
Answer: D
Explanation/Reference:
The answer is SHALLOW CLONE
SHALLOW CLONE If you wish to create a copy of a table quickly to test out applying changes without the risk of modifying
the current table, SHALLOW CLONE can be a good option. Shallow clones just copy the Delta transaction logs, meaning
that the data doesn't move so it can be very quick
DEEP CLONE fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing
this command again can sync changes from the source to the target location. It copies all of the data and transaction logs
this can take a long time based on the size of the table.

Question: 112
You are asked to write a python function that can read data from a delta table and return the DataFrame, which of the
following is correct?
Python function cannot return a DataFrame
A.
Write SQL UDF to return a DataFrame
B.
Write SQL UDF that can return tabular data
C.
Python function will result in out of memory error due to data volume
D.
Python function can return a DataFrame
E.
Answer: E
Explanation/Reference:
The answer is Python function can return a DataFrame
The function would something like this,
get_source_dataframe(tablename):
df = spark.read.table(tablename)
return df
df = get_source_dataframe('test_table'
since there is no action spark returns a Dataframe and assigns to df python variable
Question: 114
Which of the following SQL statements can replace a python variable, when the notebook is set in SQL mode
table_name = "sales"
schema_name = "bronze"
spark.sql(f"SELECT * FROM f{schema_name.table_name}")
A.
spark.sql(f"SELECT * FROM {schem_name.table_name}")
B.
spark.sql(f"SELECT * FROM ${schema_name}.${table_name}")
C.
spark.sql(f"SELECT * FROM {schema_name}.{table_name}")
D.
spark.sql("SELECT * FROM schema_name.table_name")
E.
Answer: D
Explanation/Reference:
The answer is spark.sql(f"SELECT * FROM {schema_name}.{table_name}")
Question: 115
When writing streaming data, Spark’s structured stream supports the below write modes
Append, Delta, Complete
A.
Delta, Complete, Continuous
B.
Append, Complete, Update
C.
Complete, Incremental, Update
D.
Append, overwrite, Continuous
E.
Answer: C
https://www.dumps4less.com/
Explanation/Reference:
The answer is Append, Complete, Update
Append mode (default) - This is the default mode, where only the new rows added to the Result Table since the last
trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is
never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant
sink). For example, queries with only select, where, map, flatMap, filter, join, etc. will support Append mode.
Complete mode - The whole Result Table will be outputted to the sink after every trigger. This is supported for
aggregation queries.
Update mode - (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will
be outputted to the sink. More information to be added in future releases.
Question: 116
When using the complete mode to write stream data, how does it impact the target table?
Entire stream waits for complete data to write
A.
Stream must complete to write the data
B.
Target table cannot be updated while stream is pending
C.
Target table is overwritten for each batch
D.
Delta commits transaction once the stream is stopped
E.
Answer: D
Explanation/Reference:
The answer is Target table is overwritten for each batch
Complete mode - The whole Result Table will be outputted to the sink after every trigger. This is supported for
aggregation queries
Question: 117
At the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to
ingest data which of the following method can be used to ingest the data incrementally, the schema of the file is
expected to change overtime ingestion process should be able to handle these changes automatically. Below is the auto
loader command to load the data, fill in the blanks for successful execution of the below code.
spark.readStream
.format("cloudfiles")
.option("cloudfiles.format",”csv)
.option("_______", ‘dbfs:/location/checkpoint/’)
.load(data_source)
.writeStream
.option("_______",’ dbfs:/location/checkpoint/’)
.option("mergeSchema", "true")
.table(table_name))
https://www.dumps4less.com/
checkpointlocation, schemalocation
A.
checkpointlocation, cloudfiles.schemalocation
B.
schemalocation, checkpointlocation
C.
cloudfiles.schemalocation, checkpointlocation
D.
cloudfiles.schemalocation, cloudfiles.checkpointlocation
E.
Answer: D
Explanation/Reference:
The answer is cloudfiles.schemalocation, checkpointlocation
When reading the data cloudfiles.schemalocation is used to store the inferred schema of the incoming data.
When writing a stream to recover from failures checkpointlocation is used to store the offset of the byte that was most
recently processed.
Question: 118
When working with AUTO LOADER you noticed that most of the columns that were inferred as part of loading are string
data types including columns that were supposed to be integers, how can we fix this?
Provide the schema of the source table in the cloudfiles.schemalocation
A.
Provide the schema of the target table in the cloudfiles.schemalocation
B.
Provide schema hints
C.
Update the checkpoint location
D.
Correct the incoming data by explicitly casting the data types
E.
Answer: C
Explanation/Reference:
The answer is, Provide schema hints.
spark.readStream \
.format("cloudFiles") \
.option("cloudFiles.format", "csv") \
.option("header", "true") \
.option("cloudFiles.schemaLocation", schema_location) \
.option("cloudFiles.schemaHints", "id int, description string")
.load(raw_data_location)
.writeStream \
.option("checkpointLocation", checkpoint_location) \
.start(target_delta_table_location)
https://www.dumps4less.com/
.option("cloudFiles.schemaHints", "id int, description string")
# Here we are providing a hint that id column is int and the description is a string
When cloudfiles.schemalocation is used to store the output of the schema inference during the load process, with
schema hints you can enforce data types for known columns ahead of time.
Question: 119
You have configured AUTO LOADER to process incoming IOT data from cloud object storage every 15 mins, recently a
change was made to the notebook code to update the processing logic but the team later realized that the notebook was
failing for the last 24 hours, what steps team needs to take to reprocess the data that was not loaded after the notebook
was corrected?
Move the files that were not processed to another location and manually copy the files into the ingestion path to
A.
reprocess them
Enable back_fill = TRUE to reprocess the data
B.
Delete the checkpoint folder and run the autoloader again
C.
Autoloader automatically re-processes data that was not loaded
D.
Manually re-load the data
E.
Answer: D
Explanation/Reference:
The answer is,
Autoloader automatically re-processes data that was not loaded using the checkpoint.
Question: 120
Which of the following Structured Streaming queries is performing a hop from a bronze table to a Silver table?
(spark.table("sales").groupBy("store").agg(sum("sales")).writeStream.option("checkpointLocation",checkpointPath)
A.
.outputMode("complete").table("aggregatedSales"))
(spark.table("sales").agg(sum("sales"),sum("units")).writeStream.option("checkpointLocation",checkpointPath)
B.
.outputMode("complete").table("aggregatedSales"))
(spark.table("sales").withColumn("avgPrice", col("sales") / col("units")).writeStream.option("checkpointLocation",
C.
checkpointPath).outputMode("append") .table("cleanedSales"))
(spark.readStream.load(rawSalesLocation).writeStream .option("checkpointLocation",
D.
checkpointPath) .outputMode("append") .table("uncleanedSales") )
(spark.read.load(rawSalesLocation) .writeStream .option("checkpointLocation",
E.
checkpointPath) .outputMode("append") .table("uncleanedSales") )
https://www.dumps4less.com/
Answer: C
Explanation/Reference:
The question is asking to identify a structured streaming command that is moving data from bronze to silver.
The answer is
(spark.table("sales")
.withColumn("avgPrice", col("sales") / col("units"))
.writeStream
.option("checkpointLocation", checkpointPath)
.outputMode("append")
.table("cleanedSales"))
We are preserving the grain of incoming data and enriching the data by adding avg price, the other options listed use
aggregations which are mostly performed on top of the silver to move data to Gold.
Question: 121
Which of the following Structured Streaming queries successfully performs a hop from a Silver to Gold table?
(spark.table("sales") .groupBy("store") .agg(sum("sales")) .writeStream .option("checkpointLocation",
A.
checkpointPath) .outputMode("complete") .table("aggregatedSales") )
(spark.table("sales") .writeStream .option("checkpointLocation",
B.
checkpointPath) .outputMode("complete") .table("sales") )
(spark.table("sales") .withColumn("avgPrice", col("sales") / col("units")) .writeStream .option("checkpointLocation",
C.
checkpointPath) .outputMode("append") .table("cleanedSales") )
(spark.readStream.load(rawSalesLocation) .writeStream .option("checkpointLocation",
D.
checkpointPath) .outputMode("append") .table("uncleanedSales") )
(spark.read.load(rawSalesLocation) .writeStream .option("checkpointLocation", checkpointPath)
E.
.outputMode("append") .table("uncleanedSales") )
Answer: A
Explanation/Reference:
The answer is
(spark.table("sales")
.groupBy("store")
.agg(sum("sales"))
.writeStream
.option("checkpointLocation", checkpointPath)
.outputMode("complete")
.table("aggregatedSales") )
The gold layer is normally used to store aggregated data
Review the below link for more info,
Medallion Architecture – Databricks
https://www.dumps4less.com/
Gold Layer
1. Powers Ml applications, reporting, dashboards, ad hoc analytics
2. Refined views of data, typically with aggregations
3. Reduces strain on production systems
4. Optimizes query performance for business-critical data
Question: 122
Which of the following Auto loader structured streaming commands successfully performs a hop from the landing area
into Bronze?
spark\.readStream\.format("csv")\.option("cloudFiles.schemaLocation",
A.
checkpoint_directory)\.load("landing")\.writeStream.option("checkpointLocation", checkpoint_directory)\.table(raw)
spark\.readStream\.format("cloudFiles")\.option("cloudFiles.format","csv")\.option("cloudFiles.schemaLocation",
B.
checkpoint_directory)\.load("landing")\.writeStream.option("checkpointLocation", checkpoint_directory)\.table(raw)
spark\.read\.format("cloudFiles")\.option("cloudFiles.format",”csv”)\.option("cloudFiles.schemaLocation",
C.
checkpoint_directory)\.load("landing")\.writeStream.option("checkpointLocation", checkpoint_directory)\.table(raw)
spark\.readStream\.load(rawSalesLocation)\.writeStream \.option("checkpointLocation",
D.
checkpointPath).outputMode("append")\.table("uncleanedSales")
spark\.read\.load(rawSalesLocation) \.writeStream\.option("checkpointLocation", checkpointPath)
E.
\.outputMode("append")\.table("uncleanedSales")
Answer: B
Explanation/Reference:
The answer is
spark\
.readStream\
.format("cloudFiles") \# use Auto loader
.option("cloudFiles.format","csv") \ # csv format files
.option("cloudFiles.schemaLocation", checkpoint_directory)\
.load('landing')\
.writeStream.option("checkpointLocation", checkpoint_directory)\
.table(raw
Note: if you chose the below option which is incorrect because it does not have readStream
spark.read.format("cloudFiles")
.option("cloudFiles.format",”csv”)
...
..
..
Question: 123
What are two different modes of DELTA LIVE TABLE Pipelines
https://www.dumps4less.com/
Triggered, Incremental
A.
Once, Continuous
B.
Triggered, Continuous
C.
Once, Incremental
D.
Continuous, Incremental
E.
Answer: C
Explanation/Reference:
The answer is Triggered, Continuous
https://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-concepts#--
continuous-and-triggered-pipelines
Triggered pipelines update each table with whatever data is currently available and then stop the cluster running the
pipeline. Delta Live Tables automatically analyzes the dependencies between your tables and starts by computing those
that read from external sources. Tables within the pipeline are updated after their dependent data sources have been
updated.
Continuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run
until manually stopped. Continuous pipelines require an always-running cluster but ensure that downstream consumers
have the most up-to-date data.
Question: 124
Your team member is trying to set up a delta pipeline and build a second gold table to the same pipeline with aggregated
metrics based on an existing Delta Live table called sales_orders_cleaned but he is facing a problem in starting the
pipeline, the pipeline is failing to state it cannot find the table sales_orders_cleaned, you are asked to identify and fix the
problem.
CREATE LIVE TABLE sales_order_in_chicago
AS
SELECT order_date, city, sum(price) as sales,
FROM sales_orders_cleaned
WHERE city = 'Chicago')
GROUP BY order_date, city
Use STREAMING LIVE instead of LIVE table
A.
Delta live table can be used in a group by clause
B.
Delta live tables pipeline can only have one table
C.
Sales_orders_cleaned table is missing schema name LIVE
D.
The pipeline needs to be deployed so the first table is created before we add a second table
E.
Answer: D
https://www.dumps4less.com/
Explanation/Reference:
The answer is, Sales_orders_cleaned table is missing schema name LIVE
Every Delta live table should have schema LIVE
Here is the correct syntax,
CREATE LIVE TABLE sales_order_in_chicago
AS
SELECT order_date, city, sum(price) as sales,
FROM LIVE.sales_orders_cleaned
WHERE city = 'Chicago')
GROUP BY order_date, city
Question: 125
Which of the following type of tasks cannot setup through a job?
Notebook
A.
DELTA LIVE PIPELINE
B.
Spark Submit
C.
Python
D.
Databricks SQL Dashboard refresh
E.
Answer: E
Question: 126
Which of the following approaches can the data engineer use to obtain a version-controllable configuration of the Job’s
schedule and configuration?
They can link the Job to notebooks that are a part of a Databricks Repo.
A.
They can submit the Job once on a Job cluster.
B.
They can download the JSON equivalent of the job from the Job’s page.
C.
They can submit the Job once on an all-purpose cluster.
D.
They can download the XML description of the Job from the Job’s page
E.
Answer: C
https://www.dumps4less.com/
Question: 127
what steps need to be taken to set up a DELTA LIVE PIPELINE as a job?
DELTA LIVE TABLES do not support job cluster
A.
Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the notebook
B.
Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the pipeline
C.
JSON file
Use Pipeline creation UI, select a new pipeline and job cluster
D.
Answer: B
Explanation/Reference:
The answer is,
Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the notebook.
Create a pipeline
To create a new pipeline using the Delta Live Tables notebook:
Click Workflows in the sidebar, click the Delta Live Tables tab, and click Create Pipeline.
Give the pipeline a name and click to select a notebook.
Optionally enter a storage location for output data from the pipeline. The system uses a default location if you leave
Storage Location empty.
Select Triggered for Pipeline Mode.
Click Create.
The system displays the Pipeline Details page after you click Create. You can also access your pipeline by clicking the
pipeline name in the Delta Live Tables tab.
Question: 128
Data engineering team has provided 10 queries and asked Data Analyst team to build a dashboard and refresh the data
every day at 8 AM, identify the best approach to set up data refresh for this dashaboard?
Each query requires a separate task and setup 10 tasks under a single job to run at 8 AM to refresh the dashboard
A.
The entire dashboard with 10 queries can be refreshed at once, single schedule needs to be set up to refresh at 8 AM.
B.
Setup JOB with linear dependency to all load all 10 queries into a table so the dashboard can be refreshed at once.
C.
A dashboard can only refresh one query at a time, 10 schedules to set up the refresh.
D.
Use Incremental refresh to run at 8 AM every day.
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
The answer is,
The entire dashboard with 10 queries can be refreshed at once, single schedule needs to be set up to refresh at 8 AM.
Automatically refresh a dashboard
A dashboard’s owner and users with the Can Edit permission can configure a dashboard to automatically refresh on a
schedule. To automatically refresh a dashboard:
Click the Schedule button at the top right of the dashboard. The scheduling dialog appears.
In the Refresh every drop-down, select a period.
In the SQL Warehouse drop-down, optionally select a SQL warehouse to use for all the queries. If you don’t select a
warehouse, the queries execute on the last used SQL warehouse.
Next to Subscribers, optionally enter a list of email addresses to notify when the dashboard is automatically updated.
Each email address you enter must be associated with a Azure Databricks account or configured as an alert destination.
Click Save. The Schedule button label changes to Scheduled.
Question: 129
The data engineering team is using a SQL query to review data completeness every day to monitor the ETL job, and
query output is being used in multiple dashboards which of the following approaches can be used to set up a schedule
and automate this process?
They can schedule the query to run every day from the Jobs UI.
A.
They can schedule the query to refresh every day from the query’s page in Databricks SQL
B.
They can schedule the query to run every 12 hours from the Jobs UI.
C.
They can schedule the query to refresh every day from the SQL endpoint’s page in Databricks SQL.
D.
They can schedule the query to refresh every 12 hours from the SQL endpoint’s page in Databricks SQL
E.
Answer: B
Explanation/Reference:
The answer is They can schedule the query to refresh every 12 hours from the SQL endpoint’s page in Databricks SQL,
The query pane view in Databricks SQL workspace provides the ability to add or edit and schedule individual queries to
run.
You can use scheduled query executions to keep your dashboards updated or to enable routine alerts. By default, your
queries do not have a schedule.
Note
If your query is used by an alert, the alert runs on its own refresh schedule and does not use the query schedule.
To set the schedule:
Click the query info tab.
Click the link to the right of Refresh Schedule to open a picker with schedule intervals.
Set the schedule.
The picker scrolls and allows you to choose:
An interval: 1-30 minutes, 1-12 hours, 1 or 30 days, 1 or 2 weeks
A time. The time selector displays in the picker only when the interval is greater than 1 day and the day selection is
greater than 1 week. When you schedule a specific time, Databricks SQL takes input in your computer’s timezone and
converts it to UTC. If you want a query to run at a certain time in UTC, you must adjust the picker by your local offset. For
example, if you want a query to execute at 00:00 UTC each day, but your current timezone is PDT (UTC-7), you should
select 17:00 in the picker:
https://www.dumps4less.com/
Question: 130
A data engineer is using a Databricks SQL query to monitor the performance of an ELT job. The ELT job is triggered by a
specific number of input records being ready to process. The Databricks SQL query returns the number of minutes since
the job’s most recent runtime. Which of the following approaches can enable the data engineering team to be notified if
the ELT job has not been run in an hour?
They can set up an Alert for the accompanying dashboard to notify them if the returned value is greater than 60.
A.
They can set up an Alert for the query to notify when the ELT job fails.
B.
They can set up an Alert for the accompanying dashboard to notify when it has not refreshed in 60 minutes.
C.
They can set up an Alert for the query to notify them if the returned value is greater than 60.
D.
This type of alert is not possible in Databricks
E.
Answer: D
Explanation/Reference:
The answer is, They can set up an Alert for the query to notify them if the returned value is greater than 60.
The important thing to note here is that alert can only be setup on query not on the dashboard, query can return a value,
which is used if alert can be triggered.
Question: 131
Which of the following is true, when building a Databricks SQL dashboard?
A dashboard can only use results from one query
A.
Only one visualization can be developed with one query result
B.
A dashboard can only connect to one schema/Database
C.
More than one visualization can be developed using a single query result
D.
A dashboard can only have one refresh schedule
E.
Answer: D
Explanation/Reference:
the answer is, More than one visualization can be developed using a single query result.
In the query editor pane + Add visualization tab can be used for many visualizations for a single query result.
https://www.dumps4less.com/
Question: 132
A newly joined team member John Smith in the Marketing team currently has access read access to sales tables but does
not have access to update the table, which of the following commands help you accomplish this?
GRANT UPDATE ON TABLE table_name TO john.smith@marketing.com
A.
GRANT USAGE ON TABLE table_name TO john.smith@marketing.com
B.
GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com
C.
GRANT UPDATE TO TABLE table_name ON john.smith@marketing.com
D.
GRANT MODIFY TO TABLE table_name ON john.smith@marketing.com
E.
Answer: C
Explanation/Reference:
The answer is GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com
https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#privileges
https://www.dumps4less.com/
Question: 133
A new user who currently does not have access to the catalog or schema is requesting access to the customer table in
sales schema, but the customer table contains sensitive information, so you have decided to create view on the table
excluding columns that are sensitive and granted access to the view GRANT SELECT ON view_name to
user@company.com but when the user tries to query the view, gets the error view does not exist. What is the issue
preventing user to access the view and how to fix it?
User requires SELECT on the underlying table
A.
User requires to be put in a special group that has access to PII data
B.
User has to be the owner of the view
C.
User requires USAGE privilege on Sales schema
D.
User needs ADMIN privilege on the view
E.
Answer: D
Explanation/Reference:
The answer is User requires USAGE privilege on Sales schema,
Data object privileges - Azure Databricks | Microsoft Docs
GRANT USAGE ON SCHEMA sales TO user@company.com;
USAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.
Question: 134
How do you access or use tables in the unity catalog?
schema_name.table_name
A.
schema_name.catalog_name.table_name
B.
catalog_name.table_name
C.
catalog_name.database_name.schema_name.table_name
D.
catalog_name.schema_name.table_name
E.
Answer: E
Explanation/Reference:
The answer is catalog_name.schema_name.table_name
https://www.dumps4less.com/
note: Database and Schema are analogous they are interchangeably used in the Unity catalog.
Question: 135
How do you upgrade an existing workspace managed table to a unity catalog table?
ALTER TABLE table_name SET UNITY_CATALOG = TRUE
A.
Create table catalog_name.schema_name.table_nameas select * from hive_metastore.old_schema.old_table
B.
Create table table_name as select * from hive_metastore.old_schema.old_table
C.
Create table table_name format = UNITY as select * from old_table_name
D.
Create or replace table_name format = UNITY using deep clone old_table_name
E.
Answer: B
Explanation/Reference:
The answer is Create table catalog_name.schema_name.table_name as select * from
hive_metastore.old_schema.old_table
Basically, we are moving the data from an internal hive metastore to a metastore and catalog that is registered in Unity
catalog.
note: if it is a managed table the data is copied to a different storage account, for a large tables this can take a lot of
time. For an external table the process is different.
Managed table: Upgrade a managed to Unity Catalog
External table: Upgrade an external table to Unity Catalog
Question: 136
Which of the statements are correct when choosing between lakehouse and Datawarehouse?
Traditional Data warehouses have special indexes which are optimized for Machine learning
A.
Traditional Data warehouses can serve low query latency with high reliability for BI workloads
B.
SQL support is only available for Traditional Datawarehouse’s, Lakehouses support Python and Scala
C.
https://www.dumps4less.com/
Traditional Data warehouses are the preferred choice if we need to support ACID, Lakehouse does not support ACID.
D.
Lakehouse replaces the current dependency on data lakes and data warehouses uses an open standard storage
E.
format and supports low latency BI workloads
Answer: E
Explanation/Reference:
The lakehouse replaces the current dependency on data lakes and data warehouses for modern data companies that
desire:
· Open, direct access to data stored in standard data formats.
· Indexing protocols optimized for machine learning and data science.
· Low query latency and high reliability for BI and advanced analytics.
Question: 137
Where are Interactive notebook results stored in Databricks product architecture?
Data plane
A.
Control plane
B.
Data and Control plane
C.
JDBC data source
D.
Databricks web application
E.
Answer: C
Explanation/Reference:
The answer is Data and Control plane,
Interactive notebook results are stored in a combination of the control plane (partial results for presentation in the UI)
and customer storage.
https://docs.microsoft.com/en-us/azure/databricks/getting-started/overview#--high-level-architecture
Question: 138
Which of the following statements are true about a lakehouse?
Lakehouse only supports Machine learning workloads and Data warehouses support BI workloads
A.
Lakehouse only supports end-to-end streaming workloads and Data warehouses support Batch workloads
B.
https://www.dumps4less.com/
Lakehouse does not support ACID
C.
Lakehouse do not support SQL
D.
Lakehouse supports Transactions
E.
Answer: E
Explanation/Reference:
What Is a Lakehouse? - The Databricks Blog
Question: 139
You had setup a new all-purpose cluster, but the cluster is unable to start which of the following steps do you need to
https://www.dumps4less.com/
take to resolve this issue?
Check the cluster driver logs
A.
Check the cluster event logs
B.
Workspace logs
C.
Storage account
D.
Data plane
E.
Answer: B
Explanation/Reference:
Cluster event logs are very useful, to identify issues pertaining to cluster availability. Cluster may not start due to
resource limitations or issues with cloud provider.
Question: 140
Which of the following developer operations in CI/CD flow can be implemented in Databricks Repos?
Delete branch
A.
Trigger Databricks CICD pipeline
B.
Commit and push code
C.
Create a pull request
D.
Approve the pull request
E.
Answer: C
Explanation/Reference:
The answer is Commit and push code.
See the below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workflow
All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git
provider like Github or Azure Devops
https://www.dumps4less.com/
Question: 141
You noticed that a team member is using an all-purpose cluster to run a job every 30 mins, what would you recommend
in reducing the compute cost.
Reduce the size of the cluster
A.
Reduce the number of nodes and enable auto scale
B.
Enable auto termination after 30 mins
C.
Change the cluster all-purpose to job cluster
D.
Change the cluster mode from all-purpose to single-mode
E.
Answer: D
Explanation/Reference:
Anytime you don't need to interact with a notebook, especially for a scheduled job use job cluster. Using all-purpose
cluster can be twice as expensive as a job cluster.
https://www.dumps4less.com/
Question: 142
Which of the following commands can be used to run one notebook from another notebook?
notebook.utils.run("full notebook path")
A.
execute.utils.run("full notebook path")
B.
dbutils.notebook.run("full notebook path")
C.
only job clusters can run notebook
D.
spark.notebook.run("full notebook path")
E.
Answer: C
Explanation/Reference:
The answer is dbutils.notebook.run(" full notebook path ")
Here is the full command with additional options.
run(path: String, timeout_seconds: int, arguments: Map): String
dbutils.notebook.run("ful-notebook-name", 60, {"argument": "data", "argument2": "data2", ...})
Question: 143
Which of the following SQL command can be used to insert or update or delete rows based on a condition to check if a
row(s) exists?
MERGE INTO table_name
A.
COPY INTO table_name
B.
UPDATE table_name
C.
INSERT INTO OVERWRITE table_name
D.
INSERT IF EXISTS table_name
E.
Answer: A
Explanation/Reference:
here is the additional documentation for your review.
https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-merge-into.html
MERGE INTO target_table_name [target_alias]
USING source_table_reference [source_alias]
ON merge_condition
https://www.dumps4less.com/
[ WHEN MATCHED [ AND condition ] THEN matched_action ] [...]
[ WHEN NOT MATCHED [ AND condition ] THEN not_matched_action ] [...]
matched_action
{ DELETE |
UPDATE SET * |
UPDATE SET { column1 = value1 } [, ...] }
not_matched_action
{ INSERT * |
INSERT (column1 [, ...] ) VALUES (value1 [, ...])
Question: 144
When investigating a data issue, if you are looking compare the current version of the data with the previous version,
what is the best way to query historical data?
SELECT * FROM TIME_TRAVEL(table_name) where time_stamp = 'timestamp'
A.
TIME_TRAVEL FROM table_name where time_stamp = 'timestamp'
B.
SELECT * FROM table_name as of 'timestamp'
C.
DISCRIBE HISTORY table_name as of 'timestmap'
D.
SHOW HISTORY table_name as of 'timestmap'
E.
Answer: C
Explanation/Reference:
The answer is SELECT * FROM table_name as of 'timestamp'
FYI, Time travel supports two ways one is using timestamp and the second way is using version number,
Timestamp:
SELECT count(*) FROM my_table TIMESTAMP AS OF "2019-01-01"
SELECT count(*) FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)
SELECT count(*) FROM my_table TIMESTAMP AS OF "2019-01-01 01:30:00.000"
Version Number:
SELECT count(*) FROM my_table VERSION AS OF 5238
SELECT count(*) FROM my_table@v5238
SELECT count(*) FROM delta.`/path/to/my/table@v5238`
https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html
Question: 145
While querying the previous version of the table using time travel you realized you are no longer able to query the
historical data?
You currently do not have access to view historical data
A.
By default, historical data is cleaned every 180 days in DELTA
B.
https://www.dumps4less.com/
A command VACUUM table_name RETAIN 0 was ran
C.
Time travel is disabled
D.
Time travel must be enabled before you query previous data
E.
Answer: C
Explanation/Reference:
The answer is, VACUUM table_name RETAIN 0 was ran
Recursively vacuum directories associated with the Delta table and remove data files that are no longer in the latest
state of the transaction log for the table and are older than a retention threshold. Default is 7 Days.
When VACUUM table_name RETAIN 0 is ran all of the historical versions of data are lost time travel can only provide the
current state.
Question: 146
You have accidentally deleted records from a table called transactions, what is the easiest way to restore the records
deleted or the previous state of the table?
RESTORE TABLE transactions FROM VERSION as of 'timestamp'
A.
RESTORE TABLE transactions TO VERSION as of 'timestamp'
B.
INSERT INTO OVERWRITE transactionsSELECT * FROM transactions as of ‘timestamp’MINUSSELECT * FROM
C.
transactions
INSERT INTO OVERWRITE transactionsSELECT * FROM transactions as of ‘timestamp’INTERSECTSELECT * FROM
D.
transactions
COPY OVERWRITE transactions from VERSION as of 'timestamp'
E.
Answer: B
Explanation/Reference:
RESTORE (Databricks SQL) | Databricks on AWS
RESTORE [TABLE] table_name [TO] time_travel_version
Time travel supports using timestamp or version number
time_travel_version
{ TIMESTAMP AS OF timestamp_expression |
VERSION AS OF version }
timestamp_expression can be any one of:
'2018-10-18T22:15:12.013Z', that is, a string that can be cast to a timestamp
cast('2018-10-18 13:36:32 CEST' as timestamp)
'2018-10-18', that is, a date string
current_timestamp() - interval 12 hours
date_sub(current_date(), 1)
https://www.dumps4less.com/
Any other expression that is or can be cast to a timestamp
Question: 147
Create schema called bronze using location ‘/mnt/delta/bronze’, check if schema exists before creating.
CREATE SCHEMA IF NOT EXISTS bronze LOCATION '/mnt/delta/bronze'
A.
CREATE SCHEMA bronze IF NOT EXISTS LOCATION '/mnt/delta/bronze'
B.
if IS_SCHEMA('bronze'): CREATE SCHEMA bronze LOCATION '/mnt/delta/bronze'
C.
Schema creation is not available in metastore, it can only be done in Unity catalog UI
D.
Cannot create schema without a database
E.
Answer: A
Explanation/Reference:
https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html
CREATE SCHEMA [ IF NOT EXISTS ] schema_name [ LOCATION schema_directory ]
Question: 148
How do you check the location of an existing schema in Delta Lake?
Run SQL command SHOW LOCATION schema_name
A.
Check unity catalog UI
B.
Use Data explorer
C.
Run SQL command DESCRIBE SCHEMA EXTENDED schema_name
D.
Schemas are internally in-store external hive meta stores like MySQL or SQL Server
E.
Answer: D
Explanation/Reference:
Here is an example of how it looks
https://www.dumps4less.com/
Question: 149
Which of the below SQL commands create a Global temporary view?
CREATE OR REPLACE TEMPORARY VIEW view_name AS SELECT * FROM table_name
A.
CREATE OR REPLACE LOCAL TEMPORARY VIEW view_name AS SELECT * FROM table_name
B.
CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_name AS SELECT * FROM table_name
C.
CREATE OR REPLACE VIEW view_name AS SELECT * FROM table_name
D.
CREATE OR REPLACE LOCAL VIEW view_name AS SELECT * FROM table_name
E.
Answer: C
Explanation/Reference:
CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_name
AS SELECT * FROM table_name
There are two types of temporary views that can be created Local and Global
A session-scoped temporary view is only available with a spark session, so another notebook in the same cluster can not
access it. if a notebook is detached and reattached local temporary view is lost.
A global temporary view is available to all the notebooks in the cluster but if a cluster restarts a global temporary view is
lost.
https://www.dumps4less.com/
Question: 150
When you drop a managed table using SQL syntax DROP TABLE table_name how does it impact metadata, history, and
data stored in the table?
Drops table from meta store, drops metadata, history, and data in storage.
A.
Drops table from meta store and data from storage but keeps metadata and history in storage
B.
Drops table from meta store, meta data and history but keeps the data in storage
C.
Drops table but keeps meta data, history and data in storage
D.
Drops table and history but keeps meta data and data in storage
E.
Answer: A
Explanation/Reference:
For a managed table, a drop command will drop everything from metastore and storage.
Question: 151
The team has decided to take advantage of table properties to identify a business owner for each table, which of the
following table DDL syntax allows you to populate a table property identifying the business owner of a table
CREATE TABLE inventory (id INT, units FLOAT)SET TBLPROPERTIES business_owner = 'supply chain'
A.
CREATE TABLE inventory (id INT, units FLOAT)TBLPROPERTIES (business_owner = 'supply chain')
B.
CREATE TABLE inventory (id INT, units FLOAT)SET (business_owner = ‘supply chain’)
C.
CREATE TABLE inventory (id INT, units FLOAT)SET PROPERTY (business_owner = ‘supply chain’)
D.
CREATE TABLE inventory (id INT, units FLOAT)SET TAG (business_owner = ‘supply chain’)
E.
Answer: B
Explanation/Reference:
CREATE TABLE inventory (id INT, units FLOAT) TBLPROPERTIES (business_owner = ‘supply chain’)
Table properties and table options (Databricks SQL) | Databricks on AWS
Alter table command can used to update the TBLPROPERTIES
ALTER TABLE inventory SET TBLPROPERTIES(business_owner , 'operations')
https://www.dumps4less.com/
Question: 152
Data science team has requested they are missing a column in the table called average price, this can be calculated
using units sold and sales amt, which of the following SQL statements allow you to reload the data with additional column
INSERT OVERWRITE salesSELECT *, salesAmt/unitsSold as avgPrice FROM sales
A.
CREATE OR REPALCE TABLE salesAS SELECT *, salesAmt/unitsSold as avgPrice FROM sales
B.
MERGE INTO sales USING (SELECT *, salesAmt/unitsSold as avgPrice FROM sales)
C.
OVERWRITE sales AS SELECT *, salesAmt/unitsSold as avgPrice FROM sales
D.
COPY INTO SALES AS SELECT *, salesAmt/unitsSold as avgPrice FROM sales
E.
Answer: B
Explanation/Reference:
CREATE OR REPALCE TABLE sales
AS SELECT *, salesAmt/unitsSold as avgPrice FROM sales
The main difference between INSERT OVERWRITE and CREATE OR REPLACE TABLE(CRAS) is that CRAS can modify the
schema of the table, i.e it can add new columns or change data types of existing columns. By default INSERT OVERWRITE
only overwrites the data.
INSERT OVERWRITE can also be used to overwrite schema, only when spark.databricks.delta.schema.autoMerge.enabled
is set true if this option is not enabled and if there is a schema mismatch command will fail.
Question: 153
You are working on a process to load external CSV files into a Delta by leveraging the COPY INTO command, but after
running the command for the second time no data was loaded into the table name, why is that?
COPY INTO table_name
FROM 'dbfs:/mnt/raw/*.csv'
FILEFORMAT = CSV
COPY INTO only works one time data load
A.
Run REFRESH TABLE sales before running COPY INTO
B.
COPY INTO did not detect new files after the last load
C.
Use incremental = TRUE option to load new files
D.
COPY INTO does not support incremental load, use AUTO LOADER
E.
Answer: C
https://www.dumps4less.com/
Explanation/Reference:
The answer is COPY INTO did not detect new files after the last load,
COPY INTO keeps track of files that were successfully loaded into the table, the next time when the COPY INTO runs it
skips them.
FYI, you can change this behavior by using COPY_OPTIONS 'force'= 'true', when this option is enabled all files in the
path/pattern are loaded.
COPY INTO table_identifier
FROM [ file_location | (SELECT identifier_list FROM file_location) ]
FILEFORMAT = data_source
[FILES = [file_name, ... | PATTERN = 'regex_pattern']
[FORMAT_OPTIONS ('data_source_reader_option' = 'value', ...)]
[COPY_OPTIONS 'force' = ('false'|'true')]
Question: 154
What is the main difference between the below two commands?
INSERT OVERWRITE table_name
SELECT * FROM table
CREATE OR REPLACE TABLE table_name
AS SELECT * FROM table
INSERT OVERWRITE replaces data by default, CREATE OR REPLACE replaces data and Schema by default
A.
INSERT OVERWRITE replaces data and schema by default, CREATE OR REPLACEreplaces data by default
B.
INSERT OVERWRITE maintains historical data versions by default, CREATE OR REPLACEclears the historical data
C.
versions by default
INSERT OVERWRITE clears historical data versions by default, CREATE OR REPLACE maintains the historical data
D.
versions by default
Both are same and results in identical outcomes
E.
Answer: A
Explanation/Reference:
The answer is, INSERT OVERWRITE replaces data, CRAS replaces data and Schema
The main difference between INSERT OVERWRITE and CREATE OR REPLACE TABLE(CRAS) is that CRAS can modify the
schema of the table, i.e it can add new columns or change data types of existing columns. By default INSERT OVERWRITE
only overwrites the data.
INSERT OVERWRITE can also be used to overwrite schema, only when spark.databricks.delta.schema.autoMerge.enabled
is set true if this option is not enabled and if there is a schema mismatch command will fail.
Question: 155
Which of the following functions can be used to convert JSON string to Struct data type?
TO_STRUCT (json value)
A.
https://www.dumps4less.com/
FROM_JSON (json value)
B.
FROM_JSON (json value, schema of json)
C.
CONVERT (json value, schema of json)
D.
CAST (json value as STRUCT)
E.
Answer: C
Explanation/Reference:
Syntax
Copy
from_json(jsonStr, schema [, options])
Arguments
jsonStr: A STRING expression specifying a row of CSV data.
schema: A STRING literal or invocation of schema_of_json function (Databricks SQL).
options: An optional MAP literal specifying directives.
Refer documentation for more details,
https://docs.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/from_json
Question: 156
You are working on a marketing team request to identify customers with same information between two tables
CUSTOMERS_2021 and CUSTOMERS_2020 each table contains 25 columns with same schema, You are looking to identify
rows match between two tables across all columns, which of the following can be used to perform in SQL
SELECT * FROM CUSTOMERS_2021 UNIONSELECT * FROM CUSTOMERS_2020
A.
SELECT * FROM CUSTOMERS_2021 UNION ALLSELECT * FROM CUSTOMERS_2020
B.
SELECT * FROM CUSTOMERS_2021 C1INNER JOIN CUSTOMERS_2020 C2ON C1.CUSTOMER_ID = C2.CUSTOMER_ID
C.
SELECT * FROM CUSTOMERS_2021 INTERSECTSELECT * FROM CUSTOMERS_2020
D.
SELECT * FROM CUSTOMERS_2021 EXCEPTSELECT * FROM CUSTOMERS_2020
E.
Answer: D
Explanation/Reference:
Answer is
SELECT * FROM CUSTOMERS_2021
INTERSECT
SELECT * FROM CUSTOMERS_202
INTERSECT [ALL | DISTINCT]
Returns the set of rows which are in both subqueries.
If ALL is specified a row that appears multiple times in the subquery1 as well as in subquery will be returned multiple
times.
https://www.dumps4less.com/
If DISTINCT is specified the result does not contain duplicate rows. This is the default.
Question: 157
You are looking to process the data based on two variables, one to check if the department is supply chain and second to
check if process flag is set to True
if department = “supply chain” & process:
A.
if department == “supply chain” && process:
B.
if department == “supply chain” & process == TRUE:
C.
if department == “supply chain” & if process == TRUE:
D.
if department == "supply chain" and process:
E.
Answer: E
Question: 158
You were asked to create a notebook that can take department as a parameter and process the data accordingly, which
is the following statements result in storing the notebook parameter into a python variable
SET department = dbutils.widget.get("department")
A.
ASSIGN department == dbutils.widget.get("department")
B.
department = dbutils.widget.get("department")
C.
department = notebook.widget.get("department")
D.
department = notebook.param.get("department")
E.
Answer: C
Explanation/Reference:
The answer is department = dbutils.widget.get("department")
Refer to additional documentation here
https://docs.databricks.com/notebooks/widgets.html
https://www.dumps4less.com/
Question: 159
Which of the following statements can successfully read the notebook widget and pass the python variable to a SQL
statement in a Python notebook cell?
order_date = dbutils.widgets.get("widget_order_date")spark.sql(f"SELECT * FROM sales WHERE orderDate =
A.
'f{order_date }'")
order_date = dbutils.widgets.get("widget_order_date")spark.sql(f"SELECT * FROM sales WHERE orderDate =
B.
'order_date' ")
order_date = dbutils.widgets.get("widget_order_date")spark.sql(f”SELECT * FROM sales WHERE orderDate =
C.
'${order_date }' ")
order_date = dbutils.widgets.get("widget_order_date")spark.sql(f"SELECT * FROM sales WHERE orderDate =
D.
'{order_date}' ")
order_date = dbutils.widgets.get("widget_order_date")spark.sql("SELECT * FROM sales WHERE orderDate =
E.
order_date")
Answer: D
Question: 160
For a spark stream process to read a delta table to event logs and create a summary table with customerId and the
number of times the customerId is present in the event log and write a one-time micro batch, fill in the blanks to
complete the query.
spark._________
.format("delta")
.table("events_log")
.groupBy("customerId")
.count()
._______
.format("delta")
.outputMode("complete")
.option("checkpointLocation", "/tmp/delta/eventsByCustomer/_checkpoints/")
.trigger(______)
.table("target_table")
writeStream, readStream, once
A.
readStream, writeStream, once
B.
writeStream, processingTime = once
C.
writeStream, readStream, once = True
D.
readStream, writeStream, once = True
E.
https://www.dumps4less.com/
Answer: E
Explanation/Reference:
The answer is readStream, writeStream, once = True.
spark.readStream
.format("delta")
.table("events_log")
.groupBy("customerId")
.count()
.writeStream
.format("delta")
.outputMode("complete")
.option("checkpointLocation", "/tmp/delta/eventsByCustomer/_checkpoints/")
.trigger(once = True)
.table("target_table")
Question: 161
You would like to build a streaming process to read from a Kafka queue and write to a Delta table every 15 minutes, what
is the correct trigger option
trigger("15 minutes")
A.
trigger(process "15 minutes")
B.
trigger(processingTime = 15)
C.
trigger(processingTime = "15 Minutes")
D.
trigger(15)
E.
Answer: D
Explanation/Reference:
The answer is trigger(processingTime = "15 Minutes")
Triggers:
Unspecified
This is the default. This is equivalent to using processingTime="500ms"
Fixed interval micro-batches .trigger(processingTime="2 minutes")
The query will be executed in micro-batches and kicked off at the user-specified intervals
One-time micro-batch .trigger(once=True)
The query will execute a single micro-batch to process all the available data and then stop on its own
One-time micro-batch.trigger .trigger(availableNow=True) -- New feature a better version of (once=True)
Databricks supports trigger(availableNow=True) in Databricks Runtime 10.2 and above for Delta Lake and Auto Loader
sources. This functionality combines the batch processing approach of trigger once with the ability to configure batch
size, resulting in multiple parallelized batches that give greater control for right-sizing batches and the resultant files.
https://www.dumps4less.com/
Question: 162
Which of the following scenarios is the best fit for the AUTO LOADER solution?
Efficiently process new data incrementally from cloud object storage
A.
Incrementally process new streaming from data into delta lake
B.
Incrementally process new data from relational databases like MySQL
C.
Efficiently copy data from data lake location to another data lake location
D.
Efficiently move data incrementally from one delta table to another delta table
E.
Answer: A
Explanation/Reference:
Refer to more documentation here,
https://docs.microsoft.com/en-us/azure/databricks/ingestion/auto-loader
Question: 163
You had AUTO LOADER to process millions of files a day and noticed slowness in load process, so you scaled up the
Databricks cluster but realized the performance of the Auto loader is still not improving, what is the best way to resolve
this.
AUTO LOADER is not suitable to process millions of files a day
A.
Setup a second AUTO LOADER process to process the data
B.
Increase the maxFilesPerTrigger option to a sufficiently high number
C.
Copy the data from cloud storage to local disk on the cluster for faster access
D.
Merge files to one large file
E.
Answer: C
Explanation/Reference:
The default value of maxFilesPerTrigger is 1000 it can be increased to a much higher number but will require a much
larger compute to process.
https://www.dumps4less.com/
https://docs.databricks.com/ingestion/auto-loader/options.html
Question: 164
The current ELT pipeline is receiving data from the operations team once a day so you had setup an AUTO LOADER
process to run once a day using trigger (Once = True) and scheduled a job to run once a day, operations team recently
rolled out a new feature that allows them to send data every 1 min, what changes do you need to make to AUTO
LOADER to process the data every 1 min.
Convert AUTO LOADER to structured streaming
A.
Change AUTO LOADER trigger to .trigger(ProcessingTime = "1 minute")
B.
Setup a job cluster run the notebook once a minute
C.
Enable stream processing
D.
Change AUTO LOADER trigger to ("1 minute")
E.
Answer: B
Question: 165
What is the purpose of bronze layer in a Multi hop architecture?
Copy of raw data, easy to query and ingest data for downstream processes
A.
Powers ML applications
B.
Data quality checks, corrupt data quarantined
C.
Contain aggregated data that is to be consumed into Silver
D.
Reduces data storage by compressing the data
E.
Answer: A
https://www.dumps4less.com/
Explanation/Reference:
Medallion Architecture – Databricks
Bronze Layer:
1. Raw copy of ingested data
2. Replaces traditional data lake
3. Provides efficient storage and querying of full, unprocessed history of data
4. No schema is applied at this layer
Question: 166
What is the purpose of the silver layer in a Multi hop architecture?
Replaces a traditional data lake
A.
Efficient storage and querying of full, unprocessed history of data
B.
Eliminates duplicate data, quarantines bad data
C.
Refined views with aggregated data
D.
Optimized query performance for business-critical data
E.
Answer: C
Explanation/Reference:
Medallion Architecture – Databricks
Silver Layer:
1. Reduces data storage complexity, latency, and redundency
2. Optimizes ETL throughput and analytic query performance
3. Preserves grain of original data (without aggregation)
4. Eliminates duplicate records
5. production schema enforced
6. Data quality checks, quarantine corrupt data
Question: 167
What is the purpose of gold layer in Multi hop architecture?
Optimizes ETL throughput and analytic query performance
A.
Eliminate duplicate records
B.
Preserves grain of original data, without any aggregations
C.
Data quality checks and schema enforcement
D.
Optimized query performance for business-critical data
E.
https://www.dumps4less.com/
Answer: E
Explanation/Reference:
Medallion Architecture – Databricks
Gold Layer:
1. Powers Ml applications, reporting, dashboards, ad hoc analytics
2. Refined views of data, typically with aggregations
3. Reduces strain on production systems
4. Optimizes query performance for business-critical data
Question: 168
The Delta Live Tables Pipeline is configured to run in Development mode using the Triggered Pipeline Mode. what is the
expected outcome after clicking Start to update the pipeline?
All datasets will be updated once and the pipeline will shut down. The compute resources will be terminated
A.
All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed
B.
for the update and terminated when the pipeline is stopped
All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist after
C.
the pipeline is stopped to allow for additional development and testing
All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for
D.
additional development and testing
All datasets will be updated continuously and the pipeline will not shut down. The compute resources will persist with
E.
the pipeline
Answer: D
Explanation/Reference:
The answer is All datasets will be updated once and the pipeline will shut down. The compute resources will persist to
allow for additional testing.
DLT pipeline supports two modes Development and Production, you can switch between the two based on the stage of
your development and deployment lifecycle.
Development and production modes
When you run your pipeline in development mode, the Delta Live Tables system:
Reuses a cluster to avoid the overhead of restarts.
Disables pipeline retries so you can immediately detect and fix errors.
In production mode, the Delta Live Tables system:
Restarts the cluster for specific recoverable errors, including memory leaks and stale credentials.
Retries execution in the event of specific errors, for example, a failure to start a cluster.
Use the
buttons in the Pipelines UI to switch between development and production modes. By default, pipelines run in
development mode.
https://www.dumps4less.com/
Switching between development and production modes only controls cluster and pipeline execution behavior. Storage
locations must be configured as part of pipeline settings and are not affected when switching between modes.
Please review additional DLT concepts using below link
https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#delta-live-tables-concepts
Question: 169
The DELT LIVE TABLE Pipeline is configured to run in Production mode using the continuous Pipeline Mode. what is the
expected outcome after clicking Start to update the pipeline?
All datasets will be updated once and the pipeline will shut down. The compute resources will be terminated
A.
All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed
B.
for the update and terminated when the pipeline is stopped
All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist after
C.
the pipeline is stopped to allow for additional testing
All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for
D.
additional testing
All datasets will be updated continuously and the pipeline will not shut down. The compute resources will persist with
E.
the pipeline
Answer: B
Explanation/Reference:
The answer is, All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be
deployed for the update and terminated when the pipeline is stopped.
DLT pipeline supports two modes Development and Production, you can switch between the two based on the stage of
your development and deployment lifecycle.
Development and production modes
Development:
When you run your pipeline in development mode, the Delta Live Tables system:
Reuses a cluster to avoid the overhead of restarts.
Disables pipeline retries so you can immediately detect and fix errors.
Production:
In production mode, the Delta Live Tables system:
Restarts the cluster for specific recoverable errors, including memory leaks and stale credentials.
Retries execution in the event of specific errors, for example, a failure to start a cluster.
Use the
buttons in the Pipelines UI to switch between development and production modes. By default, pipelines run in
development mode.
Switching between development and production modes only controls cluster and pipeline execution behavior. Storage
locations must be configured as part of pipeline settings and are not affected when switching between modes.
Please review additional DLT concepts using the below lin
https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#delta-live-tables-concepts
https://www.dumps4less.com/
Question: 170
you are working to set up two notebooks to run on a schedule second notebook is dependent on first notebook, both
notebooks need different types of compute to run in an optimal fashion? What is the best way to setup these notebooks
as jobs?
Use DELTA LIVE PIPELINES instead of notebook tasks
A.
A Job can only use single cluster, setup job for each notebook and use job dependency to link both jobs together
B.
Each task can use different cluster, add these two notebooks as two tasks in a single job with linear dependency and
C.
modify the cluster as needed for each of the tasks
Use a single job to setup both notebooks as individual tasks, but use the cluster API to setup the second cluster before
D.
the start of second task
Use a very large cluster to run both the tasks in a single job
E.
Answer: C
Explanation/Reference:
Tasks in Jobs support different clusters for each task in the same job.
Question: 171
You are tasked to setup a set notebook as a job for six departments and each department can run the task parallelly, the
notebook takes an input parameter dept number to process the data by department how do you go about to setup this in
job?
Use a single notebook as task in the job and use dbutils.notebook.run to run each notebook with parameter in a
A.
different cell
A task in the job cannot take an input parameter, create six notebooks with hardcoded dept number and setup six
B.
tasks with linear dependency in the job
A task accepts key-value pair parameters, creates six tasks pass department number as parameter foreach task with
C.
no dependency in the job as they can all run in parallel.
A parameter can only be passed at the job level, create six jobs pass department number to each job with linear job
D.
dependency
A parameter can only be passed at the job level, create six jobs pass department number to each job with no job
E.
dependency
Answer: C
https://www.dumps4less.com/
Explanation/Reference:
Here is how you setup
Create a single job and six tasks with the same notebook and assign a different parameter for each task ,
All tasks are added in a single job and can run parallel either using single shared cluster or with individual clusters
https://www.dumps4less.com/
Question: 172
You are asked to setup two tasks, first task runs a notebook to download the from a remote database, second task is a
DLT pipeline that can process this data, how do you plan to configure this in Jobs UI
Single job cannot have a notebook task and DLT Pipeline task, use two different jobs with linear dependency.
A.
Jobs UI does not support DTL pipeline, setup the first task using jobs UI and setup the DLT to run in continuous mode.
B.
Jobs UI does not support DTL pipeline, setup the first task using jobs UI and setup the DLT to run in trigger mode.
C.
Single job can be used to setup both notebook and DLT pipeline, use two different tasks with linear dependency.
D.
Add first step in the DLT pipeline and run the DLT pipeline as triggered mode in JOBS UI
E.
Answer: D
Explanation/Reference:
The answer is Single job can be used to set up both notebook and DLT pipeline, use two different tasks with linear
dependency,
Here is the JOB UI
Create a notebook task
Create DLT task
add notebook task as dependency
Final view
Create the notebook task
DLT task
https://www.dumps4less.com/
Final view
https://www.dumps4less.com/
Question: 173
You are asked to setup the alert to notify every time once the data is loaded into a reporting table, team also asked to
include the number of records in the alert email notification.
Use notebook and python code to run every minute, using python variables to capture send the information in an
A.
email
Setup an alert but use the default template to notify the message in email’s subject
B.
Setup an alert but use the custom template to notify the message in email’s subject
C.
Use the webhook destination instead so alert message can be customized
D.
Use custom email hook to customize the message
E.
Answer: C
Explanation/Reference:
Alerts support custom template supports using variables to customize the default message. setup the query to count the
number rows based on your criteria and use the variable QUERY_RESULT_VALUE
Alerts | Databricks on AWS
https://www.dumps4less.com/
Question: 174
Operations team is using a centralized data quality monitoring system, a user can publish data quality metrics through a
webhook, you were asked to develop a process to send messages using webhook if there is atleast one duplicate record,
which of following approaches can be taken to integrate with current data quality monitoring system
Use notebook and Jobs to use python to publish DQ metrics
A.
Setup an alert to send an email, use python to parse email, and publish a webhook message
B.
Setup an alert with custom template
C.
Setup an alert with custom Webhook destination
D.
Setup an alert with dynamic template
E.
Answer: D
Explanation/Reference:
Alerts supports multiple destinations, email is the default destinations.
Alert destinations | Databricks on AWS
https://www.dumps4less.com/
Question: 175
You are currently working with application team to setup a SQL Endpoint point, once the team started consuming the SQL
Endpoint you noticed that during peak hours as the number of users increases you are seeing degradation in the query
performance, which of the following steps can be taken to resolve the issue?
They can turn on the Serverless feature for the SQL endpoint.
A.
They can increase the maximum bound of the SQL endpoint’s scaling range.
B.
They can increase the cluster size of the SQL endpoint.
C.
They can turn on the Auto Stop feature for the SQL endpoint.
D.
https://www.dumps4less.com/
They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from “Cost
E.
optimized” to “Reliability Optimized.”
Answer: B
Explanation/Reference:
The answer is, They can increase the maximum bound of the SQL endpoint’s scaling range.
SQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.
Scale-out -> to add more clusters for a SQL endpoint, change max number of clusters scaling range.
If you are trying to improve the throughput, being able to run as many queries as possible then having an additional
cluster(s) will improve the performance.
Scale-up-> Increase the size of the SQL endpoint, change cluster size from x-small to small, to medium, X Large....
If you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the
cluster will improve the performance.
Question: 176
Data engineering team is using a SQL query to review data quality and monitor the ETL job everyday, which of the
following approaches can be used to setup a schedule and automate this process?
They can schedule the query to run every 1 day from the Jobs UI
A.
They can schedule the query to refresh every 1 day from the query’s page in Databricks SQL.
B.
They can schedule the query to run every 12 hours from the Jobs UI.
C.
They can schedule the query to refresh every 1 day from the SQL endpoint’s page in Databricks SQL.
D.
They can schedule the query to refresh every 12 hours from the SQL endpoint’s page in Databricks SQL
E.
Answer: B
Explanation/Reference:
Individual queries can be refreshed on a schedule basis,
To set the schedule:
Click the query info tab.
Click the link to the right of Refresh Schedule to open a picker with schedule intervals.
Set the schedule.
The picker scrolls and allows you to choose:
An interval: 1-30 minutes, 1-12 hours, 1 or 30 days, 1 or 2 weeks
A time. The time selector displays in the picker only when the interval is greater than 1 day and the day selection is
greater than 1 week. When you schedule a specific time, Databricks SQL takes input in your computer’s timezone and
converts it to UTC. If you want a query to run at a certain time in UTC, you must adjust the picker by your local offset. For
example, if you want a query to execute at 00:00 UTC each day, but your current timezone is PDT (UTC-7), you should
select 17:00 in the picker:
Click OK.
Your query will run automatically.
https://www.dumps4less.com/
If you experience a scheduled query not executing according to its schedule, you should manually trigger the query to
make sure it doesn’t fail. However, you should be aware of the following:
If you schedule an interval—for example, “every 15 minutes”—the interval is calculated from the last successful
execution. If you manually execute a query, the scheduled query will not be executed until the interval has passed.
If you schedule a time, Databricks SQL waits for the results to be “outdated”. For example, if you have a query set to
refresh every Thursday and you manually execute it on Wednesday, by Thursday the results will still be considered
“valid”, so the query wouldn’t be scheduled for a new execution. Thus, for example, when setting a weekly schedule,
check the last query execution time and expect the scheduled query to be executed on the selected day after that
execution is a week old. Make sure not to manually execute the query during this time.
If a query execution fails, Databricks SQL retries with a back-off algorithm. The more failures the further away the next
retry will be (and it might be beyond the refresh interval).
Refer documentation for additional info,
https://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/schedule-query
Question: 177
In order to use Unity catalog features, which of the following steps needs to be taken on managed/external tables in the
Databricks workspace?
Enable unity catalog feature in workspace settings
A.
Migrate/upgrade objects in workspace managed/external tables/view to unity catalog
B.
Upgrade to DBR version 15.0
C.
Copy data from workspace to unity catalog
D.
Upgrade workspace to Unity catalog
E.
Answer: B
Explanation/Reference:
Upgrade tables and views to Unity Catalog - Azure Databricks | Microsoft Docs
Managed table: Upgrade a managed to Unity Catalog
External table: Upgrade an external table to Unity Catalog
Question: 178
What is the top-level object in unity catalog?
Catalog
A.
Table
B.
Workspace
C.
Database
D.
https://www.dumps4less.com/
Metastore
E.
Answer: E
Explanation/Reference:
Key concepts - Azure Databricks | Microsoft Docs
Question: 179
One of the team members Steve who has the ability to create views, created a new view called regional_sales_vw on the
existing table called sales which is owned by John, and the second team member Kevin who works with regional sales
managers wanted to query the data in regional_sales_vw, so Steve granted the permission to Kevin using command
GRANT VIEW, USAGE ON regional_sales_vw to kevin@company.com but Kevin is still unable to access the view?
Kevin needs select access on the table sales
A.
Kevin needs owner access on the view regional_sales_vw
B.
Steve is not the owner of the sales table
C.
Kevin is not the owner of the sales table
D.
Table access control is not enabled on the table and view
E.
Answer: C
Explanation/Reference:
Ownership determines whether or not you can grant privileges on derived objects to other users, since Steve is not the
owner of the underlying sales table, he can not grant access to the table or data in the table indirectly.
Only owner(user or group) can grant access to a object
https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#a-user-has-select-
privileges-on-a-view-of-table-t-but-when-that-user-tries-to-select-from-that-view-they-get-the-error-user-does-not-
have-privilege-select-on-table
https://www.dumps4less.com/
Data object privileges - Azure Databricks | Microsoft Doc
Question: 180
Kevin is the owner of the schema sales, Steve wanted to create new table in sales schema called regional_sales so Kevin
grants the create table permissions to Steve. Steve creates the new table called regional_sales in sales schema, who is
the owner of the table regional_sales
Kevin is the owner of sales schema, all the tables in the schema will be owned by Kevin
A.
Steve is the owner of the table
B.
By default ownership is assigned DBO
C.
By default ownership is assigned to DEFAULT_OWNER
D.
Kevin and Smith both are owners of table
E.
Answer: B
Explanation/Reference:
A user who creates the object becomes its owner, does not matter who is the owner of the parent object.
Question: 181
You were asked to write python code to stop all running streams, which of the following command can be used to get a
list of all active streams currently running so we can stop them, fill in the blank.
for s in _______________:
s.stop()
Spark.getActiveStreams()
A.
spark.streams.active
B.
activeStreams()
C.
getActiveStreams()
D.
spark.streams.getActive
E.
Answer: B
https://www.dumps4less.com/
Question: 182
At the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to
ingest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to
change overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to
command to load the data, fill in the blanks for successful execution of below code.
spark.readStream
.format("cloudfiles")
.option("_______",”csv)
.option("_______", ‘dbfs:/location/checkpoint/’)
.load(data_source)
.writeStream
.option("_______",’ dbfs:/location/checkpoint/’)
.option("_______", "true")
.table(table_name))
format, checkpointlocation, schemalocation, overwrite
A.
cloudfiles.format, checkpointlocation, cloudfiles.schemalocation, overwrite
B.
cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema
C.
cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, append
D.
cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, overwrite
E.
Answer: C
Question: 183
Which of the following scenarios is the best fit for AUTO LOADER?
Efficiently process new data incrementally from cloud object storage
A.
Efficiently move data incrementally from one delta table to another delta table
B.
Incrementally process new data from streaming data sources like Kafka into delta lake
C.
Incrementally process new data from relational databases like MySQL
D.
Efficiently copy data from one data lake location to another data lake location
E.
Answer: A
Explanation/Reference:
The answer is, Efficiently process new data incrementally from cloud object storage, AUTO LOADER only supports
https://www.dumps4less.com/
ingesting files stored in a cloud object storage. Auto Loader cannot process streaming data sources like Kafka or Delta
streams, use Structured streaming for these data sources.
Auto Loader and Cloud Storage Integration
Auto Loader supports a couple of ways to ingest data incrementally
Directory listing - List Directory and maintain the state in RocksDB, supports incremental file listing
File notification - Uses a trigger+queue to store the file notification which can be later used to retrieve the file, unlike
Directory listing File notification can scale up to millions of files per day.
[OPTIONAL]
Auto Loader vs COPY INTO?
Auto Loader
Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional
setup. Auto Loader provides a new Structured Streaming source called cloudFiles. Given an input directory path on the
cloud file storage, the cloudFiles source automatically processes new files as they arrive, with the option of also
processing existing files in that directory.
When to use Auto Loader instead of the COPY INTO?
You want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover
files more efficiently than the COPY INTO SQL command and can split file processing into multiple batches.
You do not plan to load subsets of previously uploaded files. With Auto Loader, it can be more difficult to reprocess
subsets of files. However, you can use the COPY INTO SQL command to reload subsets of files while an Auto Loader
stream is simultaneously running.
Question: 184
You are asked to setup an AUTO LOADER to process the incoming data, this data arrives in JSON format and get dropped
into cloud object storage and you are required to process the data as soon as it arrives in cloud storage, which of the
following statements is correct
AUTO LOADER is native to DELTA lake it cannot support external cloud object storage
A.
AUTO LOADER has to be triggered from an external process when the file arrives in the cloud storage
B.
AUTO LOADER needs to be converted to a Structured stream process
C.
https://www.dumps4less.com/
AUTO LOADER can only process continuous data when stored in DELTA lake
D.
AUTO LOADER can support file notification method so it can process data as it arrives
E.
Answer: E
Explanation/Reference:
Auto Loader supports two modes when ingesting new files from cloud object storage
Directory listing: Auto Loader identifies new files by listing the input directory, and uses a directory polling approach.
File notification: Auto Loader can automatically set up a notification service and queue service that subscribe to file
events from the input directory.
File notification is more efficient and can be used to process the data in real-time as data arrives in cloud object storage.
Choosing between file notification and directory listing modes | Databricks on AWS
Question: 185
What is the main difference between bronze and silver?
Duplicates are removed in bronze, schema is applied in silver
A.
Silver may contain aggregated data
B.
Bronze is raw copy of ingested data, silver contains data with production schema and optimized for ELT/ETL
C.
throughput
Bad data is filtered in Bronze, silver is a copy of bronze data
D.
https://www.dumps4less.com/
Answer: C
Explanation/Reference:
Medallion Architecture – Databricks
Question: 186
What is the main difference between silver and gold?
Silver may contain aggregated data
A.
Gold may contain aggregated data
B.
Data quality checks are applied in gold
C.
Silver is a copy of bronze data
D.
God is a copy of silver data
E.
Answer: B
Explanation/Reference:
Medallion Architecture – Databricks.
https://www.dumps4less.com/
Question: 187
What is the main difference between silver and gold?
Silver optimized to perform ETL, Gold is optimized query performance
A.
Gold is optimized go perform ETL, Silver is optimized for query performance
B.
Silver is copy of Bronze, Gold is a copy of Silver
C.
Silver is stored in Delta Lake, Gold is stored in memory
D.
Silver may contain aggregated data, gold may preserve the granularity of original data
E.
Answer: A
Explanation/Reference:
Medallion Architecture – Databricks
Gold Layer:
https://www.dumps4less.com/
1. Powers Ml applications, reporting, dashboards, ad hoc analytics
2. Refined views of data, typically with aggregations
3. Reduces strain on production systems
4. Optimizes query performance for business-critical data
Question: 188
A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp
EXPECT (timestamp > '2020-01-01')
What is the expected behavior when a batch of data containing data that violates these constraints is processed?
Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.
A.
Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.
B.
Records that violate the expectation cause the job to fail.
C.
Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the
D.
target dataset.
Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table.
E.
Answer: A
Explanation/Reference:
The answer is, Records that violate the expectation are added to the target dataset and recorded as invalid in the event
log.
Delta live tables support three types of expectations to fix bad data in DLT pipelines
https://www.dumps4less.com/
Review below example code to examine these expectations,
Question: 189
A dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp
EXPECT (timestamp > '2020-01-01') ON VIOLATION DROP ROW
What is the expected behavior when a batch of data containing data that violates these constraints is processed?
Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.
A.
Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.
B.
Records that violate the expectation cause the job to fail.
C.
Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the
D.
target dataset.
Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table.
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
The answer is Records that violate the expectation are dropped from the target dataset and recorded as invalid in the
event log.
Delta live tables support three types of expectations to fix bad data in DLT pipelines
Review below example code to examine these expectations,
Question: 190
You are asked to debug a databricks job that is taking too long to run on Sunday’s, what are the steps you are going to
take to see the step that is taking longer to run?
A notebook activity of job run is only visible when using all-purpose cluster.
A.
Under Workflow UI and jobs select job you want to monitor and select the run, notebook activity can be viewed.
B.
Enable debug mode in the Jobs to see the output activity of a job, output should be available to view.
C.
Once a job is launched, you cannot access the job’s notebook activity.
D.
Use the compute’s spark UI to monitor the job activity.
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
The answer is, Under Workflow UI and jobs select job you want to monitor and select the run, notebook activity can be
viewed.
You have the ability to view current active runs or completed runs, once you click the run you can see the
Click on the run to view the notebook output
Question: 191
Your colleague was walking you through how a job was setup, but you noticed a warning message that said, “Jobs
running on all-purpose cluster are considered all purpose compute", the colleague was not sure why he was getting the
warning message, how do you best explain this warning message?
All-purpose clusters cannot be used for Job clusters, due to performance issues.
A.
https://www.dumps4less.com/
All-purpose clusters take longer to start the cluster vs a job cluster
B.
All-purpose clusters are less expensive than the job clusters
C.
All-purpose clusters are more expensive than the job clusters
D.
All-purpose cluster provide interactive messages that can not be viewed in a job
E.
Answer: D
Explanation/Reference:
Warning message:
Pricing for All-purpose clusters are more expensive than the job clusters
AWS pricing(Aug 15th 2022)
https://www.dumps4less.com/
Question: 192
Your team has hundreds of jobs running but it is difficult to track cost of each job run, you are asked to provide a
recommendation on how to monitor and track cost across various workloads
Create jobs in different workspaces, so we can track the cost easily
A.
Use Tags, during job creation so cost can be easily tracked
B.
Use job logs to monitor and track the costs
C.
Use workspace admin reporting
D.
Use a single cluster for all the jobs, so cost can be easily tracked
E.
Answer: B
Explanation/Reference:
The answer is Use Tags, during job creation so cost can be easily tracked
Review below link for more details https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-
aws.html
Here is a view how tags get propagated from pools to clusters and clusters without pools,
https://www.dumps4less.com/
Question: 193
The sales team has asked the Data engineering team to develop a dashboard that shows sales performance for all stores,
but the sales team would like to use the dashboard but would like to select individual store location, which of the
following approaches Data Engineering team can use to build this functionality into the dashboard.
Use query Parameters which then allow user to choose any location
A.
Currently dashboards do not support parameters
B.
Use Databricks REST API to create a dashboard for each location
C.
Use SQL UDF function to filter the data based on the location
D.
Use Dynamic views to filter the data based on the location
E.
Answer: A
Explanation/Reference:
The answer is
Databricks many types of parameters in the dashboard, a drop-down list can be created based on a query that has a
unique list of store locations.
Here is a simple query that takes a parameter for
SELECT * FROM sales WHERE field IN ( {{ Multi Select Parameter }} )
Or
SELECT * FROM sales WHERE field = {{ Single Select Parameter }}
Query parameter types
Text
Number
Dropdown List
Query Based Dropdown List
https://www.dumps4less.com/
Date and Time
Question: 194
You are working on dashboard that takes really lot of time to load in the browser, due to each visualization contains lot of
data to populate, which of the following approaches can be taken to address this issue?
Increase size of the SQL endpoint cluster
A.
Increase the scale of maximum range of SQL endpoint cluster
B.
Use Databricks SQL Query filter to limit the amount of data in each visualization
C.
Remove data from Delta Lake
D.
Use Delta cache to store the intermediate results
E.
Answer: C
Explanation/Reference:
Note*: The question may sound misleading but these are types of questions the exam tries to ask.
A query filter lets you interactively reduce the amount of data shown in a visualization, similar to query parameter but
with a few key differences. A query filter limits data after it has been loaded into your browser. This makes filters ideal for
smaller datasets and environments where query executions are time-consuming, rate-limited, or costly.
This query filter is different from than filter that needs to be applied at the data level, this filter is at the visualization
level so you can toggle how much data you want to see.
SELECT action AS `action::filter`, COUNT(0) AS "actions count"
FROM events
GROUP BY action
When queries have filters you can also apply filters at the dashboard level. Select the Use Dashboard Level Filters
checkbox to apply the filter to all queries.
Dashboard filters
Query filters | Databricks on AWS
Question: 195
One of the queries in the Dashboard takes a long time to refresh, which of the below steps can be taken to identify the
root cause of this issue?
Restart the SQL endpoint
A.
Select the SQL endpoint cluster, spark UI, SQL tab to see the execution plan and time spent in each step
B.
Run optimize and Z ordering
C.
Change the Spot Instance Policy from “Cost optimized” to “Reliability Optimized.”
D.
Use Query History, to view queries and select query, and check query profile to time spent in each step
E.
https://www.dumps4less.com/
Answer: E
Explanation/Reference:
The answer is, Use Query History, to view queries and select query, and check the query profile to see time spent in each
step.
Here is the view of the query profile, for more info use the link
https://docs.microsoft.com/en-us/azure/databricks/sql/admin/query-profile
As you can see here Databricks SQL query profile is much different to Spark UI and provides much more clear information
on how time is being spent on different queries.
Question: 196
A SQL Dashboard was built for the supply chain team to monitor the inventory and product orders, but all of the
timestamps displayed on the dashboards are showing in UTC format, so they requested to change the time zone to the
location of New York. How would you approach resolving this issue?
Move the workspace from Central US zone to East US Zone
A.
Change the timestamp on the delta tables to America/New_York format
B.
Change the spark configuration of SQL endpoint to format the timestamp to America/New_York
C.
Under SQL Admin Console, set the SQL configuration parameter time zone to America/New_York
D.
Add SET Timezone = America/New_York on every of the SQL queries in the dashboard.
E.
Answer: D
Explanation/Reference:
The answer is, Under SQL Admin Console, set the SQL configuration parameter time zone to America/New_York
Here are steps you can take this to configure, so the entire dashboard is changed without changing individual queries
Configure SQL parameters
To configure all warehouses with SQL parameters:
https://www.dumps4less.com/
Click Settings at the bottom of the sidebar and select SQL Admin Console.
Click the SQL Warehouse Settings tab.
In the SQL Configuration Parameters textbox, specify one key-value pair per line. Separate the name of the parameter
from its value using a space. For example, to enable ANSI_MODE:
Similarly, we can add a line in the SQL Configuration parameters
timezone America/New_York
SQL configuration parameters | Databricks on AWS
Question: 197
Which of the following technique can be used to fine-grained access control to rows and columns based on access?
Use Unity catalog to grant access to rows and columns
A.
Row and column access control lists
B.
Use dynamic view functions
C.
Data access control lists
D.
Dynamic Access control lists with Unity Catalog
E.
Answer: C
Explanation/Reference:
The answer is, Use dynamic view functions.
Here is an example that limits access to rows based on the user being part managers group, in the below view if a user is
not a part of the manager's group you can only see rows where the total amount is <= 1000000
Dynamic view function to filter rows
CREATE VIEW sales_redacted AS
SELECT user_id, country, product, total
FROM sales_raw
WHERE CASE WHEN is_member('managers') THEN TRUE ELSE total <= 1000000 END;
Dynamic view function to hide a column
CREATE VIEW sales_redacted AS
https://www.dumps4less.com/
SELECT user_id,
CASE WHEN is_member('auditors') THEN email ELSE 'REDACTED' END AS email,
country,
product,
total
FROM sales_raw
Please review below for more details
https://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#dynamic-view-
functions
Question: 198
Unity catalog helps you manage the below resources in Databricks at account level
Tables
A.
ML Models
B.
Dashboards
C.
Catalogs
D.
All of the above
E.
Answer: E
Question: 199
A newly joined team member John Smith in the Marketing team who currently has access read access to sales tables but
does not have access to delete rows from the table, which of the following commands help you accomplish this?
GRANT USAGE ON TABLE table_name TO john.smith@marketing.com
A.
GRANT DELETE ON TABLE table_name TO john.smith@marketing.com
B.
GRANT DELETE TO TABLE table_name ON john.smith@marketing.com
C.
GRANT MODIFY TO TABLE table_name ON john.smith@marketing.com
D.
GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com
E.
Answer: E
Explanation/Reference:
The answer is GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com
https://www.dumps4less.com/
Below are the list privileges that can be granted to a user or a group,
· SELECT: gives reada access to an object.
· CREATE: gives ability to create an object (for example, a table in a schema).
· MODIFY: gives ability to add, delete, and modify data to or from an object.
· USAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.
· READ_METADATA: gives ability to view an object and its metadata.
· CREATE_NAMED_FUNCTION: gives ability to create a named UDF in an existing catalog or schema.
· MODIFY_CLASSPATH: gives ability to add files to the Spark class path.
· ALL PRIVILEGES: gives all privileges (is translated into all the above privileges
Question: 200
Kevin is the owner of both the sales table and regional_sales_vw view which uses the sales table as underlying source for
the data, and Kevin is looking to grant select privilege on the view regional_sales_vw to one of newly joined team
members Steven. Which of the following is a true statement?
Kevin can not grant access to Steven since he does not have security admin privilege
A.
Kevin although is the owner but does not have ALL PRIVILEGES permission
B.
Kevin can grant access to the view, because he is the owner of the view and the underlying table
C.
Kevin can not grant access to Steven since he does have workspace admin privilege
D.
Steve will also require SELECT access on the underlying table
E.
Answer: C
Explanation/Reference:
The answer is, Kevin can grant access to the view, because he is the owner of the view and the underlying table,
Ownership determines whether or not you can grant privileges on derived objects to other users, a user who creates a
schema, table, view, or function becomes its owner. The owner is granted all privileges and can grant privileges to other
users
Question: 201
How does Lakehouse replace the dependency on using Data lakes and Data warehouses in a Data and Analytics solution?
Open, direct access to data stored in standard data formats.
A.
Supports ACID transactions.
B.
Supports BI and Machine learning workloads
C.
Support for end-to-end streaming and batch workloads
D.
All the above
E.
https://www.dumps4less.com/
Answer: E
Explanation/Reference:
Lakehouse combines the benefits of a data warehouse and data lakes,
Lakehouse = Data Lake + DataWarehouse
Here are some of the major benefits of a lakehouse
Lakehouse = Data Lake + DataWarehouse
https://www.dumps4less.com/
Question: 202
You are currently working on storing data you receive from different customer surveys this data is highly unstructured
and changes over time, why Lakehouse is a better choice compared to a Data warehouse?
Lakehouse supports schema enforcement and evolution, traditional data warehouses lack schema evolution.
A.
Lakehouse supports SQL
B.
Lakehouse supports ACID
C.
Lakehouse enforces data integrity
D.
Lakehouse supports primary and foreign keys like a data warehouse
E.
Answer: A
Question: 203
Which of the following locations hosts the driver and worker nodes of a Databricks-managed cluster?
Data plane
A.
Control plane
B.
Databricks Filesystem
C.
https://www.dumps4less.com/
JDBC data source
D.
Databricks web application
E.
Answer: A
Explanation/Reference:
Question: 204
You have written a notebook to generate a summary data set for reporting, Notebook was scheduled using the job
cluster, but you realized it takes 8 minutes to start the cluster, what feature can be to start the cluster in a timely
fashion?
Setup an additional job to run ahead of the actual job so the cluster is running second job starts
A.
Use the Databricks cluster pools feature to reduce the startup time
B.
Use Databricks Premium edition instead of Databricks standard edition
C.
Pin the cluster in the cluster UI page so it is always available to the jobs
D.
https://www.dumps4less.com/
Disable auto termination so the cluster is always running
E.
Answer: B
Explanation/Reference:
Cluster pools allow us to reserve VM's ahead of time, when a new job cluster is created VM are grabbed from the pool.
Note: when the VM's are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only
billed once VM is allocated to a cluster.
Here is a demo of how to setup and follow some best practices,
https://www.youtube.com/watch?v=FVtITxOabxg&ab_channel=DatabricksAcademy
Question: 205
Which of the following statement is true about Databricks repos?
You can approve the pull request if you are the owner of Databricks repos
A.
A workspace can only have one instance of git integration
B.
Databricks Repos and Notebook versioning are the same features
C.
You cannot create a new branch in Databricks repos
D.
Databricks repos allow you to comment and commit code changes and push them to remote branch
E.
Answer: E
Explanation/Reference:
See below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workdlow.
All the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git
provider like Github or Azure Devops
https://www.dumps4less.com/
Question: 206
Which of the statement is correct about the cluster pools?
Cluster pools allow you to perform load balancing
A.
Cluster pools allow you to create a cluster
B.
Cluster pools allow you to save time when starting a new cluster
C.
Cluster pools are used to share resources among multiple teams
D.
Cluster pools allow you to have all the nodes in the cluster from single physical server rack
E.
Answer: C
Question: 207
Once a Custer is deleted below additional actions needs to performed by the administrator
Remove virtual machines but storage and networking are automatically dropped
A.
Drop storage disks but Virtual machines and networking are automatically dropped
B.
https://www.dumps4less.com/
Remove networking but Virtual machines and storage disks are automatically dropped
C.
Remove logs
D.
No action needs to be performed. All resources are automatically removed.
E.
Answer: E
Question: 208
How does a Delta Lake differ from a traditional data lake?
Delta lake is Datawarehouse service on top of data lake that can provide reliability, security, and performance
A.
Delta lake is a caching layer on top of data lake that can provide reliability, security, and performance
B.
Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and
C.
performance
Delta lake is an open storage format designed to replace flat files with additional capabilities that can provide
D.
reliability, security, and performance
Delta lake is proprietary software designed by Databricks that can provide reliability, security, and performance
E.
Answer: C
Explanation/Reference:
What is Delta?
Delta lake is
· Open source
· Builds up on standard data format
· Optimized for cloud object storage
· Built for scalable metadata handling
Delta lake is not
· Proprietary technology
· Storage format
· Storage medium
· Database service or data warehouse
Question: 210
Drop a DELTA table
DROP DELTA table_name
A.
DROP TABLE table_name
B.
DROP TABLE table_name FORMAT DELTA
C.
DROP table_name
D.
Answer: B
Question: 211
Delete records from the transactions Delta table where transactionDate is greater than current timestamp?
DELETE FROM transactions FORMAT DELTA where transactionDate > currenct_timestmap()
A.
DELETE FROM transactions if transctionDate > current_timestamp()
B.
DELETE FROM transactions where transactionDate > current_timestamp()
C.
DELETE FROM transactions where transactionDate > current_timestamp() KEEP_HISTORY
D.
https://www.dumps4less.com/
DELET FROM transactions where transactionDate GE current_timestamp()
E.
Answer: C
Question: 212
Identify one of the below statements that can query a delta table in PySpark Dataframe API
Spark.read.mode(“delta”).table(“table_name)
A.
Spark.read.table.delta(“table_name)
B.
Spark.read.table(“table_name”)
C.
Spark.read.format(“delta”).LoadTableAs(“table_name”)
D.
Spark.read.format(“delta”).TableAs(“table_name”)
E.
Answer: C
Question: 213
The default threshold of VACUUM is 7 days, internal audit team asked to certain tables to maintain at least 365 days as
part of compliance requirement, which of the below setting is needed to implement.
ALTER TABLE table_name set TBLPROPERTIES (delta.deletedFileRetentionDuration= ‘interval 365 days’)
A.
MODIFY TABLE table_name set TBLPROPERTY (delta.maxRetentionDays = ‘interval 365 days’)
B.
ALTER TABLE table_name set EXENDED TBLPROPERTIES (delta.deletedFileRetentionDuration= ‘interval 365 days’)
C.
ALTER TABLE table_name set EXENDED TBLPROPERTIES (delta.vaccum.duration= ‘interval 365 days’)
D.
Answer: A
Explanation/Reference:
ALTER TABLE table_name SET TBLPROPERTIES ( property_key [ = ] property_val [, ...] )
TBLPROPERTIES allow you to set key-value pairs
Table properties and table options (Databricks SQL) | Databricks on AWS
https://www.dumps4less.com/
Question: 214
Which of the following commands can be used to read a delta table in SQL
Spark.sql(“select * from table_name”)
A.
%sql Select * from table_name
B.
Both A & B
C.
Execute.sql(“select * from table”)
D.
Delta.sql(“select * from table”)
E.
Answer: C

Question: 216
Which of the following SQL statements can be used to update a transactions table, to set a flag on the table from Y to N
MODIFY transactions SET active_flag = 'N' WHERE active_flag = 'Y'
A.
MERGE transactions SET active_flag = 'N' WHERE active_flag = 'Y'
B.
UPDATE transactions SET active_flag = 'N' WHERE active_flag = 'Y'
C.
REPLACE transactions SET active_flag = 'N' WHERE active_flag = 'Y'
D.
Answer: C
Explanation/Reference:
The answer is
UPDATE transactions SET active_flag = 'N' WHERE active_flag = 'Y'
Delta Lake supports UPDATE statements on the delta table, all of the changes as part of the update are ACID compliant.

Question: 221
Which of the following array functions takes input column return unique list of values in an array?
COLLECT_LIST
A.
COLLECT_SET
B.
COLLECT_UNION
C.
ARRAY_INTERSECT
D.
ARRAY_UNION
E.
Answer: B
https://www.dumps4less.com/
Explanation/Reference:
Question: 222
You are looking to process the data based on two variables, one to check if the department is supply chain or check if
process flag is set to True
if department = “supply chain” | process:
A.
if department == “supply chain” or process = TRUE:
B.
if department == “supply chain” | process == TRUE:
C.
if department == “supply chain” | if process == TRUE:
D.
if department == “supply chain” or process:
E.
Answer: E

Question: 224
Which of the following python statements can be used to replace the schema name and table name in the query?
table_name = "sales"schema_name = "bronze"query = f"select * from schema_name.table_name"
A.
table_name = "sales"query = "select * from {schema_name}.{table_name}"
B.
table_name = "sales"query = f"select * from {schema_name}.{table_name}"
C.
table_name = "sales"query = f"select * from + schema_name +"."+table_name"
D.
Answer: C
Explanation/Reference:
The answer is
table_name = "sales"
query = f"select * from {schema_name}.{table_name}"
It is always best to use f strings to replace python variables, rather than using string concatenation.
Question: 225
you are currently working on creating a spark stream process to read and write in for a one-time micro batch, and also
rewrite the existing target table, fill in the blanks to complete the below command sucesfully.
spark.table("source_table")
.writeStream
https://www.dumps4less.com/
.option("____", “dbfs:/location/silver")
.outputMode("____")
.trigger(Once=____)
.table("target_table")
checkpointlocation, complete, True
A.
targetlocation, overwrite, True
B.
checkpointlocation, True, overwrite
C.
checkpointlocation, True, complete
D.
checkpointlocation, overwrite, True
E.
Answer: A
Question: 226
Which of the following data workloads will utilize a gold table as its source?
A job that enriches data by parsing its timestamps into a human-readable format
A.
A job that queries aggregated data that already feeds into a dashboard
B.
A job that ingests raw data from a streaming source into the Lakehouse
C.
A job that aggregates cleaned data to create standard summary statistics
D.
A job that cleans data by removing malformatted records
E.
Answer: B
Explanation/Reference:
The answer is, A job that queries aggregated data that already feeds into a dashboard
The gold layer is used to store aggregated data, which are typically used for dashboards and reporting.
Review the below link for more info
Medallion Architecture – Databricks
Gold Layer:
1. Powers Ml applications, reporting, dashboards, ad hoc analytics
2. Refined views of data, typically with aggregations
3. Reduces strain on production systems
4. Optimizes query performance for business-critical data
https://www.dumps4less.com/
Question: 227
Which of the following programming languages can be used to build a Databricks SQL dashboard?
Python
A.
Scala
B.
SQL
C.
R
D.
All of the above
E.
Answer: C
Explanation/Reference:
The answer is SQL
Question: 228
What are two different modes of DELTA LIVE TABLE Pipelines
Triggered, Incremental
A.
Once, Continuous
B.
Triggered, Continuous
C.
Once, Incremental
D.
Continuous, Incremental
E.
Answer: C
Explanation/Reference:
The answer is Triggered, Continuous
https://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-concepts#--
continuous-and-triggered-pipelines
Triggered pipelines update each table with whatever data is currently available and then stop the cluster running the
pipeline. Delta Live Tables automatically analyzes the dependencies between your tables and starts by computing those
that read from external sources. Tables within the pipeline are updated after their dependent data sources have been
updated.
Continuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run
https://www.dumps4less.com/
until manually stopped. Continuous pipelines require an always-running cluster but ensure that downstream consumers
have the most up-to-date data.
Question: 229
You are working on a process to load external CSV files into a Delta by leveraging the COPY INTO command, but after
running the command for the second time no data was loaded into the table name, why is that?
COPY INTO table_name
FROM 'dbfs:/mnt/raw/*.csv'
FILEFORMAT = CSV
COPY INTO only works one time data load
A.
Run REFRESH TABLE sales before running COPY INTO
B.
COPY INTO did not detect new files after the last load
C.
Use incremental = TRUE option to load new files
D.
COPY INTO does not support incremental load, use AUTO LOADER
E.
Answer: C
Explanation/Reference:
The answer is COPY INTO did not detect new files after the last load,
COPY INTO keeps track of files that were successfully loaded into the table, the next time when the COPY INTO runs it
skips them.
FYI, you can change this behavior by using COPY_OPTIONS 'force'= 'true', when this option is enabled all files in the
path/pattern are loaded.
COPY INTO table_identifier
FROM [ file_location | (SELECT identifier_list FROM file_location) ]
FILEFORMAT = data_source
[FILES = [file_name, ... | PATTERN = 'regex_pattern']
[FORMAT_OPTIONS ('data_source_reader_option' = 'value', ...)]
[COPY_OPTIONS 'force' = ('false'|'true')]
Question: 230
The sales team has asked the Data engineering team to develop a dashboard that shows sales performance for all stores,
but the sales team would like to use the dashboard but would like to select individual store location, which of the
following approaches Data Engineering team can use to build this functionality into the dashboard.
Use query Parameters which then allow user to choose any location
A.
Currently dashboards do not support parameters
B.
Use Databricks REST API to create a dashboard for each location
C.
Use SQL UDF function to filter the data based on the location
D.
https://www.dumps4less.com/
Use Dynamic views to filter the data based on the location
E.
Answer: A
Explanation/Reference:
The answer is
Databricks many types of parameters in the dashboard, a drop-down list can be created based on a query that has a
unique list of store locations.
Here is a simple query that takes a parameter for
SELECT * FROM sales WHERE field IN ( {{ Multi Select Parameter }} )
Or
SELECT * FROM sales WHERE field = {{ Single Select Parameter }}
Query parameter types
Text
Number
Dropdown List
Query Based Dropdown List
Date and Time