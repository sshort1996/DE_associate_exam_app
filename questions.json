[
    {
        "question": null,
        "choices": [],
        "answer": "",
        "explanation": ""
    },
    {
        "question": "\nThe data engineering team noticed that one of the job fails randomly as a result of using spot instances, what feature in\nJobs/Tasks can be used to address this issue so the job is more stable when using spot instances?\n",
        "choices": [
            "Use Databrick REST API to monitor and restart the job",
            "Use Jobs runs, active runs UI section to monitor and restart the job",
            "Add second task and add a check condition to rerun the first task if it fails",
            "Restart the job cluster, job automatically restarts",
            "Add a retry policy to the task"
        ],
        "answer": "Add a retry policy to the task",
        "explanation": "\nThe answer is, Add a retry policy to the task\nTasks in Jobs support Retry Policy, which can be used to retry a failed tasks, especially when using spot instance it is\ncommon to have failed executors or driver.\n"
    },
    {
        "question": "\nWhat is the main difference between AUTO LOADER and COPY INTO?\n",
        "choices": [
            "COPY INTO supports schema evolution.",
            "AUTO LOADER supports schema evolution.",
            "COPY INTO supports file notification when performing incremental loads.",
            "AUTO LOADER supports directory listing when performing incremental loads.",
            "AUTO LOADER Supports file notification when performing incremental loads."
        ],
        "answer": "AUTO LOADER Supports file notification when performing incremental loads.",
        "explanation": "\nAuto loader supports both directory listing and file notification but COPY INTO only supports directory listing.\nAuto loader file notification will automatically set up a notification service and queue service that subscribe to file events\nfrom the input directory in cloud object storage like Azure blob storage or S3. File notification mode is more performant\nand scalable for large input directories or a high volume of files.\nAuto Loader and Cloud Storage Integration\nAuto Loader supports a couple of ways to ingest data incrementally\nDirectory listing - List Directory and maintain the state in RocksDB, supports incremental file listing\nFile notification - Uses a trigger+queue to store the file notification which can be later used to retrieve the file, unlike\nDirectory listing File notification can scale up to millions of files per day.\n[OPTIONAL]\nAuto Loader vs COPY INTO?\nAuto Loader\nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional\nsetup. Auto Loader provides a new Structured Streaming source called cloudFiles. Given an input directory path on the\ncloud file storage, the cloudFiles source automatically processes new files as they arrive, with the option of also\nprocessing existing files in that directory.\nWhen to use Auto Loader instead of the COPY INTO?\nYou want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover\nfiles more efficiently than the COPY INTO SQL command and can split file processing into multiple batches.\nYou do not plan to load subsets of previously uploaded files. With Auto Loader, it can be more difficult to reprocess\nsubsets of files. However, you can use the COPY INTO SQL command to reload subsets of files while an Auto Loader\nstream is simultaneously running.\nAuto loader file notification will automatically set up a notification service and queue service that subscribe to file events\nfrom the input directory in cloud object storage like Azure blob storage or S3. File notification mode is more performant\nhttps://www.dumps4less.com/\nand scalable for large input directories or a high volume of files.\nHere are some additional notes on when to use COPY INTO vs Auto Loader\nWhen to use COPY INTO\nhttps://docs.databricks.com/delta/delta-ingest.html#copy-into-sql-command\nWhen to use Auto Loader\nhttps://docs.databricks.com/delta/delta-ingest.html#auto-loader\n\n"
    },
    {
        "question": "\nWhy does AUTO LOADER require schema location?\n",
        "choices": [
            "Schema location is used to store user provided schema",
            "Schema location is used to identify the schema of target table",
            "AUTO LOADER does not require schema location, because its supports Schema evolution",
            "Schema location is used to store schema inferred by AUTO LOADER",
            "Schema location is used to identify the schema of target table and source table"
        ],
        "answer": "Schema location is used to store schema inferred by AUTO LOADER",
        "explanation": "\nThe answer is, Schema location is used to store schema inferred by AUTO LOADER\nAuto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first. To avoid incurring this\ninference cost at every stream start up, and to be able to provide a stable schema across stream restarts, you must set\nthe option cloudFiles.schemaLocation. Auto Loader creates a hidden directory _schemas at this location to track schema\nchanges to the input data over time\nThe below link contains detailed documentation on different options\nAuto Loader options | Databricks on AWS\n\n"
    },
    {
        "question": "\nWhich of the following statements are incorrect about the lakehouse\n",
        "choices": [
            "Support end-to-end streaming and batch workloads",
            "Supports ACID",
            "Support for diverse data types that can store both structured and unstructured",
            "Supports BI and Machine learning",
            "Storage is coupled with Compute"
        ],
        "answer": "Storage is coupled with Compute",
        "explanation": "\nThe answer is, Storage is coupled with Compute.\nThe question was asking what is the incorrect option, in Lakehouse Storage is decoupled with compute so both can scale\nindependently.\nWhat Is a Lakehouse? - The Databricks Blog\n"
    },
    {
        "question": "\nYou are designing a data model that works for both machine learning using images and Batch ETL/ELT workloads. Which\nof the following features of data lakehouse can help you meet the needs of both workloads?\n",
        "choices": [
            "Data lakehouse requires very little data modeling.",
            "Data lakehouse combines compute and storage for simple governance",
            "Data lakehouse provides autoscaling for compute clusters.",
            "Data lakehouse can store unstructured data and support ACID transactions.",
            "Data lakehouse fully exists in the cloud."
        ],
        "answer": "Data lakehouse can store unstructured data and support ACID transactions.",
        "explanation": "\nThe answer is A data lakehouse stores unstructured data and is ACID-compliant,\n"
    },
    {
        "question": "\nWhich of the following locations in Databricks product architecture hosts jobs/pipelines and queries?\n",
        "choices": [
            "Data plane",
            "Control plane",
            "Databricks Filesystem",
            "JDBC data source",
            "Databricks web application"
        ],
        "answer": "Control plane",
        "explanation": "\nThe answer is Control Plane,\nDatabricks operates most of its services out of a control plane and a data plane, please note serverless features like SQL\nEndpoint and DLT compute use shared compute in Control pane.\nControl Plane: Stored in Databricks Cloud Account\nThe control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands\nand many other workspace configurations are stored in the control plane and encrypted at rest.\nData Plane: Stored in Customer Cloud Account\nThe data plane is managed by your Azure account and is where your data resides. This is also where data is processed.\nYou can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure\naccount to ingest data or for storage.\nHere is the product architecture diagram highlighted where\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are currently working on a notebook that will populate a reporting table for downstream process consumption, this\nprocess needs to run on a schedule every hour. what type of cluster are you going to use to set up this job?\n",
        "choices": [
            "Since it\u2019s just a single job and we need to run every hour, we can use an all-purpose cluster",
            "The job cluster is best suited for this purpose.",
            "Use Azure VM to read and write delta tables in Python",
            "Use delta live table pipeline to run in continuous mode"
        ],
        "answer": "The job cluster is best suited for this purpose.",
        "explanation": "\nThe answer is, The Job cluster is best suited for this purpose.\nSince you don't need to interact with the notebook during the execution especially when it's a scheduled job, job cluster\nmakes sense. Using an all-purpose cluster can be twice as expensive as a job cluster.\nFYI,\nWhen you run a job scheduler with option of creating a new cluster when the job is complete it terminates the cluster.\nYou cannot restart a job cluster.\n"
    },
    {
        "question": "\nWhich of the following developer operations in CI/CD flow can be implemented in Databricks Repos?\n",
        "choices": [
            "Merge when code is committed",
            "Pull request and review process",
            "Trigger Databricks Repos pull API to update the latest version",
            "Create and edit code.",
            "Delete a branch"
        ],
        "answer": "Trigger Databricks Repos pull API to update the latest version",
        "explanation": "\nSee the below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workflow.\nAll the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git\nprovider like Github or Azure DevOps\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are currently working with the second team and both teams are looking to modify the same notebook, you noticed\nthat the second member is copying the notebooks to the personal folder to edit and replace the collaboration notebook,\nwhich notebook feature do you recommend to make the process easier to collaborate.\n",
        "choices": [
            "Databricks notebooks should be copied to a local machine and setup source control locally to version the notebooks",
            "Databricks notebooks support automatic change tracking and versioning",
            "Databricks Notebooks support real-time coauthoring on a single notebook",
            "Databricks notebooks can be exported into dbc archive files and stored in data lake",
            "Databricks notebook can be exported as HTML and imported at a later time"
        ],
        "answer": "Databricks Notebooks support real-time coauthoring on a single notebook",
        "explanation": "\nAnswer is Databricks Notebooks support real-time coauthoring on a single notebook\nEvery change is saved, and a notebook can be changed my multiple users.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are currently working on a project that requires the use of SQL and Python in a given notebook, what would be your\napproach\n",
        "choices": [
            "Create two separate notebooks, one for SQL and the second for Python",
            "A single notebook can support multiple languages, use the magic command to switch between the two.",
            "Use an All-purpose cluster for python, SQL endpoint for SQL",
            "Use job cluster to run python and SQL Endpoint for SQL"
        ],
        "answer": "A single notebook can support multiple languages, use the magic command to switch between the two.",
        "explanation": "\nThe answer is, A single notebook can support multiple languages, use the magic command to switch between the two.\nUse %sql and %python magic commands within the same notebook.\n"
    },
    {
        "question": "\nWhich of the following statements are correct on how Delta Lake implements a lake house?\n",
        "choices": [
            "Delta lake uses a proprietary format to write data, optimized for cloud storage",
            "Using Apache Hadoop on cloud object storage",
            "Delta lake always stores meta data in memory vs storage",
            "Delta lake uses open source, open format, optimized cloud storage and scalable meta data"
        ],
        "answer": "Delta lake uses open source, open format, optimized cloud storage and scalable meta data",
        "explanation": "12\nWhich of the following statements are correct on how Delta Lake implements a lake house?\nDelta lake uses a proprietary format to write data, optimized for cloud storage\nA.\nUsing Apache Hadoop on cloud object storage\nB.\nDelta lake always stores meta data in memory vs storage\nC.\nhttps://www.dumps4less.com/\nDelta lake uses open source, open format, optimized cloud storage and scalable meta data\nD.\nAnswer: D\n\n"
    },
    {
        "question": "\nif you run the command VACUUM transactions retain  hours? What is the outcome of this command?\n",
        "choices": [
            "Command will be successful, but no data is removed",
            "Command will fail if you have an active transaction running",
            "Command will fail, you cannot run the command with retentionDurationcheck enabled",
            "Command will be successful, but historical data will be removed",
            "Command runs successful and compacts all of the data in the table"
        ],
        "answer": "Command will fail, you cannot run the command with retentionDurationcheck enabled",
        "explanation": "\nThe answer is,\nCommand will fail, you cannot run the command with retentionDurationcheck enabled.\nVACUUM [ [db_name.]table_name | path] [RETAIN num HOURS] [DRY RUN]\nRecursively vacuum directories associated with the Delta table and remove data files that are no longer in the latest\nstate of the transaction log for the table and are older than a retention threshold. Default is 7 Days.\nThe reason this check is enabled is because, DELTA is trying to prevent unintentional deletion of history, and also one\nimportant thing to point out is with 0 hours of retention there is a possibility of data loss(see below kb)\nDocumentation in VACUUM https://docs.delta.io/latest/delta-utility.html\nhttps://kb.databricks.com/delta/data-missing-vacuum-parallel-write.html\n"
    },
    {
        "question": "\nYou noticed a colleague is manually copying the data to the backup folder prior to running an update command, incase if\nthe update command did not provide the expected outcome so he can use the backup copy to replace table, which Delta\nLake feature would you recommend simplifying the process?\n",
        "choices": [
            "Use time travel feature to refer old data instead of manually copying",
            "Use DEEP CLONE to clone the table prior to update to make a backup copy",
            "Use SHADOW copy of the table as preferred backup choice",
            "Cloud object storage retains previous version of the file",
            "Cloud object storage automatically backups the data"
        ],
        "answer": "Use time travel feature to refer old data instead of manually copying",
        "explanation": "\nThe answer is, Use time travel feature to refer old data instead of manually copying.\nhttps://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\nSELECT count(*) FROM my_table TIMESTAMP AS OF \"2019-01-01\"\nSELECT count(*) FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)\nSELECT count(*) FROM my_table TIMESTAMP AS OF \"2019-01-01 01:30:00.000\"\n"
    },
    {
        "question": "\nWhich one of the following is not a Databricks lakehouse object?\n",
        "choices": [
            "Tables",
            "Views",
            "Database/Schemas",
            "Catalog",
            "Functions",
            "Stored Procedures"
        ],
        "answer": "Stored Procedures",
        "explanation": "\nThe answer is, Stored Procedures.\nDatabricks lakehouse does not support stored procedures.\n"
    },
    {
        "question": "\nWhat type of table is created when you create delta table with below command?\nCREATE TABLE transactions USING DELTA LOCATION \"DBFS:/mnt/bronze/transactions\"\n",
        "choices": [
            "Managed delta table",
            "External table",
            "Managed table",
            "Temp table",
            "Delta Lake table"
        ],
        "answer": "External table",
        "explanation": "\nAnytime a table is created using the LOCATION keyword it is considered an external table, below is the current syntax.\nSyntax\nCREATE TABLE table_name ( column column_data_type\u2026) USING format LOCATION \"dbfs:/\"\nformat -> DELTA, JSON, CSV, PARQUET, TEXT\n"
    },
    {
        "question": "\nWhich of the following command can be used to drop a managed delta table and the underlying files in the storage?\n",
        "choices": [
            "DROP TABLE table_name CASCADE",
            "DROP TABLE table_name",
            "Use DROP TABLE table_name command and manually delete files using command dbutils.fs.rm(\"/path\",True)",
            "DROP TABLE table_name INCLUDE_FILES",
            "DROP TABLE table and run VACUUM command"
        ],
        "answer": "DROP TABLE table_name",
        "explanation": "\nThe answer is DROP TABLE table_name,\nWhen a managed table is dropped, the table definition is dropped from metastore and everything including data,\nmetadata, and history are also dropped from storage.\n"
    },
    {
        "question": "\nWhich of the following is the correct statement for a session scoped temporary view?\n",
        "choices": [
            "Temporary views are lost once the notebook is detached and re-attached",
            "Temporary views stored in memory",
            "Temporary views can be still accessed even if the notebook is detached and attached",
            "Temporary views can be still accessed even if cluster is restarted",
            "Temporary views are created in local_temp database"
        ],
        "answer": "Temporary views are lost once the notebook is detached and re-attached",
        "explanation": "\nThe answer is Temporary views are lost once the notebook is detached and attached\nThere are two types of temporary views that can be created, Session scoped and Global\nA local/session scoped temporary view is only available with a spark session, so another notebook in the same cluster can\nnot access it. if a notebook is detached and reattached local temporary view is lost.\nA global temporary view is available to all the notebooks in the cluster, if a cluster restarts global temporary view is lost.\n"
    },
    {
        "question": "\nWhich of the following is correct for the global temporary view?\n",
        "choices": [
            "global temporary views cannot be accessed once the notebook is detached and attached",
            "global temporary views can be accessed across many clusters",
            "global temporary views can be still accessed even if the notebook is detached and attached",
            "global temporary views can be still accessed even if the cluster is restarted",
            "global temporary views are created in a database called temp database"
        ],
        "answer": "global temporary views can be still accessed even if the notebook is detached and attached",
        "explanation": "\nThe answer is global temporary views can be still accessed even if the notebook is detached and attached\nThere are two types of temporary views that can be created Local and Global\n\u00b7 A local temporary view is only available with a spark session, so another notebook in the same cluster can not access it.\nif a notebook is detached and reattached local temporary view is lost.\n\u00b7 A global temporary view is available to all the notebooks in the cluster, even if the notebook is detached and reattached\nit can still be accessible but if a cluster is restarted the global temporary view is lost.\n"
    },
    {
        "question": "\nYou are currently working on reloading customer_sales tables using the below query\nINSERT OVERWRITE customer_sales\nSELECT * FROM customers c\nINNER JOIN sales_monthly s on s.customer_id = c.customer_id\nAfter you ran the above command, the Marketing team quickly wanted to review the old data that was in the table. How\ndoes INSERT OVERWRITE impact the data in the customer_sales table if you want to see the previous version of the data\nprior to running the above statement?\n",
        "choices": [
            "Overwrites the data in the table, all historical versions of the data, you can not time travel to previous versions",
            "Overwrites the data in the table but preserves all historical versions of the data, you can time travel to previous",
            "Overwrites the current version of the data but clears all historical versions of the data, so you can not time travel to",
            "Appends the data to the current version, you can time travel to previous versions",
            "By default, overwrites the data and schema, you cannot perform time travel"
        ],
        "answer": "Overwrites the data in the table but preserves all historical versions of the data, you can time travel to previous",
        "explanation": "\nThe answer is, INSERT OVERWRITE Overwrites the current version of the data but preserves all historical versions of the\ndata, you can time travel to previous versions.\nINSERT OVERWRITE customer_sales\nSELECT * FROM customers c\nINNER JOIN sales s on s.customer_id = c.customer_id\nLet's just assume that this is the second time you are running the above statement, you can still query the prior version\nof the data using time travel, and any DML/DDL except DROP TABLE creates new PARQUET files so you can still access\nthe previous versions of data.\nSQL Syntax for Time travel\nSELECT * FROM table_name as of [version number]\nwith customer_sales example\nSELECT * FROM customer_sales as of 1 -- previous version\nSELECT * FROM customer_sales as of 2 -- current version\nYou see all historical changes on the table using DESCRIBE HISTORY table_name\nNote: the main difference between INSERT OVERWRITE and CREATE OR REPLACE TABLE(CRAS) is that CRAS can modify\nthe schema of the table, i.e it can add new columns or change data types of existing columns. By default INSERT\nOVERWRITE only overwrites the data.\nINSERT OVERWRITE can also be used to update the schema when spark.databricks.delta.schema.autoMerge.enabled is\nset true if this option is not enabled and if there is a schema mismatch command INSERT OVERWRITEwill fail.\nAny DML/DDL operation(except DROP TABLE) on the Delta table preserves the historical version of the data.\n"
    },
    {
        "question": "\nWhich of the following SQL statement can be used to query a table by eliminating duplicate rows from the query results?\n",
        "choices": [
            "SELECT DISTINCT * FROM table_name",
            "SELECT DISTINCT * FROM table_name HAVING COUNT(*) > 1",
            "SELECT DISTINCT_ROWS (*) FROM table_name",
            "SELECT * FROM table_name GROUP BY * HAVING COUNT(*) < 1",
            "SELECT * FROM table_name GROUP BY * HAVING COUNT(*) > 1"
        ],
        "answer": "SELECT DISTINCT * FROM table_name",
        "explanation": "\nThe answer is SELECT DISTINCT * FROM table_name\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the below SQL Statements can be used to create a SQL UDF to convert Celsius to Fahrenheit and vice versa, you\nneed to pass two parameters to this function one, actual temperature, and the second that identifies if its needs to be\nconverted to Fahrenheit or Celcius with a one-word letter F or C?\nselect udf_convert(,'C') will result in .\nselect udf_convert(,'F') will result in \n",
        "choices": [
            "CREATE UDF FUNCTION udf_convert(temp DOUBLE, measure STRING) RETURNS DOUBLE RETURN CASE WHEN",
            "CREATE UDF FUNCTION udf_convert(temp DOUBLE, measure STRING) RETURN CASE WHEN measure == 'F' then",
            "CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)RETURN CASE WHEN measure == 'F' then (temp *",
            "CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)RETURNS DOUBLERETURN CASE WHEN measure ==",
            "CREATE USER FUNCTION udf_convert(temp DOUBLE, measure STRING)RETURNS DOUBLERETURN CASE WHEN"
        ],
        "answer": "CREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)RETURNS DOUBLERETURN CASE WHEN measure ==",
        "explanation": "\nThe answer is\nCREATE FUNCTION udf_convert(temp DOUBLE, measure STRING)\nRETURNS DOUBLE\nRETURN CASE WHEN measure == \u2018F\u2019 then (temp * 9/5) + 32\nELSE (temp \u2013 33 ) * 5/9\nEND\n"
    },
    {
        "question": "\nYou are trying to parse a complex JSON object below, and calculate total sales made by all the employees, how would you\napproach this in SQL\nid INT, performance ARRAY, insertDate TIMESTAMP\nSample data of performance column\nSample data of performance column\n[\n{ \"employeeId\":\n\"sales\" : . },\n{ \"employeeId\":\n\"sales\" : . }\n]\n",
        "choices": [
            "WITH CTE as (SELECT EXPLODE (performance) FROM table_name)SELECT SUM (performance.sales) FROM CTE",
            "WITH CTE as (SELECT FLATTEN (performance) FROM table_name)SELECT SUM (sales) FROM CTE",
            "SELECT SUM (performance.sales) FROM employee",
            "SELECT SUM (SLICE (performance, sales)) FROM employee",
            "SELECT SUM (ELEMENT_AT (performance, sales)) FROM employee"
        ],
        "answer": "SELECT SUM (performance.sales) FROM employee",
        "explanation": "\nThe answer is SELECT SUM (performance.sales) FROM employee\nNested JSON can be queried using the . notation performance.sales will give you access to all the sales values in the JSON\nstructure.\n"
    },
    {
        "question": "\nWhich of the following statements can be used to test the functionality of code to test number of rows in the table equal\nto  in python?\nrow_count = spark.sql(\"select count(*) from table\").collect()[][]\nassert (row_count = , \"Row count did not match\")\nA.\nassert if (row_count = , \"Row count did not match\")\nB.\nassert row_count == , \"Row count did not match\"\nC.\nassert if row_count == , \"Row count did not match\"\nD.\nassert row_count = , \"Row count did not match\"\nE.\nAnswer: C\nExplanation/Reference:\nThe answer is assert row_count == , \"Row count did not match\"\nReview below documentation\nAssert Python\n",
        "choices": [
            "assert (row_count = 10, \"Row count did not match\")",
            "assert if (row_count = 10, \"Row count did not match\")",
            "assert row_count == 10, \"Row count did not match\"",
            "assert if row_count == 10, \"Row count did not match\"",
            "assert row_count = 10, \"Row count did not match\""
        ],
        "answer": "assert row_count == 10, \"Row count did not match\"",
        "explanation": "\nThe answer is assert row_count == 10, \"Row count did not match\"\nReview below documentation\nAssert Python\n"
    },
    {
        "question": "\nHow do you handle failures gracefully when writing code in Pyspark, fill in the blanks to complete the below statement\n_____\nSpark.read.table(\"table_name\").select(\"column\").write.mode(\"append\").SaveAsTable(\"new_table_name\")\n_____\nprint(f\"query failed\")\n",
        "choices": [
            "try: failure:",
            "try: catch:",
            "try: except:",
            "try: fail:",
            "try: error:"
        ],
        "answer": "try: except:",
        "explanation": "\nThe answer is try: and except:\n"
    },
    {
        "question": "\nYou are working on a process to query the table based on batch date, and batch date is an input parameter and expected\nto change every time the program runs, what is the best way to we can parameterize the query to run without manually\nchanging the batch date?\n",
        "choices": [
            "Create a notebook parameter for batch date and assign the value to a python variable and use a spark data frame to",
            "Create a dynamic view that can calculate the batch date automatically and use the view to query the data",
            "There is no way we can combine python variable and spark code",
            "Manually edit code every time to change the batch date",
            "Store the batch date in the spark configuration and use a spark data frame to filter the data based on the spark"
        ],
        "answer": "Create a notebook parameter for batch date and assign the value to a python variable and use a spark data frame to",
        "explanation": "\nThe answer is, Create a notebook parameter for batch date and assign the value to a python variable and use a spark\ndata frame to filter the data based on the python variable\n"
    },
    {
        "question": "\nWhich of the following commands results in the successful creation of a view on top of the streaming data frame?\n",
        "choices": [
            "Spark.read.table(\"sales\").createOrReplaceTempView(\"streaming_vw\")",
            "Spark.readStream.table(\"sales\").createOrReplaceTempView(\"streaming_vw\")",
            "Spark.read.table(\"sales\").mode(\"stream\").createOrReplaceTempView(\"streaming_vw\")",
            "Spark.read.table(\"sales\").trigger(\"stream\").createOrReplaceTempView(\"streaming_vw\")",
            "Spark.read.stream(\"sales\").createOrReplaceTempView(\"streaming_vw\")"
        ],
        "answer": "Spark.readStream.table(\"sales\").createOrReplaceTempView(\"streaming_vw\")",
        "explanation": "\nThe answer is\nSpark.readStream.table(\"sales\").createOrReplaceTempView(\"streaming_vw\")\n"
    },
    {
        "question": "\nWhich of the following techniques structured streaming uses to create an end-to-end fault tolerance?\n",
        "choices": [
            "Checkpointing and Water marking",
            "Checkpointing and Water marking",
            "Checkpointing and idempotent sinks",
            "Write ahead logging and idempotent sinks",
            "Stream will failover to available nodes in the cluste"
        ],
        "answer": "Checkpointing and idempotent sinks",
        "explanation": "\nThe answer is Checkpointing and idempotent sinks\nHow does structured streaming achieves end to end fault tolerance:\nFirst, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed\nduring each trigger interval.\nNext, the streaming sinks are designed to be _idempotent_\u2014that is, multiple writes of the same data (as identified by the\noffset) do not result in duplicates being written to the sink.\nTaken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure end-to-end, exactly-\nonce semantics under any failure condition.\n"
    },
    {
        "question": "\nWhich of the following two options are supported in identifying the arrival of new files, and incremental data from Cloud\nobject storage using Auto Loader?\n",
        "choices": [
            "Directory listing, File notification",
            "Checking pointing, watermarking",
            "Writing ahead logging, read head logging",
            "File hashing, Dynamic file lookup",
            "Checkpointing and Write ahead logging"
        ],
        "answer": "Directory listing, File notification",
        "explanation": "\nThe answer is A, Directory listing, File notifications\nDirectory listing: Auto Loader identifies new files by listing the input directory.\nFile notification: Auto Loader can automatically set up a notification service and queue service that subscribe to file\nevents from the input directory.\nChoosing between file notification and directory listing modes | Databricks on AWS\n"
    },
    {
        "question": "\nWhich of the following data workloads will utilize a Bronze table as its destination?\n",
        "choices": [
            "A job that aggregates cleaned data to create standard summary statistics",
            "A job that queries aggregated data to publish key insights into a dashboard",
            "A job that ingests raw data from a streaming source into the Lakehouse",
            "A job that develops a feature set for a machine learning application",
            "A job that enriches data by parsing its timestamps into a human-readable format"
        ],
        "answer": "A job that ingests raw data from a streaming source into the Lakehouse",
        "explanation": "\nThe answer is A job that ingests raw data from a streaming source into the Lakehouse.\nThe ingested data from the raw streaming data source like Kafka is first stored in the Bronze layer as first destination\nbefore it is further optimized and stored in Silver.\nMedallion Architecture \u2013 Databricks\nBronze Layer:\n1. Raw copy of ingested data\n2. Replaces traditional data lake\n3. Provides efficient storage and querying of full, unprocessed history of data\n4. No schema is applied at this layer\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the following data workloads will utilize a silver table as its source?\n",
        "choices": [
            "A job that enriches data by parsing its timestamps into a human-readable format",
            "A job that queries aggregated data that already feeds into a dashboard",
            "A job that ingests raw data from a streaming source into the Lakehouse",
            "A job that aggregates cleaned data to create standard summary statistics",
            "A job that cleans data by removing malformatted records"
        ],
        "answer": "A job that aggregates cleaned data to create standard summary statistics",
        "explanation": "\nThe answer is, A job that aggregates cleaned data to create standard summary statistics\nSilver zone maintains the grain of the original data, in this scenario a job is taking data from the silver zone as the source\nand aggregating and storing them in the gold zone.\nMedallion Architecture \u2013 Databricks\nSilver Layer:\n1. Reduces data storage complexity, latency, and redundency\n2. Optimizes ETL throughput and analytic query performance\n3. Preserves grain of original data (without aggregation)\n4. Eliminates duplicate records\n5. production schema enforced\n6. Data quality checks, quarantine corrupt data\n"
    },
    {
        "question": "\nWhich of the following data workloads will utilize a gold table as its source?\n",
        "choices": [
            "A job that enriches data by parsing its timestamps into a human-readable format",
            "A job that queries aggregated data that already feeds into a dashboard",
            "A job that ingests raw data from a streaming source into the Lakehouse",
            "A job that aggregates cleaned data to create standard summary statistics",
            "A job that cleans data by removing malformatted records"
        ],
        "answer": "A job that queries aggregated data that already feeds into a dashboard",
        "explanation": "\nThe answer is, A job that queries aggregated data that already feeds into a dashboard\nThe gold layer is used to store aggregated data, which are typically used for dashboards and reporting.\nReview the below link for more info\nMedallion Architecture \u2013 Databricks\nGold Layer:\n1. Powers Ml applications, reporting, dashboards, ad hoc analytics\n2. Refined views of data, typically with aggregations\n3. Reduces strain on production systems\n4. Optimizes query performance for business-critical data\n"
    },
    {
        "question": "\nYou are currently asked to work on building a data pipeline, you have noticed that you are currently working with data\nsource with a lot of data quality issues and you need to monitor data quality and enforce it as part of the data ingestion\nprocess, which of the following tools can be used to address this problem?\n",
        "choices": [
            "AUTO LOADER",
            "DELTA LIVE TABLES",
            "JOBS and TASKS",
            "UNITY Catalog and Data Governance",
            "STRUCTURED STREAMING with MULTI HOP"
        ],
        "answer": "DELTA LIVE TABLES",
        "explanation": "\nThe answer is, DELTA LIVE TABLES\nDelta live tables expectations can be used to identify and quarantine bad data, all of the data quality metrics are stored\nin the event logs which can be used to later analyze and monitor.\nDELTA LIVE Tables expectations\nBelow are three types of expectations, make sure to pay attention differences between these three.\nRetain invalid records:\nUse the expect operator when you want to keep records that violate the expectation. Records that violate the\nexpectation are added to the target dataset along with valid records:\nPython\n@dlt.expect(\"valid timestamp\", \"col(\u201ctimestamp\u201d) > '2012-01-01'\")\nSQL\nCONSTRAINT valid_timestamp EXPECT (timestamp > '2012-01-01')\nDrop invalid records:\nUse the expect or drop operator to prevent the processing of invalid records. Records that violate the expectation are\ndropped from the target dataset:\nPython\n@dlt.expect_or_drop(\"valid_current_page\", \"current_page_id IS NOT NULL AND current_page_title IS NOT NULL\")\nSQL\nCONSTRAINT valid_current_page EXPECT (current_page_id IS NOT NULL and current_page_title IS NOT NULL) ON\nVIOLATION DROP ROW\nFail on invalid records:\nhttps://www.dumps4less.com/\nWhen invalid records are unacceptable, use the expect or fail operator to halt execution immediately when a record fails\nvalidation. If the operation is a table update, the system atomically rolls back the transaction:\nPython\n@dlt.expect_or_fail(\"valid_count\", \"count > 0\")\nSQL\nCONSTRAINT valid_count EXPECT (count > 0) ON VIOLATION FAIL UPDATE\n"
    },
    {
        "question": "\nWhat is the main difference between CREATE STREAMING LIVE TABLE vs CREATE LIVE TABLE?\n",
        "choices": [
            "CREATE STREAMING LIVE table is used in MULTI HOP Architecture",
            "CREATE LIVE TABLE is used when working with Streaming data sources and Incremental data",
            "CREATE STREAMING LIVE TABLE is used when working with Streaming data sources and Incremental data",
            "There is no difference both are the same, CREATE STRAMING LIVE will be deprecated soon",
            "CREATE LIVE TABLE is used in DELTA LIVE TABLES, CREATE STREAMING LIVE can only used in Structured Streaming"
        ],
        "answer": "CREATE STREAMING LIVE TABLE is used when working with Streaming data sources and Incremental data",
        "explanation": "\nThe answer is, CREATE STREAMING LIVE TABLE is used when working with Streaming data sources and Incremental data\n"
    },
    {
        "question": "\nA particular job seems to be performing slower and slower over time, the team thinks this started to happen when a\nrecent production change was implemented, you were asked to take look at the job history and see if we can identify\ntrends and root cause, where in the workspace UI can you perform this analysis?\nUnder jobs UI select the job you are interested, under runs we can see current active runs and last  days historical\nA.\nrun\nUnder jobs UI select the job cluster, under spark UI select the application job logs, then you can access last  day\nB.\nhistorical runs\nUnder Workspace logs, select job logs and select the job you want to monitor to view the last  day historical runs\nC.\nUnder Compute UI, select Job cluster and select the job cluster to see last  day historical runs\nD.\nHistorical job runs can only be accessed by REST API\nE.\nAnswer: A\nhttps://www.dumpsless.com/\nExplanation/Reference:\nThe answer is,\nUnder jobs UI select the job you are interested, under runs we can see current active runs and last  days historical run\n",
        "choices": [
            "Under jobs UI select the job you are interested, under runs we can see current active runs and last 60 days historical",
            "Under jobs UI select the job cluster, under spark UI select the application job logs, then you can access last 60 day",
            "Under Workspace logs, select job logs and select the job you want to monitor to view the last 60 day historical runs",
            "Under Compute UI, select Job cluster and select the job cluster to see last 60 day historical runs",
            "Historical job runs can only be accessed by REST API"
        ],
        "answer": "Under jobs UI select the job you are interested, under runs we can see current active runs and last 60 days historical",
        "explanation": "\nThe answer is,\nUnder jobs UI select the job you are interested, under runs we can see current active runs and last 60 days historical run\n"
    },
    {
        "question": "\nWhat are the different ways you can schedule a job in Databricks workspace?\n",
        "choices": [
            "Continuous, Incremental",
            "On-Demand runs, File notification from Cloud object storage",
            "Cron, On Demand runs",
            "Cron, File notification from Cloud object storage",
            "Once, Continuous"
        ],
        "answer": "Cron, On Demand runs",
        "explanation": "\nThe answer is, Cron, On-Demand runs\nSupports running job immediately or using can be scheduled using CRON syntax\nJobs in Databricks\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou have noticed that Databricks SQL queries are running slow, you are asked to look reason why queries are running\nslow and identify steps to improve the performance, when you looked at the issue you noticed all the queries are running\nin parallel and using a SQL endpoint cluster. Which of the following steps can be taken to resolve the issue?\n",
        "choices": [
            "They can turn on the Serverless feature for the SQL endpoint.",
            "They can increase the maximum bound of the SQL endpoint\u2019s scaling range.",
            "They can increase the cluster size of the SQL endpoint.",
            "They can turn on the Auto Stop feature for the SQL endpoint.",
            "They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to \u201cReliability"
        ],
        "answer": "They can increase the maximum bound of the SQL endpoint\u2019s scaling range.",
        "explanation": "\nThe answer is, They can increase the maximum bound of the SQL endpoint\u2019s scaling range.\nSQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.\nScale-out -> to add more clusters for a SQL endpoint, change max number of clusters\nIf you are trying to improve the throughput, being able to run as many queries as possible then having an additional\ncluster(s) will improve the performance.\nScale-up-> Increase the size of the SQL endpoint, change cluster size from x-small to small, to medium, X Large....\nIf you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the\ncluster will improve the performance.\n"
    },
    {
        "question": "\nYou currently working with marketing team to setup a dashboard on ad campaign analysis, since the team is not sure\nhow often the dashboard should refreshed they have decided to do a manual refresh on as needed basis. Which of the\nfollowing steps can be taken to reduce the overall cost of the compute?\n",
        "choices": [
            "They can turn on the Serverless feature for the SQL endpoint.",
            "They can decrease the maximum bound of the SQL endpoint\u2019s scaling range.",
            "They can decrease the cluster size of the SQL endpoint.",
            "They can turn on the Auto Stop feature for the SQL endpoint.",
            "They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from \u201cReliability"
        ],
        "answer": "They can turn on the Auto Stop feature for the SQL endpoint.",
        "explanation": "\nThe answer is, They can turn on the Auto Stop feature for the SQL endpoint.\nUse auto stop to automatically terminate the cluster when you are not using it.\n"
    },
    {
        "question": "\nYou had worked with the Data analysts team to set up a SQL Endpoint point so they can easily query and analze data in\nthe gold layer, but once they started consuming the SQL Endpoint you noticed that during the peak hours as the number\nof users increase you are seeing a degradation in the query performance, which of the following steps can be taken to\nresolve the issue?\n",
        "choices": [
            "They can turn on the Serverless feature for the SQL endpoint.",
            "They can increase the maximum bound of the SQL endpoint\u2019s scaling range.",
            "They can increase the cluster size of the SQL endpoint.",
            "They can turn on the Auto Stop feature for the SQL endpoint.",
            "They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from \u201cCost"
        ],
        "answer": "They can increase the maximum bound of the SQL endpoint\u2019s scaling range.",
        "explanation": "\nthe answer is,\nThey can increase the maximum bound of the SQL endpoint\u2019s scaling range.\nSQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.\nScale-out -> to add more clusters for a SQL endpoint, change max number of clusters\nIf you are trying to improve the throughput, being able to run as many queries as possible then having an additional\ncluster(s) will improve the performance.\nScale-up-> Increase the size of the SQL endpoint, change cluster size from x-small to small, to medium, X Large.\nIf you are trying to improve the performance of a single query having additional memory, additional nodes, and cpu in\nthe cluster will improve the performance\n"
    },
    {
        "question": "\nThe research team has put together a funnel analysis query to monitor the customer traffic on the e-commerce platform,\nthe query takes about  mins to run on a small SQL endpoint cluster with max scaling set to  cluster.\n",
        "choices": [
            "They can turn on the Serverless feature for the SQL endpoint.",
            "They can increase the maximum bound of the SQL endpoint\u2019s scaling range anywhere from between 1 to 100 to",
            "They can increase the cluster size anywhere from X small to 3XL to review the performance and select the size that",
            "They can turn off the Auto Stop feature for the SQL endpoint to more than 30 mins.",
            "They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from \u201cCost"
        ],
        "answer": "They can increase the cluster size anywhere from X small to 3XL to review the performance and select the size that",
        "explanation": "\nThe answer is, They can increase the cluster size anywhere from small to 3XL to review the performance and select the\nsize that meets your SLA.\nSQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.\nScale-out -> to add more clusters for a SQL endpoint, change max number of clusters\nIf you are trying to improve the throughput, being able to run as many queries as possible then having an additional\ncluster(s) will improve the performance.\nScale-up-> Increase the size of the SQL endpoint, change cluster size from x-small to small, to medium, X Large....\nIf you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the\ncluster will improve the performance.\n"
    },
    {
        "question": "\nUnity catalog simplifies managing multiple workspaces, by storing and managing permissions and ACL at _______ level\n",
        "choices": [
            "Workspace",
            "Account",
            "Storage",
            "Data pane",
            "Control pane"
        ],
        "answer": "Account",
        "explanation": "\nThe answer is, Account Level\nThe classic access control list (tables, workspace, cluster) is at the workspace level, Unity catalog is at the account level\nand can manage all the workspaces in an Account.\n"
    },
    {
        "question": "\nWhich of the following section in the UI can be used to manage permissions and grants to tables?\n",
        "choices": [
            "User Settings",
            "Admin UI",
            "Workspace admin settings",
            "User access control lists",
            "Data Explorer"
        ],
        "answer": "Data Explorer",
        "explanation": "\nThe answer is Data Explorer\nData explorer\n"
    },
    {
        "question": "\nWhich of the following is not a privilege in the Unity catalog?\n",
        "choices": [
            "SELECT",
            "MODIFY",
            "EXECUTE",
            "CREATE",
            "USAGE"
        ],
        "answer": "EXECUTE",
        "explanation": "\nThe Answer is EXECUTE,\nHere are the list of all privileges\nPrivileges\nSELECT: gives read access to an object.\nCREATE: gives ability to create an object (for example, a table in a schema).\nMODIFY: gives ability to add, delete, and modify data to or from an object.\nUSAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.\nREAD_METADATA: gives ability to view an object and its metadata.\nCREATE_NAMED_FUNCTION: gives ability to create a named UDF in an existing catalog or schema.\nMODIFY_CLASSPATH: gives ability to add files to the Spark class path.\nALL PRIVILEGES: gives all privileges (is translated into all the above privileges).\nPrivileges\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nA team member is leaving the team and he/she is currently the owner of the few tables, instead of transfering the\nownership to a user you have decided to transfer the ownership to a group so in the future anyone in the group can\nmanage the permissions rather than a single individual, which of the following commands help you accomplish this?\n",
        "choices": [
            "ALTER TABLE table_name OWNER to 'group'",
            "TRANSFER OWNER table_name to 'group'",
            "GRANT OWNER table_name to 'group'",
            "ALTER OWNER ON table_name to 'group'",
            "GRANT OWNER On table_name to 'group'"
        ],
        "answer": "ALTER TABLE table_name OWNER to 'group'",
        "explanation": "\nThe answer is ALTER TABLE table_name OWNER to \u2018group\u2019\nAssign owner to object\n"
    },
    {
        "question": "\nWhat is the best way to describe a data lakehouse compared to a data warehouse?\n",
        "choices": [
            "A data lakehouse provides a relational system of data management",
            "A data lakehouse captures snapshots of data for version control purposes.",
            "A data lakehouse couples storage and compute for complete control.",
            "A data lakehouse utilizes proprietary storage formats for data.",
            "A data lakehouse enables both batch and streaming analytics."
        ],
        "answer": "A data lakehouse enables both batch and streaming analytics.",
        "explanation": "\nAnser is A data lakehouse enables both batch and streaming analytics\nA lakehouse has the following key features:\nTransaction support: In an enterprise lakehouse many data pipelines will often be reading and writing data concurrently.\nSupport for ACID transactions ensures consistency as multiple parties concurrently read or write data, typically using\nSQL.\nSchema enforcement and governance: The Lakehouse should have a way to support schema enforcement and evolution,\nsupporting DW schema architectures such as star/snowflake-schemas. The system should be able to reason about data\nhttps://www.dumps4less.com/\nintegrity, and it should have robust governance and auditing mechanisms.\nBI support: Lakehouses enable using BI tools directly on the source data. This reduces staleness and improves recency,\nreduces latency, and lowers the cost of having to operationalize two copies of the data in both a data lake and a\nwarehouse.\nStorage is decoupled from compute: In practice this means storage and compute use separate clusters, thus these\nsystems are able to scale to many more concurrent users and larger data sizes. Some modern data warehouses also\nhave this property.\nOpenness: The storage formats they use are open and standardized, such as Parquet, and they provide an API so a\nvariety of tools and engines, including machine learning and Python/R libraries, can efficiently access the data directly.\nSupport for diverse data types ranging from unstructured to structured data: The lakehouse can be used to store, refine,\nanalyze, and access data types needed for many new data applications, including images, video, audio, semi-structured\ndata, and text.\nSupport for diverse workloads: including data science, machine learning, and SQL and analytics. Multiple tools might be\nneeded to support all these workloads but they all rely on the same data repository.\nEnd-to-end streaming: Real-time reports are the norm in many enterprises. Support for streaming eliminates the need for\nseparate systems dedicated to serving real-time data applications.\n"
    },
    {
        "question": "\nYou are designing an analytical to store structured data from your e-commerce platform and unstructured data from\nwebsite traffic and app store, how would you approach where you store this data?\n",
        "choices": [
            "Use traditional data warehouse for structured data and use data lakehouse for unstructured data.",
            "Data lakehouse can only store unstructured data but cannot enforce a schema",
            "Data lakehouse can store structured and unstructured data and can enforce schema",
            "Traditional data warehouses are good for storing structured data and enforcing schema"
        ],
        "answer": "Data lakehouse can store structured and unstructured data and can enforce schema",
        "explanation": "\nThe answer is, Data lakehouse can store structured and unstructured data and can enforce schema\nWhat Is a Lakehouse? - The Databricks Blog\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are currently working on a production job failure with a job set up in job clusters due to a data issue, what cluster do\nyou need to start to investigate and analyze the data?\n",
        "choices": [
            "A Job cluster can be used to analyze the problem",
            "All-purpose cluster/ interactive cluster is the recommended way to run commands and view the data.",
            "Existing job cluster can be used to investigate the issue",
            "Databricks SQL Endpoint can be used to investigate the issue"
        ],
        "answer": "All-purpose cluster/ interactive cluster is the recommended way to run commands and view the data.",
        "explanation": "\nAnswer is All-purpose cluster/ interactive cluster is the recommended way to run commands and view the data.\nA job cluster can not provide a way for a user to interact with a notebook once the job is submitted, but an Interactive\ncluster allows to you display data, view visualizations write or edit quries, which makes it a perfect fit to investigate and\nanalyze the data.\n"
    },
    {
        "question": "\nWhich of the following describes how Databricks Repos can help facilitate CI/CD workflows on the Databricks Lakehouse\nPlatform?\n",
        "choices": [
            "Databricks Repos can facilitate the pull request, review, and approval process before merging branches",
            "Databricks Repos can merge changes from a secondary Git branch into a main Git branch",
            "Databricks Repos can be used to design, develop, and trigger Git automation pipelines",
            "Databricks Repos can store the single-source-of-truth Git repository",
            "Databricks Repos can commit or push code changes to trigger a CI/CD process"
        ],
        "answer": "Databricks Repos can commit or push code changes to trigger a CI/CD process",
        "explanation": "\nAnswer is Databricks Repos can commit or push code changes to trigger a CI/CD process\nSee below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workdlow.\nAll the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git\nprovider like Github or Azure Devops.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou noticed that colleague is manually copying the notebook with _bkp to store the previous versions, which of the\nfollowing feature would you recommend instead.\n",
        "choices": [
            "Databricks notebooks support change tracking and versioning",
            "Databricks notebooks should be copied to a local machine and setup source control locally to version the notebooks",
            "Databricks notebooks can be exported into dbc archive files and stored in data lake",
            "Databricks notebook can be exported as HTML and imported at a later time"
        ],
        "answer": "Databricks notebooks support change tracking and versioning",
        "explanation": "\nAnswer is Databricks notebooks support automatic change tracking and versioning.\nWhen you are editing the notebook on the right side check version history to view all the changes, every change you are\nmaking is captured and saved.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nNewly joined data analyst requested read-only access to tables, assuming you are owner/admin which section of\nDatabricks platform is going to facilitate granting select access to the user\n",
        "choices": [
            "Admin console",
            "User settings",
            "Data explorer",
            "Azure Databricks control pane IAM",
            "Azure RBAC"
        ],
        "answer": "Data explorer",
        "explanation": "\nAnser is Data Explorer\nhttps://docs.databricks.com/sql/user/data/index.html\nData explorer lets you easily explore and manage permissions on databases and tables. Users can view schema details,\npreview sample data, and see table details and properties. Administrators can view and change owners, and admins and\ndata object owners can grant and revoke permissions.\nTo open data explorer, click Data in the sidebar.\n"
    },
    {
        "question": "\nHow does a Delta Lake differ from a traditional data lake?\nhttps://www.dumpsless.com/\n",
        "choices": [
            "Delta lake is Datawarehouse service on top of data lake that can provide reliability, security, and performance",
            "Delta lake is a caching layer on top of data lake that can provide reliability, security, and performance",
            "Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and",
            "Delta lake is an open storage format designed to replace flat files with additional capabilities that can provide",
            "Delta lake is proprietary software designed by Databricks that can provide reliability, security, and performance"
        ],
        "answer": "Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and",
        "explanation": "\nAnswer is, Delta lake is an open storage format like parquet with additional capabilities that can provide reliability,\nsecurity, and performance\nDelta lake is\n\u00b7 Open source\n\u00b7 Builds up on standard data format\n\u00b7 Optimized for cloud object storage\n\u00b7 Built for scalable metadata handling\nDelta lake is not\n\u00b7 Proprietary technology\n\u00b7 Storage format\n\u00b7 Storage medium\n\u00b7 Database service or data warehouse\n\n"
    },
    {
        "question": "\nWhich of the following is a correct statement on how the data is organized in the storage when when managing a DELTA\ntable?\n",
        "choices": [
            "All of the data is broken down into one or many parquet files, log files are broken down into one or many JSON files,",
            "All of the data and log are stored in a single parquet file",
            "All of the data is broken down into one or many parquet files, but the log file is stored as a single json file, and every",
            "All of the data is broken down into one or many parquet files, log file is removed once the transaction is committed.",
            "All of the data is stored into one parquet file, log files are broken down into one or many json files."
        ],
        "answer": "All of the data is broken down into one or many parquet files, log files are broken down into one or many JSON files,",
        "explanation": "\nAnswer is\nAll of the data is broken down into one or many parquet files, log files are broken down into one or many json files, and\neach transaction creates a new data file(s) and log file.\nhere is sample layout of how DELTA table might look,\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are still noticing slowness in query after performing optimize which helped you to resolve the small files problem, the\ncolumn(transactionId) you are using to filter the data has high cardinality and auto incrementing number. Which delta\noptimization can you enable to filter data effectively based on this column?\n",
        "choices": [
            "Create BLOOM FLTER index on the transactionId",
            "Perform Optimize with Zorder on transactionId",
            "transactionId has high cardinality, you cannot enable any optimization.",
            "Increase the cluster size and enable delta optimization",
            "Increase the driver size and enable delta optimization"
        ],
        "answer": "Perform Optimize with Zorder on transactionId",
        "explanation": "\nThe answer is, perform Optimize with Z-order by transactionid\nHere is a simple explanation of how Z-order works, once the data is naturally ordered, when a flle is scanned it only\nbrings the data it needs into spark's memory\nBased on the column min and max it knows which data files needs to be scanned.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nIf you create a database sample_db with the statement CREATE DATABASE sample_db what will be the location of the\ndatabase in DBFS?\n",
        "choices": [
            "Default location, DBFS:/user/",
            "The location assigned to parameter warehouse.dir",
            "Storage account",
            "Statement fails \u201cUnable to create database without location\u201d",
            "Default Location, DBFS:/user/databases/"
        ],
        "answer": "The location assigned to parameter warehouse.dir",
        "explanation": "\nAnswer is The location assigned to parameter warehouse.dir\n"
    },
    {
        "question": "\nWhich of the following results in the creation of an external table?\n",
        "choices": [
            "CREATE TABLE transactions (id int, desc string) USING DELTA LOCATION EXTERNAL",
            "CREATE TABLE transactions (id int, desc string)",
            "CREATE EXTERNAL TABLE transactions (id int, desc string)",
            "CREATE TABLE transactions (id int, desc string) TYPE EXTERNAL",
            "CREATE TABLE transactions (id int, desc string) LOCATION '/mnt/delta/transactions'"
        ],
        "answer": "CREATE TABLE transactions (id int, desc string) LOCATION '/mnt/delta/transactions'",
        "explanation": "\nAnswer is CREATE TABLE transactions (id int, desc string) USING DELTA LOCATION \u2018/mnt/delta/transactions\u2019\nAnytime a table is created using Location it is considered an external table, below is the current syntax.\nSyntax\nCREATE TABLE table_name ( column column_data_type\u2026) USING format LOCATION \u201cdbfs:/\u201d\n"
    },
    {
        "question": "\nWhen you drop an external DELTA table using DROP TABLE table_name, how does it impact metadata, data stored in the\nstorage?\n",
        "choices": [
            "Drops table from metastore, metadata and data in storage",
            "Drops table from metastore, data but keeps metadata in storage",
            "Drops table from metastore, metadata but keeps the data in storage",
            "Drops table from metastore, but keeps metadata and data in storage",
            "Drops table from metastore and data in storage but keeps meta data"
        ],
        "answer": "Drops table from metastore, but keeps metadata and data in storage",
        "explanation": "\nThe answer is Drops table from metastore, but keeps metadata and data in storage.\nWhen an external table is dropped, only the table definition is dropped from metastore everything including data and\nmetadata remains in the storage\n"
    },
    {
        "question": "\nWhich of the following is a true statement about the global temporary view?\n",
        "choices": [
            "A global temporary view is available only on the cluster it was created, when the cluster restarts global temporary",
            "A global temporary view is available on all clusters for a given workspace",
            "A global temporary view persists even if the cluster is restarted",
            "A global temporary view is stored in a user database",
            "A global temporary view is automatically dropped after 7 days"
        ],
        "answer": "A global temporary view is available only on the cluster it was created, when the cluster restarts global temporary",
        "explanation": "\nAnswer is, A global temporary view is available only on the cluster it was created.\nTwo types of temporary views can be created Local and Global\nA local temporary view is only available with a spark session, so another notebook in the same cluster can not access it. if\na notebook is detached and re attached local temporary view is lost.\nA global temporary view is available to all the notebooks in the cluster, if a cluster restarts global temporary view is lost.\n"
    },
    {
        "question": "\nYou are trying to create an object by joining two tables that and it is accessible to data scientist\u2019s team, so it does not get\ndropped if the cluster restarts or if the notebook is detached. What type of object are you trying to create?\n",
        "choices": [
            "Temporary view",
            "Global Temporary view",
            "Global Temporary view with cache option",
            "External view",
            "View"
        ],
        "answer": "View",
        "explanation": "\nAnswer is View, A view can be used to join multiple tables but also persist into meta stores so others can accesses it\n"
    },
    {
        "question": "\nWhat is the best way to query external csv files located on DBFS Storage to inspect the data using SQL?\n",
        "choices": [
            "SELECT * FROM 'dbfs:/location/csv_files/' FORMAT = 'CSV'",
            "SELECT CSV. * from 'dbfs:/location/csv_files/'",
            "SELECT * FROM CSV. 'dbfs:/location/csv_files/'",
            "You can not query external files directly, us COPY INTO to load the data into a table first",
            "SELECT * FROM 'dbfs:/location/csv_files/' USING CSV"
        ],
        "answer": "SELECT * FROM CSV. 'dbfs:/location/csv_files/'",
        "explanation": "\nAnswer is, SELECT * FROM CSV. \u2018dbfs:/location/csv_files/\u2019\nyou can query external files stored on the storage using below syntax\nSELECT * FROM format.`/Location`\nformat - CSV, JSON, PARQUET, TEXT\n"
    },
    {
        "question": "\nDirect query on external files limited options, create external tables for CSV files with header and pipe delimited CSV\nfiles, fill in the blanks to complete the create table statement\nCREATE TABLE sales (id int, unitsSold int, price FLOAT, items STRING)\n________\n________\nLOCATION \u201cdbfs:/mnt/sales/*.csv\u201d\n",
        "choices": [
            "FORMAT CSVOPTIONS ( \u201ctrue\u201d,\u201d|\u201d)",
            "USING CSVTYPE ( \u201ctrue\u201d,\u201d|\u201d)",
            "USING CSVOPTIONS ( header =\u201ctrue\u201d, delimiter = \u201d|\u201d)",
            "FORMAT CSVFORMAT TYPE ( header =\u201ctrue\u201d, delimiter = \u201d|\u201d)",
            "FORMAT CSVTYPE ( header =\u201ctrue\u201d, delimiter = \u201d|\u201d)"
        ],
        "answer": "USING CSVOPTIONS ( header =\u201ctrue\u201d, delimiter = \u201d|\u201d)",
        "explanation": "\nAnswer is\nUSING CSV\nOPTIONS ( header =\u201ctrue\u201d, delimiter = \u201d|\u201d)\nHere is the syntax to create an external table with additional options\nCREATE TABLE table_name (col_name1 col_typ1,..)\nUSING data_source\nOPTIONS (key=\u2019value\u2019, key2=vla2)\nLOCATION = \u201c/location\u201c\n"
    },
    {
        "question": "\nYou are working on a table called orders which contains data for  and you have the second table called\norders_archive which contains data for , you need to combine the data from two tables and there could be a\npossibility of the same rows between both the tables, you are looking to combine the results from both the tables and\neliminate the duplicate rows, which of the following SQL statements helps you accomplish this?\n",
        "choices": [
            "SELECT * FROM orders UNION SELECT * FROM orders_archive",
            "SELECT * FROM orders INTERSECT SELECT * FROM orders_archive",
            "SELECT * FROM orders UNION ALL SELECT * FROM orders_archive",
            "SELECT * FROM orders_archive MINUS SELECT * FROM orders",
            "SELECT distinct * FROM orders JOIN orders_archive on order.id = orders_archive.id"
        ],
        "answer": "SELECT * FROM orders UNION SELECT * FROM orders_archive",
        "explanation": "\nAnswer is SELECT * FROM orders UNION SELECT * FROM orders_archive\nUNION and UNION ALL are set operators,\nUNION combines the output from both queries but also eliminates the duplicates.\nUNION ALL combines the output from both queries.\n"
    },
    {
        "question": "\nWhich of the following python statement can be used to replace the schema name and table name in the query\nstatement?\n",
        "choices": [
            "table_name = \"sales\"schema_name = \"bronze\"query = f\u201dselect * from schema_name.table_name\u201d",
            "table_name = \"sales\"schema_name = \"bronze\"query = \"select * from {schema_name}.{table_name}\"",
            "table_name = \"sales\"schema_name = \"bronze\"query = f\"select * from { schema_name}.{table_name}\"",
            "table_name = \"sales\"schema_name = \"bronze\"query = f\"select * from + schema_name +\".\"+table_name\""
        ],
        "answer": "table_name = \"sales\"schema_name = \"bronze\"query = f\"select * from { schema_name}.{table_name}\"",
        "explanation": "\nAnswer is\ntable_name = \u201csales\u201d\nquery = f\u201dselect * from {schema_name}.{table_name}\u201d\nf strings can be used to format a string. f\" This is string {python variable}\"\nhttps://realpython.com/python-f-strings/\n"
    },
    {
        "question": "\nWhich of the following SQL statements can replace python variables in Databricks SQL code, when the notebook is set in\nSQL mode?\n%python\ntable_name = \"sales\"\nschema_name = \"bronze\"\n%sql\nSELECT * FROM ____________________\n",
        "choices": [
            "SELECT * FROM f{schema_name.table_name}",
            "SELECT * FROM {schem_name.table_name}",
            "SELECT * FROM ${schema_name}.${table_name}",
            "SELECT * FROM schema_name.table_name"
        ],
        "answer": "SELECT * FROM ${schema_name}.${table_name}",
        "explanation": "\nThe answer is, SELECT * FROM ${schema_name}.${table_name}\n%python\ntable_name = \"sales\"\nschema_name = \"bronze\"\n%sql\nSELECT * FROM ${schema_name}.${table_name}\n${python variable} -> Python variables in Databricks SQL code\n"
    },
    {
        "question": "\nYour notebook accepts an input parameter called department and you are looking to control the flow of the code using\ndepartment, if the department is passed then execute code and if no department is passed skip code execution.\n",
        "choices": [
            "if department is not None: #Execute codeelse: pass",
            "if (department is not None) #Execute codeelse pass",
            "if department is not None: #Execute codeend: pass",
            "if department is not None: #Execute codethen: pass",
            "if department is None: #Execute codeelse: pass"
        ],
        "answer": "if department is not None: #Execute codeelse: pass",
        "explanation": "\nThe answer is,\nif department is not None:\n#Execute code\nelse:\npass\n"
    },
    {
        "question": "\nWhich of the following operations are not supported on a streaming dataset view ?\nspark.readStream.table(\"sales\").createOrReplaceTempView(\"streaming_view\")\n",
        "choices": [
            "SELECT sum(unitssold) FROM streaming_view",
            "SELECT max(unitssold) FROM streaming_view",
            "SELECT id, sum(unitssold) FROM streaming_view GROUP BY id ORDER BY id",
            "SELECT id, count(*) FROM streaming_view GROUP BY id",
            "SELECT * FROM streadming_view ORDER BY id"
        ],
        "answer": "SELECT * FROM streadming_view ORDER BY id",
        "explanation": "\nThe answer isSELECT * FROM streadming_view order by id Please Note: Sorting with Group by will work without any\nissues\nsee below explanation for each option of the options,\nCertain operations are not allowed on streaming data, please see highlighted in bold.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations\nMultiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming\nDatasets.\nLimit and take the first N rows are not supported on streaming Datasets.\nDistinct operations on streaming Datasets are not supported.\nDeduplication operation is not supported after aggregation on a streaming Datasets.\nSorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.\nNote: Sorting without aggregation function is not supported.\nHere is the sample code to prove this,\nSetup test stream\nhttps://www.dumps4less.com/\nSum aggregation function has no issues on stream\nMax aggregation function has no issues on stream\nGroup by with Order by has no issues on stream\nhttps://www.dumps4less.com/\nGroup by has no issues on stream\nOrder by without group by fails.\n"
    },
    {
        "question": "\nWhich of the following techniques structured streaming uses to ensure recovery of failures during stream processing?\n",
        "choices": [
            "Checkpointing and Watermarking",
            "Write ahead logging and watermarking",
            "Checkpointing and write-ahead logging",
            "Delta time travel",
            "The stream will failover to available nodes in the cluster",
            "Checkpointing and Idempotent sinks"
        ],
        "answer": "Checkpointing and write-ahead logging",
        "explanation": "\nThe answer is Checkpointing and write-ahead logging.\nStructured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during\neach trigger interval.\n"
    },
    {
        "question": "\nWhat is the underlying process that makes the Auto ",
        "choices": [
            "Loader",
            "Delta Live Tables",
            "Structured Streaming",
            "DataFrames",
            "Live DataFames"
        ],
        "answer": "Structured Streaming",
        "explanation": "\nThe answer is Structured Streaming\nAuto Loader is built on top of Structured Streaming, Auto Loader provides a Structured Streaming source called\ncloudFiles. Given an input directory path on the cloud file storage, the cloudFiles source automatically processes new files\nas they arrive, with the option of also processing existing files in that directory\n"
    },
    {
        "question": "\nThousands of files get uploaded to the cloud object storage for consumption, you are asked to build a process to ingest\ndata which of the following method can be used to ingest the data incrementally, the schema of the file is expected to\nchange overtime ingestion process should be able to handle these changes automatically.\n",
        "choices": [
            "AUTO APPEND",
            "AUTO LOADER",
            "COPY INTO",
            "Structured Streaming",
            "Checkpoint"
        ],
        "answer": "AUTO LOADER",
        "explanation": "\nThe answer is AUTO LOADER,\nUse Auto Loader instead of the COPY INTO SQL command when:\nYou want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover\nfiles more efficiently than the COPY INTO SQL command and can split file processing into multiple batches.\nYour data schema evolves frequently. Auto Loader provides better support for schema inference and evolution. See\nConfiguring schema inference and evolution in Auto Loader.\n"
    },
    {
        "question": "\nAt the end of the inventory process, a file gets uploaded to the cloud object storage, you are asked to build a process to\ningest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to\nchange overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to\ncommand to load the data, fill in the blanks for successful execution of below code.\nspark.readStream\n.format(\"cloudfiles\")\n.option(\"_______\",\u201dcsv)\n.option(\"_______\", \u2018dbfs:/location/checkpoint/\u2019)\n.load(data_source)\n.writeStream\n.option(\"_______\",\u2019 dbfs:/location/checkpoint/\u2019)\n.option(\"_______\", \"true\")\n.table(table_name))\n",
        "choices": [
            "format, checkpointlocation, schemalocation, overwrite",
            "cloudfiles.format, checkpointlocation, cloudfiles.schemalocation, overwrite",
            "cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema",
            "cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, overwrite",
            "cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, append"
        ],
        "answer": "cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema",
        "explanation": "\nThe answer is cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema.\nHere is the end to end syntax of streaming ELT, below link contains complete options Auto Loader options | Databricks on\nAWS\nspark.readStream\n.format(\"cloudfiles\") # Returns a stream data source, reads data as it arrives based on the trigger.\n.option(\"cloudfiles.format\",\"csv\") # Format of the incoming files\n.option(\"cloudfiles.schemalocation\", \"dbfs:/location/checkpoint/\") The location to store the inferred schema and\nsubsequent changes\n.load(data_source)\n.writeStream\n.option(\"checkpointlocation\",\"dbfs:/location/checkpoint/\") # The location of the stream\u2019s checkpoint\n.option(\"mergeSchema\", \"true\") # Infer the schema across multiple files and to merge the schema of each file. Enabled\nby default for Auto Loader when inferring the schema.\n.table(table_name)) # target table\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhat is the purpose of the bronze layer in a Multi-hop architecture?\n",
        "choices": [
            "Eliminates duplicate records",
            "Powers ML applications",
            "Data quality checks, corrupt data quarantined",
            "Contain aggregated data that is to be consumed into Silver",
            "Efficient storage and querying of full, unprocessed history"
        ],
        "answer": "Efficient storage and querying of full, unprocessed history",
        "explanation": "\nThe answer is Efficient storage and querying of full, unprocessed history.\nMedallion Architecture \u2013 Databricks\nBronze Layer:\nRaw copy of ingested data\nReplaces traditional data lake\nProvides efficient storage and querying of full, unprocessed history of data\nNo schema is applied at this layer\n"
    },
    {
        "question": "\nWhat is the purpose of a silver layer in Multi hop architecture?\n",
        "choices": [
            "Replaces a traditional data lake",
            "Efficient storage and querying of full, unprocessed history of data",
            "A schema is enforced, with data quality checks.",
            "Refined views with aggregated data",
            "Optimized query performance for business-critical data"
        ],
        "answer": "A schema is enforced, with data quality checks.",
        "explanation": "\nThe answer is, A schema is enforced, with data quality checks.\nhttps://www.dumps4less.com/\nMedallion Architecture \u2013 Databricks\nSilver Layer:\nReduces data storage complexity, latency, and redundency\nOptimizes ETL throughput and analytic query performance\nPreserves grain of original data (without aggregation)\nEliminates duplicate record\nproduction schema enforced\nData quality checks, quarantine corrupt data\n"
    },
    {
        "question": "\nWhat is the purpose of a gold layer in Multi-hop architecture?\n",
        "choices": [
            "Optimizes ETL throughput and analytic query performance",
            "Eliminate duplicate records",
            "Preserves grain of original data, without any aggregations",
            "Data quality checks and schema enforcement",
            "Powers ML applications, reporting, dashboards and adhoc reports."
        ],
        "answer": "Powers ML applications, reporting, dashboards and adhoc reports.",
        "explanation": "\nThe answer is Powers ML applications, reporting, dashboards and adhoc reports.\nReview the below link for more info,\nMedallion Architecture \u2013 Databricks\nGold Layer:\nPowers Ml applications, reporting, dashboards, ad hoc analytics\nRefined views of data, typically with aggregations\nReduces strain on production systems\nOptimizes query performance for business-critical data\n"
    },
    {
        "question": "\nYou are currently asked to work on building a data pipeline, you have noticed that you are currently working on a very\nlarge scale ETL many data dependencies, which of the following tools can be used to address this problem?\n",
        "choices": [
            "AUTO LOADER",
            "JOBS and TASKS",
            "SQL Endpoints",
            "DELTA LIVE TABLES",
            "STRUCTURED STREAMING with MULTI HOP"
        ],
        "answer": "DELTA LIVE TABLES",
        "explanation": "\nThe answer is, DELTA LIVE TABLES\nDLT simplifies data dependencies by building DAG-based joins between live tables. Here is a view of how the dag looks\nwith data dependencies without additional meta data,\ncreate or replace live view customers\nselect * from customers;\ncreate or replace live view sales_orders_raw\nselect * from sales_orders;\ncreate or replace live view sales_orders_cleaned\nas\nselect sales.* from\nlive.sales_orders_raw s\njoin live.customers c\non c.customer_id = s.customer_id\nwhere c.city = 'LA';\ncreate or replace live table sales_orders_in_la\nselect * from sales_orders_cleaned;\nAbove code creates below dag\nDocumentation on DELTA LIVE TABLES,\nhttps://databricks.com/product/delta-live-tables\nhttps://databricks.com/blog/2022/04/05/announcing-generally-availability-of-databricks-delta-live-tables-dlt.html\nhttps://www.dumps4less.com/\nDELTA LIVE TABLES, addresses below challenges when building ETL processes\nComplexities of large scale ETL\nHard to build and maintain dependencies\nDifficult to switch between batch and stream\nData quality and governance\nDifficult to monitor and enforce data quality\nImpossible to trace data lineage\nDifficult pipeline operations\nPoor observability at granular data level\nError handling and recovery is laborious\n"
    },
    {
        "question": "\nHow do you create a delta live tables pipeline and deploy using DLT UI?\n",
        "choices": [
            "Within the Workspace UI, click on Workflows, select Delta Live tables and create a pipeline and select the notebook",
            "Under Cluster UI, select SPARK UI and select Structured Streaming and click create pipeline and select the notebook",
            "There is no UI, you can only setup DELTA LIVE TABLES using Python and SQL API and select the notebook with DLT",
            "Use VS Code and download DBX plugin, once the plugin is loaded you can build DLT pipelines and select the notebook",
            "Within the Workspace UI, click on SQL Endpoint, select Delta Live tables and create pipelinea and select the notebook"
        ],
        "answer": "Within the Workspace UI, click on Workflows, select Delta Live tables and create a pipeline and select the notebook",
        "explanation": "\nThe answer is, Within the Workspace UI, click on Workflows, select Delta Live tables and create a pipeline and select the\nnotebook with DLT code.\nhttps://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-quickstart.html\n"
    },
    {
        "question": "\nYou are noticing job cluster is taking  to  mins to start which is delaying your job to finish on time, what steps you can\ntake to reduce the amount of time cluster startup time\n",
        "choices": [
            "Setup a second job ahead of first job to start the cluster, so the cluster is ready with resources when the job starts",
            "Use All purpose cluster instead to reduce cluster start up time",
            "Reduce the size of the cluster, smaller the cluster size shorter it takes to start the cluster",
            "Use cluster pools to reduce the startup time of the jobs",
            "Use SQL endpoints to reduce the startup time"
        ],
        "answer": "Use cluster pools to reduce the startup time of the jobs",
        "explanation": "\nThe answer is, Use cluster pools to reduce the startup time of the jobs.\nCluster pools allow us to reserve VM's ahead of time, when a new job cluster is created VM are grabbed from the pool.\nNote: when the VM's are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only\nbilled once VM is allocated to a cluster.\nHere is a demo of how to setup and follow some best practices,\nhttps://www.youtube.com/watch?v=FVtITxOabxg&ab_channel=DatabricksAcademy\n"
    },
    {
        "question": "\nData engineering team has a job currently setup to run a task load data into a reporting table every day at :  AM\ntakes about  mins, Operations teams are planning to use that data to run a second job, so they access latest complete\nset of data. What is the best to way to orchestrate this job setup?\n",
        "choices": [
            "Add Operation reporting task in the same job and set the Data Engineering task to depend on Operations reporting",
            "Setup a second job to run at 8:20 AM in the same workspace",
            "Add Operation reporting task in the same job and set the operations reporting task to depend on Data Engineering",
            "Use Auto Loader to run every 20 mins to read the initial table and set the trigger to once and create a second job",
            "Setup a Delta live to table based on the first table, set the job to run in continuous mode"
        ],
        "answer": "Add Operation reporting task in the same job and set the operations reporting task to depend on Data Engineering",
        "explanation": "\nThe answer is Add Operation reporting task in the same job and set the operations reporting task to depend on Data\nEngineering task.\nhttps://www.dumps4less.com/\nJob View\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nThe data engineering team noticed that one of the job normally finishes in  mins but gets stuck randomly when reading\nremote databases due to network packet drops and the job takes really long time to finish, which of the following option\ncan be used to fix the problem?\n",
        "choices": [
            "Use Databrick REST API to monitor long running jobs and issue a kill command",
            "Use Jobs runs, active runs UI section to monitor and kill long running job",
            "Modify the task, to include a timeout to kill the job if it runs more than 15 mins.",
            "Use Spark job time out setting in the Spark UI",
            "Use Cluster timeout setting in the Job cluster UI"
        ],
        "answer": "Modify the task, to include a timeout to kill the job if it runs more than 15 mins.",
        "explanation": "\nThe answer is, Modify the task, to include time out to kill the job if it runs more than 15 mins.\nhttps://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs#timeout\n"
    },
    {
        "question": "\nWhich of the following programming languages can be used to build a Databricks SQL dashboard?\nhttps://www.dumpsless.com/\n",
        "choices": [
            "Python",
            "Scala",
            "SQL",
            "R",
            "All of the above"
        ],
        "answer": "SQL",
        "explanation": "\nThe answer is SQL\n"
    },
    {
        "question": "\nYou have noticed that Databricks SQL queries are running slow, you were asked to look reason why queries are running\nslow and identify steps to improve the performance, when you looked at the code you noticed all the queries are running\nsequentially and using a SQL endpoint cluster. Which of the following steps can be taken to resolve the issue?\n",
        "choices": [
            "Turn on the Serverless feature for the SQL endpoint.",
            "Increase the maximum bound of the SQL endpoint\u2019s scaling range.",
            "Increase the cluster size of the SQL endpoint.",
            "Turn on the Auto Stop feature for the SQL endpoint.",
            "Turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to \u201cReliability Optimized.\u201d"
        ],
        "answer": "Increase the cluster size of the SQL endpoint.",
        "explanation": "\nThe answer is increase the cluster size of the SQL Endoint,\nSQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.\nScale-out -> Add more clusters, change max number of clusters\nIf you are trying to improve the throughput, being able to run as many queries as possible then having an additional\ncluster(s) will improve the performance.\nScale-up-> Increase the size of the cluster from x-small to small, to medium, X Large....\nIf you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the\ncluster will improve the performance.\nSQL endpoint\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nThe operations team is interested in monitoring the recently launched product, team wants to set up an email alert when\nthe number of units sold increases by more than , units. They want to monitor this every  mins.\nFill in the below blanks to finish the steps we need to take\n\u00b7 Create ___ query that calculates total units sold\n\u00b7 Setup ____ with query on trigger condition Units Sold > ,\n\u00b7 Setup ____ to run every  mins\n\u00b7 Add destination ______\n",
        "choices": [
            "Python, Job, SQL Cluster, email address",
            "SQL, Alert, Refresh, email address",
            "SQL, Job, SQL Cluster, email address",
            "SQL, Job, Refresh, email address",
            "Python, Job, Refresh, email address"
        ],
        "answer": "SQL, Alert, Refresh, email address",
        "explanation": "\nThe answer is SQL, Alert, Refresh, email address\nHere the steps from Databricks documentation,\nCreate an alert\nFollow these steps to create an alert on a single column of a query.\nDo one of the following:\nhttps://www.dumps4less.com/\nClick Create in the sidebar and select Alert.\nClick Alerts in the sidebar and click the + New Alert button.\nSearch for a target query.\nTo alert on multiple columns, you need to modify your query. See Alert on multiple columns\nIn the Trigger when field, configure the alert.\nThe Value column drop-down controls which field of your query result is evaluated.\nThe Condition drop-down controls the logical operation to be applied.\nThe Threshold text input is compared against the Value column using the Condition you specify.\nNote\nIf a target query returns multiple records, Databricks SQL alerts act on the first one. As you change the Value column\nsetting, the current value of that field in the top row is shown beneath it.\nIn the When triggered, send notification field, select how many notifications are sent when your alert is triggered:\nJust once: Send a notification when the alert status changes from OK to TRIGGERED.\nEach time alert is evaluated: Send a notification whenever the alert status is TRIGGERED regardless of its status at the\nprevious evaluation.\nAt most every: Send a notification whenever the alert status is TRIGGERED at a specific interval. This choice lets you\navoid notification spam for alerts that trigger often.\nRegardless of which notification setting you choose, you receive a notification whenever the status goes from OK to\nTRIGGERED or from TRIGGERED to OK. The schedule settings affect how many notifications you will receive if the status\nremains TRIGGERED from one execution to the next. For details, see Notification frequency.\nIn the Template drop-down, choose a template:\nhttps://www.dumps4less.com/\nUse default template: Alert notification is a message with links to the Alert configuration screen and the Query screen.\nUse custom template: Alert notification includes more specific information about the alert.\nA box displays, consisting of input fields for subject and body. Any static content is valid, and you can incorporate built-in\ntemplate variables:\nALERT_STATUS: The evaluated alert status (string).\nALERT_CONDITION: The alert condition operator (string).\nALERT_THRESHOLD: The alert threshold (string or number).\nALERT_NAME: The alert name (string).\nALERT_URL: The alert page URL (string).\nQUERY_NAME: The associated query name (string).\nQUERY_URL: The associated query page URL (string).\nQUERY_RESULT_VALUE: The query result value (string or number).\nQUERY_RESULT_ROWS: The query result rows (value array).\nQUERY_RESULT_COLS: The query result columns (string array).\nAn example subject, for instance, could be: Alert \"{{ALERT_NAME}}\" changed status to {{ALERT_STATUS}}.\nClick the Preview toggle button to preview the rendered result.\nImportant\nThe preview is useful for verifying that template variables are rendered correctly. It is not an accurate representation of\nthe eventual notification content, as each alert destination can display notifications differently.\nClick the Save Changes button.\nIn Refresh, set a refresh schedule. An alert\u2019s refresh schedule is independent of the query\u2019s refresh schedule.\nIf the query is a Run as owner query, the query runs using the query owner\u2019s credential on the alert\u2019s refresh schedule.\nIf the query is a Run as viewer query, the query runs using the alert creator\u2019s credential on the alert\u2019s refresh schedule.\nClick Create Alert.\nChoose an alert destination.\nImportant\nIf you skip this step you will not be notified when the alert is triggered.\n"
    },
    {
        "question": "\nThe marketing team is launching a new campaign to monitor the performance of the new campaign for the first two\nweeks, they would like to set up a dashboard with a refresh schedule to run every  minutes, which of the below steps\ncan be taken to reduce of the cost of this refresh over time?\n",
        "choices": [
            "Reduce the size of the SQL Cluster size",
            "Reduce the max size of auto scaling from 10 to 5",
            "Setup the dashboard refresh schedule to end in two weeks",
            "Change the spot instance policy from reliability optimized to cost optimized",
            "Always use X-small cluster"
        ],
        "answer": "Setup the dashboard refresh schedule to end in two weeks",
        "explanation": "\nThe answer is Setup the dashboard refresh schedule to end in two weeks\n"
    },
    {
        "question": "\nWhich of the following tool provides Data Access control, Access Audit, Data Lineage, and Data discovery?\n",
        "choices": [
            "DELTA LIVE Pipelines",
            "Unity Catalog",
            "Data Governance",
            "DELTA lake",
            "Lakehouse"
        ],
        "answer": "Unity Catalog",
        "explanation": "\nThe answer is Unity Catalog\n"
    },
    {
        "question": "\nData engineering team is required to share the data across with Data science team, both the teams are using different\nworkspaces which of the following techniques can be used to simplify sharing data across?\n",
        "choices": [
            "Data Sharing",
            "Unity Catalog",
            "DELTA lake",
            "Use single storage location",
            "DELTA LIVE Pipelines"
        ],
        "answer": "Unity Catalog",
        "explanation": "\nThe answer is Unity catalog.\nReview product features\nhttps://databricks.com/product/unity-catalog\n"
    },
    {
        "question": "\nA newly joined team member John Smith in the Marketing team who currently does not have any access to the data\nrequires read access to customers table, which of the following statements can be used to grant access.\n",
        "choices": [
            "GRANT SELECT, USAGE TO john.smith@marketing.com ON TABLE customers",
            "GRANT READ, USAGE TO john.smith@marketing.com ON TABLE customers",
            "GRANT SELECT, USAGE ON TABLE customers TO john.smith@marketing.com",
            "GRANT READ, USAGE ON TABLE customers TO john.smith@marketing.com",
            "GRANT READ, USAGE ON customers TO john.smith@marketing.com"
        ],
        "answer": "GRANT SELECT, USAGE ON TABLE customers TO john.smith@marketing.com",
        "explanation": "\nThe answer is GRANT SELECT, USAGE ON TABLE customers TO john.smith@marketing.com\nData object privileges - Azure Databricks | Microsoft Docs\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nGrant full privileges to new marketing user Kevin Smith to table sales\n",
        "choices": [
            "GRANT FULL PRIVILEGES TO kevin.smith@marketing.com ON TABLE sales",
            "GRANT ALL PRIVILEGES TO kevin.smith@marketing.com ON TABLE sales",
            "GRANT FULL PRIVILEGES ON TABLE sales TO kevin.smith@marketing.com",
            "GRANT ALL PRIVILEGES ON TABLE sales TO kevin.smith@marketing.com",
            "GRANT ANY PRIVILEGE ON TABLE sales TO kevin.smith@marketing.com"
        ],
        "answer": "GRANT ALL PRIVILEGES ON TABLE sales TO kevin.smith@marketing.com",
        "explanation": "\nThe answer is GRANT ALL PRIVILEGE ON TABLE sales TO kevin.smith@marketing.com\nGRANT < privilege > ON < securable_type > < securable_name > TO < principal >\nHere are the available privileges and ALL Privileges gives full access to an object.\nPrivileges\nSELECT: gives read access to an object.\nCREATE: gives ability to create an object (for example, a table in a schema).\nMODIFY: gives ability to add, delete, and modify data to or from an object.\nUSAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.\nREAD_METADATA: gives ability to view an object and its metadata.\nCREATE_NAMED_FUNCTION: gives ability to create a named UDF in an existing catalog or schema.\nMODIFY_CLASSPATH: gives ability to add files to the Spark class path.\nALL PRIVILEGES: gives all privileges (is translated into all the above privileges).\n"
    },
    {
        "question": "\nWhich of the following locations in the Databricks product architecture hosts the notebooks and jobs?\n",
        "choices": [
            "Data plane",
            "Control plane",
            "Databricks Filesystem",
            "JDBC data source",
            "Databricks web application"
        ],
        "answer": "Control plane",
        "explanation": "\nThe answer is Control Pane,\nDatabricks operates most of its services out of a control plane and a data plane, please note serverless features like SQL\nEndpoint and DLT compute use shared compute in Control pane.\nControl Plane: Stored in Databricks Cloud Account\nThe control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands\nand many other workspace configurations are stored in the control plane and encrypted at rest.\nData Plane: Stored in Customer Cloud Account\nThe data plane is managed by your Azure account and is where your data resides. This is also where data is processed.\nYou can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure\naccount to ingest data or for storage.\n"
    },
    {
        "question": "\nA dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp\nEXPECT (timestamp > '--') ON VIOLATION FAIL UPDATE\nWhat is the expected behavior when a batch of data containing data that violates these constraints is processed?\n",
        "choices": [
            "Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.",
            "Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.",
            "Records that violate the expectation cause the job to fail",
            "Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the",
            "Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table."
        ],
        "answer": "Records that violate the expectation cause the job to fail",
        "explanation": "\nThe answer is Records that violate the expectation cause the job to fail\nSee below notes for additional notes,\nThere are three types of DLT expectations\nInvalid records:\nUse the expect operator when you want to keep records that violate the expectation. Records that violate the\nexpectation are added to the target dataset along with valid records:\nSQL\nCONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01')\nDrop invalid records:\nUse the expect or drop operator to prevent the processing of invalid records. Records that violate the expectation are\ndropped from the target dataset:\nSQL\nCONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01') ON VIOLATION DROP ROW\nFail on invalid records:\nWhen invalid records are unacceptable, use the expect or fail operator to halt execution immediately when a record fails\nvalidation. If the operation is a table update, the system atomically rolls back the transaction\nSQL\nCONSTRAINT valid_timestamp EXPECT (timestamp > '2020-01-01') ON VIOLATION FAIL UPDATE\n"
    },
    {
        "question": "\nWhich of the statements are incorrect when choosing between lakehouse and Datawarehouse?\n",
        "choices": [
            "Lakehouse can have special indexes and caching which are optimized for Machine learning",
            "Lakehouse cannot serve low query latency with high reliability for BI workloads, only suitable for batch workloads.",
            "Lakehouse can be accessed through various API\u2019s including but not limited to Python/R/SQL",
            "Traditional Data warehouses have storage and compute are coupled.",
            "Lakehouse uses standard data formats like Parquet."
        ],
        "answer": "Lakehouse cannot serve low query latency with high reliability for BI workloads, only suitable for batch workloads.",
        "explanation": "\nThe answer is Lakehouse cannot serve low query latency with high reliability for BI workloads, only suitable for batch\nworkloads.\nLakehouse can replace traditional warehouses by leveraging storage and compute optimizations like caching to serve\nthem with low query latency with high reliability.\nhttps://www.dumps4less.com/\nFocus on comparisons between Spark Cache vs Delta Cache.\nhttps://docs.databricks.com/delta/optimizations/delta-cache.html\nWhat Is a Lakehouse? - The Databricks Blog\n"
    },
    {
        "question": "\nWhich of the statements are correct about lakehouse?\n",
        "choices": [
            "Lakehouse only supports Machine learning workloads and Data warehouses support BI workloads",
            "Lakehouse only supports end-to-end streaming workloads and Data warehouses support Batch workloads",
            "Lakehouse does not support ACID",
            "In Lakehouse Storage and compute are coupled",
            "Lakehouse supports schema enforcement and evolution"
        ],
        "answer": "Lakehouse supports schema enforcement and evolution",
        "explanation": "\nThe answer is Lakehouse supports schema enforcement and evolution,\nLakehouse using Delta lake can not only enforce a schema on write which is contrary to traditional big data systems that\ncan only enforce a schema on read, it also supports evolving schema over time with the ability to control the evolution.\nFor example below is the Dataframe writer API and it supports three modes of enforcement and evolution,\nDefault: Only enforcement, no changes are allowed and any schema drift/evolution will result in failure.\nMerge: Flexible, supports enforcement and evolution\nNew columns are added\nEvolves nested columns\nSupports evolving data types, like Byte to Short to Integer to Bigin\nHow to enable:\nDF.write.format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"table_name\")\nor\nspark.databricks.delta.schema.autoMerge = True ## Spark session\nOverwrite: No enforcement\nDropping columns\nChange string to integer\nRename columns\nHow to enable:\nDF.write.format(\"delta\").option(\"overwriteSchema\", \"True\").saveAsTable(\"table_name\")\nWhat Is a Lakehouse? - The Databricks Blog\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the following are stored in the control pane of Databricks Architecture?\n",
        "choices": [
            "Job Clusters",
            "All Purpose Clusters",
            "Databricks Filesystem",
            "Databricks Web Application",
            "Delta tables"
        ],
        "answer": "Databricks Web Application",
        "explanation": "\nThe answer is Databricks Web Application\nAzure Databricks architecture overview - Azure Databricks | Microsoft Docs\nDatabricks operates most of its services out of a control plane and a data plane, please note serverless features like SQL\nEndpoint and DLT compute use shared compute in Control pane.\nControl Plane: Stored in Databricks Cloud Account\nThe control plane includes the backend services that Databricks manages in its own Azure account. Notebook commands\nand many other workspace configurations are stored in the control plane and encrypted at rest.\nData Plane: Stored in Customer Cloud Account\nThe data plane is managed by your Azure account and is where your data resides. This is also where data is processed.\nYou can use Azure Databricks connectors so that your clusters can connect to external data sources outside of your Azure\naccount to ingest data or for storage.\n"
    },
    {
        "question": "\nYou have written a notebook to generate a summary data set for reporting, Notebook was scheduled using the job\ncluster, but you realized it takes  minutes to start the cluster, what feature can be to start the cluster in a timely fashion\n",
        "choices": [
            "Setup an additional job to run ahead of the actual job so the cluster is running second job starts",
            "Use the Databricks cluster pools feature to reduce the startup time",
            "Use Databricks Premium edition instead of Databricks standard edition",
            "Pin the cluster in the cluster UI page so it is always available to the jobs",
            "Disable auto termination so the cluster is always running"
        ],
        "answer": "Use the Databricks cluster pools feature to reduce the startup time",
        "explanation": "\nCluster pools allow us to reserve VM's ahead of time, when a new job cluster is created VM are grabbed from the pool.\nNote: when the VM's are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only\nbilled once VM is allocated to a cluster.\nHere is a demo of how to setup a pool and follow some best practices,\n"
    },
    {
        "question": "\nWhich of the following developer operations in the CI/CD can only be implemented through a GIT provider when using\nDatabricks Repos.\n",
        "choices": [
            "Trigger Databricks Repos pull API to update the latest version",
            "Commit and push code",
            "Create and edit code",
            "Create a new branch",
            "Pull request and review process"
        ],
        "answer": "Pull request and review process",
        "explanation": "\nThe answer is Pull request and review process, please note: the question is asking for steps that are being implemented\nin GIT provider not Databricks Repos.\nSee below diagram to understand the role of Databricks Repos and Git provider plays when building a CI/CD workdlow.\nAll the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git\nprovider like Github or Azure Devops.\n"
    },
    {
        "question": "\nYou have noticed the Data scientist team is using the notebook versioning feature with git integration, you have\nrecommended them to switch to using Databricks Repos, which of the below reasons could be the reason the why the\nteam needs to switch to Databricks Repos.\n",
        "choices": [
            "Databricks Repos allows multiple users to make changes",
            "Databricks Repos allows merge and conflict resolution",
            "Databricks Repos has a built-in version control system",
            "Databricks Repos automatically saves changes",
            "Databricks Repos allow you to add comments and select the changes you want to commit."
        ],
        "answer": "Databricks Repos allow you to add comments and select the changes you want to commit.",
        "explanation": "\nhttps://www.dumps4less.com/\nThe answer is Databricks Repos allow you to add comments and select the changes you want to commit.\n"
    },
    {
        "question": "\nData science team members are using a single cluster to perform data analysis, although cluster size was chosen to\nhandle multiple users and auto-scaling was enabled, the team realized queries are still running slow, what would be the\nsuggested fix for this?\n",
        "choices": [
            "Setup multiple clusters so each team member has their own cluster",
            "Disable the auto-scaling feature",
            "Use High concurrency mode instead of the standard mode",
            "Increase the size of the driver node"
        ],
        "answer": "Use High concurrency mode instead of the standard mode",
        "explanation": "\nThe answer is Use High concurrency mode instead of the standard mode,\nhttps://docs.databricks.com/clusters/cluster-config-best-practices.html#cluster-mode\nHigh Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs. Administrators\nusually create High Concurrency clusters. Databricks recommends enabling autoscaling for High Concurrency clusters.\n"
    },
    {
        "question": "\nWhich of the following SQL commands are used to append rows to an existing delta table?\n",
        "choices": [
            "APPEND INTO DELTA table_name",
            "APPEND INTO table_name",
            "COPY DELTA INTO table_name",
            "INSERT INTO table_name",
            "UPDATE table_name"
        ],
        "answer": "INSERT INTO table_name",
        "explanation": "\nThe answer is INSERT INTO table_name\nInsert adds rows to existing table\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nHow are Delt tables stored?\n",
        "choices": [
            "A Directory where parquet data files are stored, a sub directory _delta_log where meta data, and the transaction log is",
            "A Directory where parquet data files are stored, all of the meta data is stored in memory",
            "A Directory where parquet data files are stored in Data plane, a sub directory _delta_log where meta data, history and",
            "A Directory where parquet data files are stored, all of the metadata is stored in parquet files",
            "Data is stored in Data plane and Metadata and delta log are stored in control pane"
        ],
        "answer": "A Directory where parquet data files are stored, a sub directory _delta_log where meta data, and the transaction log is",
        "explanation": "\nThe answer is A Directory where parquet data files are stored, a sub directory _delta_log where meta data, and the\ntransaction log is stored as JSON files.\n"
    },
    {
        "question": "\nWhile investigating a data issue in a Delta table, you wanted to review when and who updated the table, what is the best\nway to review this data?\n",
        "choices": [
            "Review event logs in the Workspace",
            "Run SQL SHOW HISTORY table_name",
            "Check Databricks SQL Audit logs",
            "Run SQL command DESCRIBE HISTORY table_name",
            "Review workspace audit logs"
        ],
        "answer": "Run SQL command DESCRIBE HISTORY table_name",
        "explanation": "\nThe answer is Run SQL command DESCRIBE HISTORY table_name.\nhere is the sample data of how DESCRIBE HISTORY table_name looks\n"
    },
    {
        "question": "\nWhile investigating a performance issue, you realized that you have too many small files for a given table, which\ncommand are you going to run to fix this issue\n",
        "choices": [
            "COMPACT table_name",
            "VACUUM table_name",
            "MERGE table_name",
            "SHRINK table_name",
            "OPTIMIZE table_name"
        ],
        "answer": "OPTIMIZE table_name",
        "explanation": "\nThe answer is OPTIMIZE table_name,\nOptimize compacts small parquet files into a bigger file, by default the size of the files are determined based on the table\nsize at the time of OPTIMIZE, the file size can also be set manually or adjusted based on the workload.\nhttps://docs.databricks.com/delta/optimizations/file-mgmt.html\nTune file size based on Table size\nTo minimize the need for manual tuning, Databricks automatically tunes the file size of Delta tables based on the size of\nthe table. Databricks will use smaller file sizes for smaller tables and larger file sizes for larger tables so that the number\nof files in the table does not grow too large.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nCreate a sales database using the DBFS location 'dbfs:/mnt/delta/databases/sales.db/'\n",
        "choices": [
            "CREATE DATABASE sales FORMAT DELTA LOCATION 'dbfs:/mnt/delta/databases/sales.db/'\u2019",
            "CREATE DATABASE sales USING LOCATION 'dbfs:/mnt/delta/databases/sales.db/'",
            "CREATE DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'",
            "The sales database can only be created in Delta lake",
            "CREATE DELTA DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'"
        ],
        "answer": "CREATE DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'",
        "explanation": "\nThe answer is\nCREATE DATABASE sales LOCATION 'dbfs:/mnt/delta/databases/sales.db/'\nNote: with the introduction of the Unity catalog and three-layer namespace usage of SCHEMA and DATABASE is\ninterchangeable\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhat is the type of table created when you issue SQL DDL command CREATE TABLE sales (id int, units int)\n",
        "choices": [
            "Query fails due to missing location",
            "Query fails due to missing format",
            "Managed Delta table",
            "External Table",
            "Managed Parquet table"
        ],
        "answer": "Managed Delta table",
        "explanation": "\nAnswer is Managed Delta table\nAnytime a table is created without the Location keyword it is considered a managed table, by default all managed tables\nDELTA tables\nSyntax\nCREATE TABLE table_name ( column column_data_type\u2026)\n"
    },
    {
        "question": "\nHow to determine if a table is a managed table vs external table?\n",
        "choices": [
            "Run IS_MANAGED(\u2018table_name\u2019) function",
            "All external tables are stored in data lake, managed tables are stored in DELTA lake",
            "All managed tables are stored in unity catalog",
            "Run SQL command DESCRIBE EXTENDED table_name and check type",
            "A. Run SQL command SHOW TABLES to see the type of the table"
        ],
        "answer": "Run SQL command DESCRIBE EXTENDED table_name and check type",
        "explanation": "\nThe answer is Run SQL command DESCRIBE EXTENDED table_name and check type\nExample of External table\nhttps://www.dumps4less.com/\nExample of managed table\n"
    },
    {
        "question": "\nWhich of the below SQL commands creates a session scoped temporary view?\n",
        "choices": [
            "CREATE OR REPLACE TEMPORARY VIEW view_nameAS SELECT * FROM table_name",
            "CREATE OR REPLACE LOCAL TEMPORARY VIEW view_nameAS SELECT * FROM table_name",
            "CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_nameAS SELECT * FROM table_name",
            "CREATE OR REPLACE VIEW view_nameAS SELECT * FROM table_name",
            "CREATE OR REPLACE LOCAL VIEW view_nameAS SELECT * FROM table_name"
        ],
        "answer": "CREATE OR REPLACE TEMPORARY VIEW view_nameAS SELECT * FROM table_name",
        "explanation": "\nThe answer is\nhttps://www.dumps4less.com/\nCREATE OR REPLACE TEMPORARY VIEW view_name\nAS SELECT * FROM table_name\nThe default temporary view is session scoped, as soon as the session ends or if a notebook is detached session scoped\ntemporary view is dropped.\n"
    },
    {
        "question": "\nDrop the customers database and associated tables and data, all of the tables inside the database are managed tables.\n",
        "choices": [
            "DROP DATABASE customers FORCE",
            "DROP DATABASE customers CASCADE",
            "DROP DATABASE customers INCLUDE",
            "All the tables must be dropped first before dropping database",
            "DROP DELTA DATABSE customers"
        ],
        "answer": "DROP DATABASE customers CASCADE",
        "explanation": "\nThe answer is DROP DATABASE customers CASCADE\nDrop database with cascade option drops all the tables, since all of the tables inside the database are managed tables we\ndo not need to perform any additional steps to clean the data in the storage.\n"
    },
    {
        "question": "\nDefine an external SQL table by connecting to a local instance of an SQLite database using JDBC\n",
        "choices": [
            "CREATE TABLE users_jdbcUSING SQLITEOPTIONS ( url = \"jdbc:/sqmple_db\", dbtable = \"users\")",
            "CREATE TABLE users_jdbcUSING SQLURL = {server:\"jdbc:/sqmple_db\",dbtable: \u201cusers\u201d}",
            "CREATE TABLE users_jdbcUSING SQLOPTIONS ( url = \"jdbc:sqlite:/sqmple_db\", dbtable = \"users\")",
            "CREATE TABLE users_jdbcUSING org.apache.spark.sql.jdbc.sqliteOPTIONS ( url = \"jdbc:/sqmple_db\", dbtable =",
            "CREATE TABLE users_jdbcUSING org.apache.spark.sql.jdbcOPTIONS ( url = \"jdbc:sqlite:/sqmple_db\", dbtable ="
        ],
        "answer": "CREATE TABLE users_jdbcUSING org.apache.spark.sql.jdbcOPTIONS ( url = \"jdbc:sqlite:/sqmple_db\", dbtable =",
        "explanation": "\nThe answer is,\nCREATE TABLE users_jdbc\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\nurl = \"jdbc:sqlite:/sqmple_db\",\ndbtable = \"users\"\n)\nDatabricks runtime currently supports connecting to a few flavors of SQL Database including SQL Server, My SQL, SQL\nLite and Snowflake using JDBC.\nCREATE TABLE\nUSING org.apache.spark.sql.jdbc or JDBC\nOPTIONS (\nurl = \"jdbc:://:\",\ndbtable \" = .atable\",\nuser = \"\",\npassword = \"\"\n)\nMore detailed documentation\nSQL databases using JDBC - Azure Databricks | Microsoft Docs\n"
    },
    {
        "question": "\nWhen defining external tables using formats CSV, JSON, TEXT, BINARY any query on the external tables caches the data\nand location for performance reasons, so within a given spark session any new files that may have arrived will not be\navailable after the initial query. How can we address this limitation?\n",
        "choices": [
            "UNCACHE TABLE table_name",
            "CACHE TABLE table_name",
            "REFRESH TABLE table_name",
            "BROADCAST TABLE table_name",
            "CLEAR CACH table_name"
        ],
        "answer": "REFRESH TABLE table_name",
        "explanation": "\nThe answer is REFRESH TABLE table_name\nREFRESH TABLE table_name will force Spark to refresh the availability of external files and any changes.\n"
    },
    {
        "question": "\nWhich of the following table constraints are supported in Delta tables?\nhttps://www.dumpsless.com/\n",
        "choices": [
            "Primary key, foreign key, Not Null, Check Constraints",
            "Primary key, Not Null, Check Constraints",
            "Default, Not Null, Check Constraints",
            "Not Null, Check Constraints",
            "Unique, Not Null, Check Constraints"
        ],
        "answer": "Not Null, Check Constraints",
        "explanation": "\nThe answer is Not Null, Check Constraints\nhttps://docs.microsoft.com/en-us/azure/databricks/delta/delta-constraints\nCREATE TABLE events( id LONG,\ndate STRING,\nlocation STRING,\ndescription STRING\n) USING DELTA\nALTER TABLE events CHANGE COLUMN id SET NOT NULL;\nALTER TABLE events ADD CONSTRAINT dateWithinRange CHECK (date > '1900-01-01');\n"
    },
    {
        "question": "\nThe data engineering team is looking to add a new column to the table, but the QA team would like to test the change\nwhich of the below options allow you to quickly copy the table from Prod to QA environment, modify and run the tests\n",
        "choices": [
            "DEEP CLONE",
            "SHADOW CLONE",
            "ZERO COPY CLONE",
            "SHALLOW CLONE",
            "METADATA CLONE"
        ],
        "answer": "SHALLOW CLONE",
        "explanation": "\nThe answer is SHALLOW CLONE\nSHALLOW CLONE If you wish to create a copy of a table quickly to test out applying changes without the risk of modifying\nthe current table, SHALLOW CLONE can be a good option. Shallow clones just copy the Delta transaction logs, meaning\nthat the data doesn't move so it can be very quick\nDEEP CLONE fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing\nthis command again can sync changes from the source to the target location. It copies all of the data and transaction logs\nthis can take a long time based on the size of the table.\n\n"
    },
    {
        "question": "\nYou are asked to write a python function that can read data from a delta table and return the DataFrame, which of the\nfollowing is correct?\n",
        "choices": [
            "Python function cannot return a DataFrame",
            "Write SQL UDF to return a DataFrame",
            "Write SQL UDF that can return tabular data",
            "Python function will result in out of memory error due to data volume",
            "Python function can return a DataFrame"
        ],
        "answer": "Python function can return a DataFrame",
        "explanation": "\nThe answer is Python function can return a DataFrame\nThe function would something like this,\nget_source_dataframe(tablename):\ndf = spark.read.table(tablename)\nreturn df\ndf = get_source_dataframe('test_table'\nsince there is no action spark returns a Dataframe and assigns to df python variable\n"
    },
    {
        "question": "\nWhich of the following SQL statements can replace a python variable, when the notebook is set in SQL mode\ntable_name = \"sales\"\nschema_name = \"bronze\"\n",
        "choices": [
            "spark.sql(f\"SELECT * FROM f{schema_name.table_name}\")",
            "spark.sql(f\"SELECT * FROM {schem_name.table_name}\")",
            "spark.sql(f\"SELECT * FROM ${schema_name}.${table_name}\")",
            "spark.sql(f\"SELECT * FROM {schema_name}.{table_name}\")",
            "spark.sql(\"SELECT * FROM schema_name.table_name\")"
        ],
        "answer": "spark.sql(f\"SELECT * FROM {schema_name}.{table_name}\")",
        "explanation": "\nThe answer is spark.sql(f\"SELECT * FROM {schema_name}.{table_name}\")\n"
    },
    {
        "question": "\nWhen writing streaming data, Spark\u2019s structured stream supports the below write modes\n",
        "choices": [
            "Append, Delta, Complete",
            "Delta, Complete, Continuous",
            "Append, Complete, Update",
            "Complete, Incremental, Update",
            "Append, overwrite, Continuous"
        ],
        "answer": "Append, Complete, Update",
        "explanation": "\nThe answer is Append, Complete, Update\nAppend mode (default) - This is the default mode, where only the new rows added to the Result Table since the last\ntrigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is\nnever going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant\nsink). For example, queries with only select, where, map, flatMap, filter, join, etc. will support Append mode.\nComplete mode - The whole Result Table will be outputted to the sink after every trigger. This is supported for\naggregation queries.\nUpdate mode - (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will\nbe outputted to the sink. More information to be added in future releases.\n"
    },
    {
        "question": "\nWhen using the complete mode to write stream data, how does it impact the target table?\n",
        "choices": [
            "Entire stream waits for complete data to write",
            "Stream must complete to write the data",
            "Target table cannot be updated while stream is pending",
            "Target table is overwritten for each batch",
            "Delta commits transaction once the stream is stopped"
        ],
        "answer": "Target table is overwritten for each batch",
        "explanation": "\nThe answer is Target table is overwritten for each batch\nComplete mode - The whole Result Table will be outputted to the sink after every trigger. This is supported for\naggregation queries\n"
    },
    {
        "question": "\nAt the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to\ningest data which of the following method can be used to ingest the data incrementally, the schema of the file is\nexpected to change overtime ingestion process should be able to handle these changes automatically. Below is the auto\nloader command to load the data, fill in the blanks for successful execution of the below code.\nspark.readStream\n.format(\"cloudfiles\")\n.option(\"cloudfiles.format\",\u201dcsv)\n.option(\"_______\", \u2018dbfs:/location/checkpoint/\u2019)\n.load(data_source)\n.writeStream\n.option(\"_______\",\u2019 dbfs:/location/checkpoint/\u2019)\n.option(\"mergeSchema\", \"true\")\n.table(table_name))\nhttps://www.dumpsless.com/\n",
        "choices": [
            "checkpointlocation, schemalocation",
            "checkpointlocation, cloudfiles.schemalocation",
            "schemalocation, checkpointlocation",
            "cloudfiles.schemalocation, checkpointlocation",
            "cloudfiles.schemalocation, cloudfiles.checkpointlocation"
        ],
        "answer": "cloudfiles.schemalocation, checkpointlocation",
        "explanation": "\nThe answer is cloudfiles.schemalocation, checkpointlocation\nWhen reading the data cloudfiles.schemalocation is used to store the inferred schema of the incoming data.\nWhen writing a stream to recover from failures checkpointlocation is used to store the offset of the byte that was most\nrecently processed.\n"
    },
    {
        "question": "\nWhen working with AUTO LOADER you noticed that most of the columns that were inferred as part of loading are string\ndata types including columns that were supposed to be integers, how can we fix this?\n",
        "choices": [
            "Provide the schema of the source table in the cloudfiles.schemalocation",
            "Provide the schema of the target table in the cloudfiles.schemalocation",
            "Provide schema hints",
            "Update the checkpoint location",
            "Correct the incoming data by explicitly casting the data types"
        ],
        "answer": "Provide schema hints",
        "explanation": "\nThe answer is, Provide schema hints.\nspark.readStream \\\n.format(\"cloudFiles\") \\\n.option(\"cloudFiles.format\", \"csv\") \\\n.option(\"header\", \"true\") \\\n.option(\"cloudFiles.schemaLocation\", schema_location) \\\n.option(\"cloudFiles.schemaHints\", \"id int, description string\")\n.load(raw_data_location)\n.writeStream \\\n.option(\"checkpointLocation\", checkpoint_location) \\\n.start(target_delta_table_location)\nhttps://www.dumps4less.com/\n.option(\"cloudFiles.schemaHints\", \"id int, description string\")\n# Here we are providing a hint that id column is int and the description is a string\nWhen cloudfiles.schemalocation is used to store the output of the schema inference during the load process, with\nschema hints you can enforce data types for known columns ahead of time.\n"
    },
    {
        "question": "\nYou have configured AUTO LOADER to process incoming IOT data from cloud object storage every  mins, recently a\nchange was made to the notebook code to update the processing logic but the team later realized that the notebook was\nfailing for the last  hours, what steps team needs to take to reprocess the data that was not loaded after the notebook\nwas corrected?\n",
        "choices": [
            "Move the files that were not processed to another location and manually copy the files into the ingestion path to",
            "Enable back_fill = TRUE to reprocess the data",
            "Delete the checkpoint folder and run the autoloader again",
            "Autoloader automatically re-processes data that was not loaded",
            "Manually re-load the data"
        ],
        "answer": "Autoloader automatically re-processes data that was not loaded",
        "explanation": "\nThe answer is,\nAutoloader automatically re-processes data that was not loaded using the checkpoint.\n"
    },
    {
        "question": "\nWhich of the following Structured Streaming queries is performing a hop from a bronze table to a Silver table?\n",
        "choices": [
            "(spark.table(\"sales\").groupBy(\"store\").agg(sum(\"sales\")).writeStream.option(\"checkpointLocation\",checkpointPath)",
            "(spark.table(\"sales\").agg(sum(\"sales\"),sum(\"units\")).writeStream.option(\"checkpointLocation\",checkpointPath)",
            "(spark.table(\"sales\").withColumn(\"avgPrice\", col(\"sales\") / col(\"units\")).writeStream.option(\"checkpointLocation\",",
            "(spark.readStream.load(rawSalesLocation).writeStream .option(\"checkpointLocation\",",
            "(spark.read.load(rawSalesLocation) .writeStream .option(\"checkpointLocation\","
        ],
        "answer": "(spark.table(\"sales\").withColumn(\"avgPrice\", col(\"sales\") / col(\"units\")).writeStream.option(\"checkpointLocation\",",
        "explanation": "\nThe question is asking to identify a structured streaming command that is moving data from bronze to silver.\nThe answer is\n(spark.table(\"sales\")\n.withColumn(\"avgPrice\", col(\"sales\") / col(\"units\"))\n.writeStream\n.option(\"checkpointLocation\", checkpointPath)\n.outputMode(\"append\")\n.table(\"cleanedSales\"))\nWe are preserving the grain of incoming data and enriching the data by adding avg price, the other options listed use\naggregations which are mostly performed on top of the silver to move data to Gold.\n"
    },
    {
        "question": "\nWhich of the following Structured Streaming queries successfully performs a hop from a Silver to Gold table?\n",
        "choices": [
            "(spark.table(\"sales\") .groupBy(\"store\") .agg(sum(\"sales\")) .writeStream .option(\"checkpointLocation\",",
            "(spark.table(\"sales\") .writeStream .option(\"checkpointLocation\",",
            "(spark.table(\"sales\") .withColumn(\"avgPrice\", col(\"sales\") / col(\"units\")) .writeStream .option(\"checkpointLocation\",",
            "(spark.readStream.load(rawSalesLocation) .writeStream .option(\"checkpointLocation\",",
            "(spark.read.load(rawSalesLocation) .writeStream .option(\"checkpointLocation\", checkpointPath)"
        ],
        "answer": "(spark.table(\"sales\") .groupBy(\"store\") .agg(sum(\"sales\")) .writeStream .option(\"checkpointLocation\",",
        "explanation": "\nThe answer is\n(spark.table(\"sales\")\n.groupBy(\"store\")\n.agg(sum(\"sales\"))\n.writeStream\n.option(\"checkpointLocation\", checkpointPath)\n.outputMode(\"complete\")\n.table(\"aggregatedSales\") )\nThe gold layer is normally used to store aggregated data\nReview the below link for more info,\nMedallion Architecture \u2013 Databricks\nhttps://www.dumps4less.com/\nGold Layer\n1. Powers Ml applications, reporting, dashboards, ad hoc analytics\n2. Refined views of data, typically with aggregations\n3. Reduces strain on production systems\n4. Optimizes query performance for business-critical data\n"
    },
    {
        "question": "\nWhich of the following Auto loader structured streaming commands successfully performs a hop from the landing area\ninto Bronze?\n",
        "choices": [
            "spark\\.readStream\\.format(\"csv\")\\.option(\"cloudFiles.schemaLocation\",",
            "spark\\.readStream\\.format(\"cloudFiles\")\\.option(\"cloudFiles.format\",\"csv\")\\.option(\"cloudFiles.schemaLocation\",",
            "spark\\.read\\.format(\"cloudFiles\")\\.option(\"cloudFiles.format\",\u201dcsv\u201d)\\.option(\"cloudFiles.schemaLocation\",",
            "spark\\.readStream\\.load(rawSalesLocation)\\.writeStream \\.option(\"checkpointLocation\",",
            "spark\\.read\\.load(rawSalesLocation) \\.writeStream\\.option(\"checkpointLocation\", checkpointPath)"
        ],
        "answer": "spark\\.readStream\\.format(\"cloudFiles\")\\.option(\"cloudFiles.format\",\"csv\")\\.option(\"cloudFiles.schemaLocation\",",
        "explanation": "\nThe answer is\nspark\\\n.readStream\\\n.format(\"cloudFiles\") \\# use Auto loader\n.option(\"cloudFiles.format\",\"csv\") \\ # csv format files\n.option(\"cloudFiles.schemaLocation\", checkpoint_directory)\\\n.load('landing')\\\n.writeStream.option(\"checkpointLocation\", checkpoint_directory)\\\n.table(raw\nNote: if you chose the below option which is incorrect because it does not have readStream\nspark.read.format(\"cloudFiles\")\n.option(\"cloudFiles.format\",\u201dcsv\u201d)\n...\n..\n..\n"
    },
    {
        "question": "\nWhat are two different modes of DELTA LIVE TABLE Pipelines\nhttps://www.dumpsless.com/\n",
        "choices": [
            "Triggered, Incremental",
            "Once, Continuous",
            "Triggered, Continuous",
            "Once, Incremental",
            "Continuous, Incremental"
        ],
        "answer": "Triggered, Continuous",
        "explanation": "\nThe answer is Triggered, Continuous\nhttps://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-concepts#--\ncontinuous-and-triggered-pipelines\nTriggered pipelines update each table with whatever data is currently available and then stop the cluster running the\npipeline. Delta Live Tables automatically analyzes the dependencies between your tables and starts by computing those\nthat read from external sources. Tables within the pipeline are updated after their dependent data sources have been\nupdated.\nContinuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run\nuntil manually stopped. Continuous pipelines require an always-running cluster but ensure that downstream consumers\nhave the most up-to-date data.\n"
    },
    {
        "question": "\nYour team member is trying to set up a delta pipeline and build a second gold table to the same pipeline with aggregated\nmetrics based on an existing Delta Live table called sales_orders_cleaned but he is facing a problem in starting the\npipeline, the pipeline is failing to state it cannot find the table sales_orders_cleaned, you are asked to identify and fix the\nproblem.\nCREATE LIVE TABLE sales_order_in_chicago\nAS\nSELECT order_date, city, sum(price) as sales,\nFROM sales_orders_cleaned\nWHERE city = 'Chicago')\nGROUP BY order_date, city\n",
        "choices": [
            "Use STREAMING LIVE instead of LIVE table",
            "Delta live table can be used in a group by clause",
            "Delta live tables pipeline can only have one table",
            "Sales_orders_cleaned table is missing schema name LIVE",
            "The pipeline needs to be deployed so the first table is created before we add a second table"
        ],
        "answer": "Sales_orders_cleaned table is missing schema name LIVE",
        "explanation": "\nThe answer is, Sales_orders_cleaned table is missing schema name LIVE\nEvery Delta live table should have schema LIVE\nHere is the correct syntax,\nCREATE LIVE TABLE sales_order_in_chicago\nAS\nSELECT order_date, city, sum(price) as sales,\nFROM LIVE.sales_orders_cleaned\nWHERE city = 'Chicago')\nGROUP BY order_date, city\n"
    },
    {
        "question": "\nWhich of the following type of tasks cannot setup through a job?\n",
        "choices": [
            "Notebook",
            "DELTA LIVE PIPELINE",
            "Spark Submit",
            "Python",
            "Databricks SQL Dashboard refresh"
        ],
        "answer": "Databricks SQL Dashboard refresh",
        "explanation": "125\nWhich of the following type of tasks cannot setup through a job?\nNotebook\nA.\nDELTA LIVE PIPELINE\nB.\nSpark Submit\nC.\nPython\nD.\nDatabricks SQL Dashboard refresh\nE.\nAnswer: E\n"
    },
    {
        "question": "\nWhich of the following approaches can the data engineer use to obtain a version-controllable configuration of the Job\u2019s\nschedule and configuration?\n",
        "choices": [
            "They can link the Job to notebooks that are a part of a Databricks Repo.",
            "They can submit the Job once on a Job cluster.",
            "They can download the JSON equivalent of the job from the Job\u2019s page.",
            "They can submit the Job once on an all-purpose cluster.",
            "They can download the XML description of the Job from the Job\u2019s page"
        ],
        "answer": "They can download the JSON equivalent of the job from the Job\u2019s page.",
        "explanation": "126\nWhich of the following approaches can the data engineer use to obtain a version-controllable configuration of the Job\u2019s\nschedule and configuration?\nThey can link the Job to notebooks that are a part of a Databricks Repo.\nA.\nThey can submit the Job once on a Job cluster.\nB.\nThey can download the JSON equivalent of the job from the Job\u2019s page.\nC.\nThey can submit the Job once on an all-purpose cluster.\nD.\nThey can download the XML description of the Job from the Job\u2019s page\nE.\nAnswer: C\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nwhat steps need to be taken to set up a DELTA LIVE PIPELINE as a job?\n",
        "choices": [
            "DELTA LIVE TABLES do not support job cluster",
            "Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the notebook",
            "Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the pipeline",
            "Use Pipeline creation UI, select a new pipeline and job cluster"
        ],
        "answer": "Select Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the notebook",
        "explanation": "\nThe answer is,\nSelect Workflows UI and Delta live tables tab, under task type select Delta live tables pipeline and select the notebook.\nCreate a pipeline\nTo create a new pipeline using the Delta Live Tables notebook:\nClick Workflows in the sidebar, click the Delta Live Tables tab, and click Create Pipeline.\nGive the pipeline a name and click to select a notebook.\nOptionally enter a storage location for output data from the pipeline. The system uses a default location if you leave\nStorage Location empty.\nSelect Triggered for Pipeline Mode.\nClick Create.\nThe system displays the Pipeline Details page after you click Create. You can also access your pipeline by clicking the\npipeline name in the Delta Live Tables tab.\n"
    },
    {
        "question": "\nData engineering team has provided  queries and asked Data Analyst team to build a dashboard and refresh the data\nevery day at  AM, identify the best approach to set up data refresh for this dashaboard?\nEach query requires a separate task and setup  tasks under a single job to run at  AM to refresh the dashboard\nA.\nThe entire dashboard with  queries can be refreshed at once, single schedule needs to be set up to refresh at  AM.\nB.\nSetup JOB with linear dependency to all load all  queries into a table so the dashboard can be refreshed at once.\nC.\nA dashboard can only refresh one query at a time,  schedules to set up the refresh.\nD.\nUse Incremental refresh to run at  AM every day.\nE.\nAnswer: B\nhttps://www.dumpsless.com/\nExplanation/Reference:\nThe answer is,\nThe entire dashboard with  queries can be refreshed at once, single schedule needs to be set up to refresh at  AM.\nAutomatically refresh a dashboard\nA dashboard\u2019s owner and users with the Can Edit permission can configure a dashboard to automatically refresh on a\nschedule. To automatically refresh a dashboard:\nClick the Schedule button at the top right of the dashboard. The scheduling dialog appears.\nIn the Refresh every drop-down, select a period.\nIn the SQL Warehouse drop-down, optionally select a SQL warehouse to use for all the queries. If you don\u2019t select a\nwarehouse, the queries execute on the last used SQL warehouse.\nNext to Subscribers, optionally enter a list of email addresses to notify when the dashboard is automatically updated.\nEach email address you enter must be associated with a Azure Databricks account or configured as an alert destination.\nClick Save. The Schedule button label changes to Scheduled.\n",
        "choices": [
            "Each query requires a separate task and setup 10 tasks under a single job to run at 8 AM to refresh the dashboard",
            "The entire dashboard with 10 queries can be refreshed at once, single schedule needs to be set up to refresh at 8 AM.",
            "Setup JOB with linear dependency to all load all 10 queries into a table so the dashboard can be refreshed at once.",
            "A dashboard can only refresh one query at a time, 10 schedules to set up the refresh.",
            "Use Incremental refresh to run at 8 AM every day."
        ],
        "answer": "The entire dashboard with 10 queries can be refreshed at once, single schedule needs to be set up to refresh at 8 AM.",
        "explanation": "\nThe answer is,\nThe entire dashboard with 10 queries can be refreshed at once, single schedule needs to be set up to refresh at 8 AM.\nAutomatically refresh a dashboard\nA dashboard\u2019s owner and users with the Can Edit permission can configure a dashboard to automatically refresh on a\nschedule. To automatically refresh a dashboard:\nClick the Schedule button at the top right of the dashboard. The scheduling dialog appears.\nIn the Refresh every drop-down, select a period.\nIn the SQL Warehouse drop-down, optionally select a SQL warehouse to use for all the queries. If you don\u2019t select a\nwarehouse, the queries execute on the last used SQL warehouse.\nNext to Subscribers, optionally enter a list of email addresses to notify when the dashboard is automatically updated.\nEach email address you enter must be associated with a Azure Databricks account or configured as an alert destination.\nClick Save. The Schedule button label changes to Scheduled.\n"
    },
    {
        "question": "\nThe data engineering team is using a SQL query to review data completeness every day to monitor the ETL job, and\nquery output is being used in multiple dashboards which of the following approaches can be used to set up a schedule\nand automate this process?\n",
        "choices": [
            "They can schedule the query to run every day from the Jobs UI.",
            "They can schedule the query to refresh every day from the query\u2019s page in Databricks SQL",
            "They can schedule the query to run every 12 hours from the Jobs UI.",
            "They can schedule the query to refresh every day from the SQL endpoint\u2019s page in Databricks SQL.",
            "They can schedule the query to refresh every 12 hours from the SQL endpoint\u2019s page in Databricks SQL"
        ],
        "answer": "They can schedule the query to refresh every day from the query\u2019s page in Databricks SQL",
        "explanation": "\nThe answer is They can schedule the query to refresh every 12 hours from the SQL endpoint\u2019s page in Databricks SQL,\nThe query pane view in Databricks SQL workspace provides the ability to add or edit and schedule individual queries to\nrun.\nYou can use scheduled query executions to keep your dashboards updated or to enable routine alerts. By default, your\nqueries do not have a schedule.\nNote\nIf your query is used by an alert, the alert runs on its own refresh schedule and does not use the query schedule.\nTo set the schedule:\nClick the query info tab.\nClick the link to the right of Refresh Schedule to open a picker with schedule intervals.\nSet the schedule.\nThe picker scrolls and allows you to choose:\nAn interval: 1-30 minutes, 1-12 hours, 1 or 30 days, 1 or 2 weeks\nA time. The time selector displays in the picker only when the interval is greater than 1 day and the day selection is\ngreater than 1 week. When you schedule a specific time, Databricks SQL takes input in your computer\u2019s timezone and\nconverts it to UTC. If you want a query to run at a certain time in UTC, you must adjust the picker by your local offset. For\nexample, if you want a query to execute at 00:00 UTC each day, but your current timezone is PDT (UTC-7), you should\nselect 17:00 in the picker:\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nA data engineer is using a Databricks SQL query to monitor the performance of an ELT job. The ELT job is triggered by a\nspecific number of input records being ready to process. The Databricks SQL query returns the number of minutes since\nthe job\u2019s most recent runtime. Which of the following approaches can enable the data engineering team to be notified if\nthe ELT job has not been run in an hour?\nThey can set up an Alert for the accompanying dashboard to notify them if the returned value is greater than .\nA.\nThey can set up an Alert for the query to notify when the ELT job fails.\nB.\nThey can set up an Alert for the accompanying dashboard to notify when it has not refreshed in  minutes.\nC.\nThey can set up an Alert for the query to notify them if the returned value is greater than .\nD.\nThis type of alert is not possible in Databricks\nE.\nAnswer: D\nExplanation/Reference:\nThe answer is, They can set up an Alert for the query to notify them if the returned value is greater than .\nThe important thing to note here is that alert can only be setup on query not on the dashboard, query can return a value,\nwhich is used if alert can be triggered.\n",
        "choices": [
            "They can set up an Alert for the accompanying dashboard to notify them if the returned value is greater than 60.",
            "They can set up an Alert for the query to notify when the ELT job fails.",
            "They can set up an Alert for the accompanying dashboard to notify when it has not refreshed in 60 minutes.",
            "They can set up an Alert for the query to notify them if the returned value is greater than 60.",
            "This type of alert is not possible in Databricks"
        ],
        "answer": "They can set up an Alert for the query to notify them if the returned value is greater than 60.",
        "explanation": "\nThe answer is, They can set up an Alert for the query to notify them if the returned value is greater than 60.\nThe important thing to note here is that alert can only be setup on query not on the dashboard, query can return a value,\nwhich is used if alert can be triggered.\n"
    },
    {
        "question": "\nWhich of the following is true, when building a Databricks SQL dashboard?\n",
        "choices": [
            "A dashboard can only use results from one query",
            "Only one visualization can be developed with one query result",
            "A dashboard can only connect to one schema/Database",
            "More than one visualization can be developed using a single query result",
            "A dashboard can only have one refresh schedule"
        ],
        "answer": "More than one visualization can be developed using a single query result",
        "explanation": "\nthe answer is, More than one visualization can be developed using a single query result.\nIn the query editor pane + Add visualization tab can be used for many visualizations for a single query result.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nA newly joined team member John Smith in the Marketing team currently has access read access to sales tables but does\nnot have access to update the table, which of the following commands help you accomplish this?\n",
        "choices": [
            "GRANT UPDATE ON TABLE table_name TO john.smith@marketing.com",
            "GRANT USAGE ON TABLE table_name TO john.smith@marketing.com",
            "GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com",
            "GRANT UPDATE TO TABLE table_name ON john.smith@marketing.com",
            "GRANT MODIFY TO TABLE table_name ON john.smith@marketing.com"
        ],
        "answer": "GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com",
        "explanation": "\nThe answer is GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com\nhttps://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#privileges\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nA new user who currently does not have access to the catalog or schema is requesting access to the customer table in\nsales schema, but the customer table contains sensitive information, so you have decided to create view on the table\nexcluding columns that are sensitive and granted access to the view GRANT SELECT ON view_name to\nuser@company.com but when the user tries to query the view, gets the error view does not exist. What is the issue\npreventing user to access the view and how to fix it?\n",
        "choices": [
            "User requires SELECT on the underlying table",
            "User requires to be put in a special group that has access to PII data",
            "User has to be the owner of the view",
            "User requires USAGE privilege on Sales schema",
            "User needs ADMIN privilege on the view"
        ],
        "answer": "User requires USAGE privilege on Sales schema",
        "explanation": "\nThe answer is User requires USAGE privilege on Sales schema,\nData object privileges - Azure Databricks | Microsoft Docs\nGRANT USAGE ON SCHEMA sales TO user@company.com;\nUSAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.\n"
    },
    {
        "question": "\nHow do you access or use tables in the unity catalog?\n",
        "choices": [
            "schema_name.table_name",
            "schema_name.catalog_name.table_name",
            "catalog_name.table_name",
            "catalog_name.database_name.schema_name.table_name",
            "catalog_name.schema_name.table_name"
        ],
        "answer": "catalog_name.schema_name.table_name",
        "explanation": "\nThe answer is catalog_name.schema_name.table_name\nhttps://www.dumps4less.com/\nnote: Database and Schema are analogous they are interchangeably used in the Unity catalog.\n"
    },
    {
        "question": "\nHow do you upgrade an existing workspace managed table to a unity catalog table?\n",
        "choices": [
            "ALTER TABLE table_name SET UNITY_CATALOG = TRUE",
            "Create table catalog_name.schema_name.table_nameas select * from hive_metastore.old_schema.old_table",
            "Create table table_name as select * from hive_metastore.old_schema.old_table",
            "Create table table_name format = UNITY as select * from old_table_name",
            "Create or replace table_name format = UNITY using deep clone old_table_name"
        ],
        "answer": "Create table catalog_name.schema_name.table_nameas select * from hive_metastore.old_schema.old_table",
        "explanation": "\nThe answer is Create table catalog_name.schema_name.table_name as select * from\nhive_metastore.old_schema.old_table\nBasically, we are moving the data from an internal hive metastore to a metastore and catalog that is registered in Unity\ncatalog.\nnote: if it is a managed table the data is copied to a different storage account, for a large tables this can take a lot of\ntime. For an external table the process is different.\nManaged table: Upgrade a managed to Unity Catalog\nExternal table: Upgrade an external table to Unity Catalog\n"
    },
    {
        "question": "\nWhich of the statements are correct when choosing between lakehouse and Datawarehouse?\n",
        "choices": [
            "Traditional Data warehouses have special indexes which are optimized for Machine learning",
            "Traditional Data warehouses can serve low query latency with high reliability for BI workloads",
            "SQL support is only available for Traditional Datawarehouse\u2019s, Lakehouses support Python and Scala",
            "Traditional Data warehouses are the preferred choice if we need to support ACID, Lakehouse does not support ACID.",
            "Lakehouse replaces the current dependency on data lakes and data warehouses uses an open standard storage"
        ],
        "answer": "Lakehouse replaces the current dependency on data lakes and data warehouses uses an open standard storage",
        "explanation": "\nThe lakehouse replaces the current dependency on data lakes and data warehouses for modern data companies that\ndesire:\n\u00b7 Open, direct access to data stored in standard data formats.\n\u00b7 Indexing protocols optimized for machine learning and data science.\n\u00b7 Low query latency and high reliability for BI and advanced analytics.\n"
    },
    {
        "question": "\nWhere are Interactive notebook results stored in Databricks product architecture?\n",
        "choices": [
            "Data plane",
            "Control plane",
            "Data and Control plane",
            "JDBC data source",
            "Databricks web application"
        ],
        "answer": "Data and Control plane",
        "explanation": "\nThe answer is Data and Control plane,\nInteractive notebook results are stored in a combination of the control plane (partial results for presentation in the UI)\nand customer storage.\nhttps://docs.microsoft.com/en-us/azure/databricks/getting-started/overview#--high-level-architecture\n"
    },
    {
        "question": "\nWhich of the following statements are true about a lakehouse?\n",
        "choices": [
            "Lakehouse only supports Machine learning workloads and Data warehouses support BI workloads",
            "Lakehouse only supports end-to-end streaming workloads and Data warehouses support Batch workloads",
            "Lakehouse does not support ACID",
            "Lakehouse do not support SQL",
            "Lakehouse supports Transactions"
        ],
        "answer": "Lakehouse supports Transactions",
        "explanation": "\nWhat Is a Lakehouse? - The Databricks Blog\n"
    },
    {
        "question": "\nYou had setup a new all-purpose cluster, but the cluster is unable to start which of the following steps do you need to\nhttps://www.dumpsless.com/\ntake to resolve this issue?\n",
        "choices": [
            "Check the cluster driver logs",
            "Check the cluster event logs",
            "Workspace logs",
            "Storage account",
            "Data plane"
        ],
        "answer": "Check the cluster event logs",
        "explanation": "\nCluster event logs are very useful, to identify issues pertaining to cluster availability. Cluster may not start due to\nresource limitations or issues with cloud provider.\n"
    },
    {
        "question": "\nWhich of the following developer operations in CI/CD flow can be implemented in Databricks Repos?\n",
        "choices": [
            "Delete branch",
            "Trigger Databricks CICD pipeline",
            "Commit and push code",
            "Create a pull request",
            "Approve the pull request"
        ],
        "answer": "Commit and push code",
        "explanation": "\nThe answer is Commit and push code.\nSee the below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workflow\nAll the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git\nprovider like Github or Azure Devops\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou noticed that a team member is using an all-purpose cluster to run a job every  mins, what would you recommend\nin reducing the compute cost.\n",
        "choices": [
            "Reduce the size of the cluster",
            "Reduce the number of nodes and enable auto scale",
            "Enable auto termination after 30 mins",
            "Change the cluster all-purpose to job cluster",
            "Change the cluster mode from all-purpose to single-mode"
        ],
        "answer": "Change the cluster all-purpose to job cluster",
        "explanation": "\nAnytime you don't need to interact with a notebook, especially for a scheduled job use job cluster. Using all-purpose\ncluster can be twice as expensive as a job cluster.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the following commands can be used to run one notebook from another notebook?\n",
        "choices": [
            "notebook.utils.run(\"full notebook path\")",
            "execute.utils.run(\"full notebook path\")",
            "dbutils.notebook.run(\"full notebook path\")",
            "only job clusters can run notebook",
            "spark.notebook.run(\"full notebook path\")"
        ],
        "answer": "dbutils.notebook.run(\"full notebook path\")",
        "explanation": "\nThe answer is dbutils.notebook.run(\" full notebook path \")\nHere is the full command with additional options.\nrun(path: String, timeout_seconds: int, arguments: Map): String\ndbutils.notebook.run(\"ful-notebook-name\", 60, {\"argument\": \"data\", \"argument2\": \"data2\", ...})\n"
    },
    {
        "question": "\nWhich of the following SQL command can be used to insert or update or delete rows based on a condition to check if a\nrow(s) exists?\n",
        "choices": [
            "MERGE INTO table_name",
            "COPY INTO table_name",
            "UPDATE table_name",
            "INSERT INTO OVERWRITE table_name",
            "INSERT IF EXISTS table_name"
        ],
        "answer": "MERGE INTO table_name",
        "explanation": "\nhere is the additional documentation for your review.\nhttps://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-merge-into.html\nMERGE INTO target_table_name [target_alias]\nUSING source_table_reference [source_alias]\nON merge_condition\nhttps://www.dumps4less.com/\n[ WHEN MATCHED [ AND condition ] THEN matched_action ] [...]\n[ WHEN NOT MATCHED [ AND condition ] THEN not_matched_action ] [...]\nmatched_action\n{ DELETE |\nUPDATE SET * |\nUPDATE SET { column1 = value1 } [, ...] }\nnot_matched_action\n{ INSERT * |\nINSERT (column1 [, ...] ) VALUES (value1 [, ...])\n"
    },
    {
        "question": "\nWhen investigating a data issue, if you are looking compare the current version of the data with the previous version,\nwhat is the best way to query historical data?\n",
        "choices": [
            "SELECT * FROM TIME_TRAVEL(table_name) where time_stamp = 'timestamp'",
            "TIME_TRAVEL FROM table_name where time_stamp = 'timestamp'",
            "SELECT * FROM table_name as of 'timestamp'",
            "DISCRIBE HISTORY table_name as of 'timestmap'",
            "SHOW HISTORY table_name as of 'timestmap'"
        ],
        "answer": "SELECT * FROM table_name as of 'timestamp'",
        "explanation": "\nThe answer is SELECT * FROM table_name as of 'timestamp'\nFYI, Time travel supports two ways one is using timestamp and the second way is using version number,\nTimestamp:\nSELECT count(*) FROM my_table TIMESTAMP AS OF \"2019-01-01\"\nSELECT count(*) FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)\nSELECT count(*) FROM my_table TIMESTAMP AS OF \"2019-01-01 01:30:00.000\"\nVersion Number:\nSELECT count(*) FROM my_table VERSION AS OF 5238\nSELECT count(*) FROM my_table@v5238\nSELECT count(*) FROM delta.`/path/to/my/table@v5238`\nhttps://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\n"
    },
    {
        "question": "\nWhile querying the previous version of the table using time travel you realized you are no longer able to query the\nhistorical data?\n",
        "choices": [
            "You currently do not have access to view historical data",
            "By default, historical data is cleaned every 180 days in DELTA",
            "A command VACUUM table_name RETAIN 0 was ran",
            "Time travel is disabled",
            "Time travel must be enabled before you query previous data"
        ],
        "answer": "A command VACUUM table_name RETAIN 0 was ran",
        "explanation": "\nThe answer is, VACUUM table_name RETAIN 0 was ran\nRecursively vacuum directories associated with the Delta table and remove data files that are no longer in the latest\nstate of the transaction log for the table and are older than a retention threshold. Default is 7 Days.\nWhen VACUUM table_name RETAIN 0 is ran all of the historical versions of data are lost time travel can only provide the\ncurrent state.\n"
    },
    {
        "question": "\nYou have accidentally deleted records from a table called transactions, what is the easiest way to restore the records\ndeleted or the previous state of the table?\n",
        "choices": [
            "RESTORE TABLE transactions FROM VERSION as of 'timestamp'",
            "RESTORE TABLE transactions TO VERSION as of 'timestamp'",
            "INSERT INTO OVERWRITE transactionsSELECT * FROM transactions as of \u2018timestamp\u2019MINUSSELECT * FROM",
            "INSERT INTO OVERWRITE transactionsSELECT * FROM transactions as of \u2018timestamp\u2019INTERSECTSELECT * FROM",
            "COPY OVERWRITE transactions from VERSION as of 'timestamp'"
        ],
        "answer": "RESTORE TABLE transactions TO VERSION as of 'timestamp'",
        "explanation": "\nRESTORE (Databricks SQL) | Databricks on AWS\nRESTORE [TABLE] table_name [TO] time_travel_version\nTime travel supports using timestamp or version number\ntime_travel_version\n{ TIMESTAMP AS OF timestamp_expression |\nVERSION AS OF version }\ntimestamp_expression can be any one of:\n'2018-10-18T22:15:12.013Z', that is, a string that can be cast to a timestamp\ncast('2018-10-18 13:36:32 CEST' as timestamp)\n'2018-10-18', that is, a date string\ncurrent_timestamp() - interval 12 hours\ndate_sub(current_date(), 1)\nhttps://www.dumps4less.com/\nAny other expression that is or can be cast to a timestamp\n"
    },
    {
        "question": "\nCreate schema called bronze using location \u2018/mnt/delta/bronze\u2019, check if schema exists before creating.\n",
        "choices": [
            "CREATE SCHEMA IF NOT EXISTS bronze LOCATION '/mnt/delta/bronze'",
            "CREATE SCHEMA bronze IF NOT EXISTS LOCATION '/mnt/delta/bronze'",
            "if IS_SCHEMA('bronze'): CREATE SCHEMA bronze LOCATION '/mnt/delta/bronze'",
            "Schema creation is not available in metastore, it can only be done in Unity catalog UI",
            "Cannot create schema without a database"
        ],
        "answer": "CREATE SCHEMA IF NOT EXISTS bronze LOCATION '/mnt/delta/bronze'",
        "explanation": "\nhttps://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-schema.html\nCREATE SCHEMA [ IF NOT EXISTS ] schema_name [ LOCATION schema_directory ]\n"
    },
    {
        "question": "\nHow do you check the location of an existing schema in Delta Lake?\n",
        "choices": [
            "Run SQL command SHOW LOCATION schema_name",
            "Check unity catalog UI",
            "Use Data explorer",
            "Run SQL command DESCRIBE SCHEMA EXTENDED schema_name",
            "Schemas are internally in-store external hive meta stores like MySQL or SQL Server"
        ],
        "answer": "Run SQL command DESCRIBE SCHEMA EXTENDED schema_name",
        "explanation": "\nHere is an example of how it looks\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the below SQL commands create a Global temporary view?\n",
        "choices": [
            "CREATE OR REPLACE TEMPORARY VIEW view_name AS SELECT * FROM table_name",
            "CREATE OR REPLACE LOCAL TEMPORARY VIEW view_name AS SELECT * FROM table_name",
            "CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_name AS SELECT * FROM table_name",
            "CREATE OR REPLACE VIEW view_name AS SELECT * FROM table_name",
            "CREATE OR REPLACE LOCAL VIEW view_name AS SELECT * FROM table_name"
        ],
        "answer": "CREATE OR REPLACE GLOBAL TEMPORARY VIEW view_name AS SELECT * FROM table_name",
        "explanation": "\nCREATE OR REPLACE GLOBAL TEMPORARY VIEW view_name\nAS SELECT * FROM table_name\nThere are two types of temporary views that can be created Local and Global\nA session-scoped temporary view is only available with a spark session, so another notebook in the same cluster can not\naccess it. if a notebook is detached and reattached local temporary view is lost.\nA global temporary view is available to all the notebooks in the cluster but if a cluster restarts a global temporary view is\nlost.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhen you drop a managed table using SQL syntax DROP TABLE table_name how does it impact metadata, history, and\ndata stored in the table?\n",
        "choices": [
            "Drops table from meta store, drops metadata, history, and data in storage.",
            "Drops table from meta store and data from storage but keeps metadata and history in storage",
            "Drops table from meta store, meta data and history but keeps the data in storage",
            "Drops table but keeps meta data, history and data in storage",
            "Drops table and history but keeps meta data and data in storage"
        ],
        "answer": "Drops table from meta store, drops metadata, history, and data in storage.",
        "explanation": "\nFor a managed table, a drop command will drop everything from metastore and storage.\n"
    },
    {
        "question": "\nThe team has decided to take advantage of table properties to identify a business owner for each table, which of the\nfollowing table DDL syntax allows you to populate a table property identifying the business owner of a table\n",
        "choices": [
            "CREATE TABLE inventory (id INT, units FLOAT)SET TBLPROPERTIES business_owner = 'supply chain'",
            "CREATE TABLE inventory (id INT, units FLOAT)TBLPROPERTIES (business_owner = 'supply chain')",
            "CREATE TABLE inventory (id INT, units FLOAT)SET (business_owner = \u2018supply chain\u2019)",
            "CREATE TABLE inventory (id INT, units FLOAT)SET PROPERTY (business_owner = \u2018supply chain\u2019)",
            "CREATE TABLE inventory (id INT, units FLOAT)SET TAG (business_owner = \u2018supply chain\u2019)"
        ],
        "answer": "CREATE TABLE inventory (id INT, units FLOAT)TBLPROPERTIES (business_owner = 'supply chain')",
        "explanation": "\nCREATE TABLE inventory (id INT, units FLOAT) TBLPROPERTIES (business_owner = \u2018supply chain\u2019)\nTable properties and table options (Databricks SQL) | Databricks on AWS\nAlter table command can used to update the TBLPROPERTIES\nALTER TABLE inventory SET TBLPROPERTIES(business_owner , 'operations')\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nData science team has requested they are missing a column in the table called average price, this can be calculated\nusing units sold and sales amt, which of the following SQL statements allow you to reload the data with additional column\n",
        "choices": [
            "INSERT OVERWRITE salesSELECT *, salesAmt/unitsSold as avgPrice FROM sales",
            "CREATE OR REPALCE TABLE salesAS SELECT *, salesAmt/unitsSold as avgPrice FROM sales",
            "MERGE INTO sales USING (SELECT *, salesAmt/unitsSold as avgPrice FROM sales)",
            "OVERWRITE sales AS SELECT *, salesAmt/unitsSold as avgPrice FROM sales",
            "COPY INTO SALES AS SELECT *, salesAmt/unitsSold as avgPrice FROM sales"
        ],
        "answer": "CREATE OR REPALCE TABLE salesAS SELECT *, salesAmt/unitsSold as avgPrice FROM sales",
        "explanation": "\nCREATE OR REPALCE TABLE sales\nAS SELECT *, salesAmt/unitsSold as avgPrice FROM sales\nThe main difference between INSERT OVERWRITE and CREATE OR REPLACE TABLE(CRAS) is that CRAS can modify the\nschema of the table, i.e it can add new columns or change data types of existing columns. By default INSERT OVERWRITE\nonly overwrites the data.\nINSERT OVERWRITE can also be used to overwrite schema, only when spark.databricks.delta.schema.autoMerge.enabled\nis set true if this option is not enabled and if there is a schema mismatch command will fail.\n"
    },
    {
        "question": "\nYou are working on a process to load external CSV files into a Delta by leveraging the COPY INTO command, but after\nrunning the command for the second time no data was loaded into the table name, why is that?\nCOPY INTO table_name\nFROM 'dbfs:/mnt/raw/*.csv'\nFILEFORMAT = CSV\n",
        "choices": [
            "COPY INTO only works one time data load",
            "Run REFRESH TABLE sales before running COPY INTO",
            "COPY INTO did not detect new files after the last load",
            "Use incremental = TRUE option to load new files",
            "COPY INTO does not support incremental load, use AUTO LOADER"
        ],
        "answer": "COPY INTO did not detect new files after the last load",
        "explanation": "\nThe answer is COPY INTO did not detect new files after the last load,\nCOPY INTO keeps track of files that were successfully loaded into the table, the next time when the COPY INTO runs it\nskips them.\nFYI, you can change this behavior by using COPY_OPTIONS 'force'= 'true', when this option is enabled all files in the\npath/pattern are loaded.\nCOPY INTO table_identifier\nFROM [ file_location | (SELECT identifier_list FROM file_location) ]\nFILEFORMAT = data_source\n[FILES = [file_name, ... | PATTERN = 'regex_pattern']\n[FORMAT_OPTIONS ('data_source_reader_option' = 'value', ...)]\n[COPY_OPTIONS 'force' = ('false'|'true')]\n"
    },
    {
        "question": "\nWhat is the main difference between the below two commands?\nINSERT OVERWRITE table_name\nSELECT * FROM table\nCREATE OR REPLACE TABLE table_name\nAS SELECT * FROM table\n",
        "choices": [
            "INSERT OVERWRITE replaces data by default, CREATE OR REPLACE replaces data and Schema by default",
            "INSERT OVERWRITE replaces data and schema by default, CREATE OR REPLACEreplaces data by default",
            "INSERT OVERWRITE maintains historical data versions by default, CREATE OR REPLACEclears the historical data",
            "INSERT OVERWRITE clears historical data versions by default, CREATE OR REPLACE maintains the historical data",
            "Both are same and results in identical outcomes"
        ],
        "answer": "INSERT OVERWRITE replaces data by default, CREATE OR REPLACE replaces data and Schema by default",
        "explanation": "\nThe answer is, INSERT OVERWRITE replaces data, CRAS replaces data and Schema\nThe main difference between INSERT OVERWRITE and CREATE OR REPLACE TABLE(CRAS) is that CRAS can modify the\nschema of the table, i.e it can add new columns or change data types of existing columns. By default INSERT OVERWRITE\nonly overwrites the data.\nINSERT OVERWRITE can also be used to overwrite schema, only when spark.databricks.delta.schema.autoMerge.enabled\nis set true if this option is not enabled and if there is a schema mismatch command will fail.\n"
    },
    {
        "question": "\nWhich of the following functions can be used to convert JSON string to Struct data type?\n",
        "choices": [
            "TO_STRUCT (json value)",
            "FROM_JSON (json value)",
            "FROM_JSON (json value, schema of json)",
            "CONVERT (json value, schema of json)",
            "CAST (json value as STRUCT)"
        ],
        "answer": "FROM_JSON (json value, schema of json)",
        "explanation": "\nSyntax\nCopy\nfrom_json(jsonStr, schema [, options])\nArguments\njsonStr: A STRING expression specifying a row of CSV data.\nschema: A STRING literal or invocation of schema_of_json function (Databricks SQL).\noptions: An optional MAP literal specifying directives.\nRefer documentation for more details,\nhttps://docs.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/from_json\n"
    },
    {
        "question": "\nYou are working on a marketing team request to identify customers with same information between two tables\nCUSTOMERS_ and CUSTOMERS_ each table contains  columns with same schema, You are looking to identify\nrows match between two tables across all columns, which of the following can be used to perform in SQL\nSELECT * FROM CUSTOMERS_ UNIONSELECT * FROM CUSTOMERS_\nA.\nSELECT * FROM CUSTOMERS_ UNION ALLSELECT * FROM CUSTOMERS_\nB.\nSELECT * FROM CUSTOMERS_ CINNER JOIN CUSTOMERS_ CON C.CUSTOMER_ID = C.CUSTOMER_ID\nC.\nSELECT * FROM CUSTOMERS_ INTERSECTSELECT * FROM CUSTOMERS_\nD.\nSELECT * FROM CUSTOMERS_ EXCEPTSELECT * FROM CUSTOMERS_\nE.\nAnswer: D\nExplanation/Reference:\nAnswer is\nSELECT * FROM CUSTOMERS_\nINTERSECT\nSELECT * FROM CUSTOMERS_\nINTERSECT [ALL | DISTINCT]\nReturns the set of rows which are in both subqueries.\nIf ALL is specified a row that appears multiple times in the subquery as well as in subquery will be returned multiple\ntimes.\nhttps://www.dumpsless.com/\nIf DISTINCT is specified the result does not contain duplicate rows. This is the default.\n",
        "choices": [
            "SELECT * FROM CUSTOMERS_2021 UNIONSELECT * FROM CUSTOMERS_2020",
            "SELECT * FROM CUSTOMERS_2021 UNION ALLSELECT * FROM CUSTOMERS_2020",
            "SELECT * FROM CUSTOMERS_2021 C1INNER JOIN CUSTOMERS_2020 C2ON C1.CUSTOMER_ID = C2.CUSTOMER_ID",
            "SELECT * FROM CUSTOMERS_2021 INTERSECTSELECT * FROM CUSTOMERS_2020",
            "SELECT * FROM CUSTOMERS_2021 EXCEPTSELECT * FROM CUSTOMERS_2020"
        ],
        "answer": "SELECT * FROM CUSTOMERS_2021 INTERSECTSELECT * FROM CUSTOMERS_2020",
        "explanation": "\nAnswer is\nSELECT * FROM CUSTOMERS_2021\nINTERSECT\nSELECT * FROM CUSTOMERS_202\nINTERSECT [ALL | DISTINCT]\nReturns the set of rows which are in both subqueries.\nIf ALL is specified a row that appears multiple times in the subquery1 as well as in subquery will be returned multiple\ntimes.\nhttps://www.dumps4less.com/\nIf DISTINCT is specified the result does not contain duplicate rows. This is the default.\n"
    },
    {
        "question": "\nYou are looking to process the data based on two variables, one to check if the department is supply chain and second to\ncheck if process flag is set to True\n",
        "choices": [
            "if department = \u201csupply chain\u201d & process:",
            "if department == \u201csupply chain\u201d && process:",
            "if department == \u201csupply chain\u201d & process == TRUE:",
            "if department == \u201csupply chain\u201d & if process == TRUE:",
            "if department == \"supply chain\" and process:"
        ],
        "answer": "if department == \"supply chain\" and process:",
        "explanation": "157\nYou are looking to process the data based on two variables, one to check if the department is supply chain and second to\ncheck if process flag is set to True\nif department = \u201csupply chain\u201d & process:\nA.\nif department == \u201csupply chain\u201d && process:\nB.\nif department == \u201csupply chain\u201d & process == TRUE:\nC.\nif department == \u201csupply chain\u201d & if process == TRUE:\nD.\nif department == \"supply chain\" and process:\nE.\nAnswer: E\n"
    },
    {
        "question": "\nYou were asked to create a notebook that can take department as a parameter and process the data accordingly, which\nis the following statements result in storing the notebook parameter into a python variable\n",
        "choices": [
            "SET department = dbutils.widget.get(\"department\")",
            "ASSIGN department == dbutils.widget.get(\"department\")",
            "department = dbutils.widget.get(\"department\")",
            "department = notebook.widget.get(\"department\")",
            "department = notebook.param.get(\"department\")"
        ],
        "answer": "department = dbutils.widget.get(\"department\")",
        "explanation": "\nThe answer is department = dbutils.widget.get(\"department\")\nRefer to additional documentation here\nhttps://docs.databricks.com/notebooks/widgets.html\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the following statements can successfully read the notebook widget and pass the python variable to a SQL\nstatement in a Python notebook cell?\n",
        "choices": [
            "order_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\"SELECT * FROM sales WHERE orderDate =",
            "order_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\"SELECT * FROM sales WHERE orderDate =",
            "order_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\u201dSELECT * FROM sales WHERE orderDate =",
            "order_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\"SELECT * FROM sales WHERE orderDate =",
            "order_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(\"SELECT * FROM sales WHERE orderDate ="
        ],
        "answer": "order_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\"SELECT * FROM sales WHERE orderDate =",
        "explanation": "159\nWhich of the following statements can successfully read the notebook widget and pass the python variable to a SQL\nstatement in a Python notebook cell?\norder_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\"SELECT * FROM sales WHERE orderDate =\nA.\n'f{order_date }'\")\norder_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\"SELECT * FROM sales WHERE orderDate =\nB.\n'order_date' \")\norder_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\u201dSELECT * FROM sales WHERE orderDate =\nC.\n'${order_date }' \")\norder_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(f\"SELECT * FROM sales WHERE orderDate =\nD.\n'{order_date}' \")\norder_date = dbutils.widgets.get(\"widget_order_date\")spark.sql(\"SELECT * FROM sales WHERE orderDate =\nE.\norder_date\")\nAnswer: D\n"
    },
    {
        "question": "\nFor a spark stream process to read a delta table to event logs and create a summary table with customerId and the\nnumber of times the customerId is present in the event log and write a one-time micro batch, fill in the blanks to\ncomplete the query.\nspark._________\n.format(\"delta\")\n.table(\"events_log\")\n.groupBy(\"customerId\")\n.count()\n._______\n.format(\"delta\")\n.outputMode(\"complete\")\n.option(\"checkpointLocation\", \"/tmp/delta/eventsByCustomer/_checkpoints/\")\n.trigger(______)\n.table(\"target_table\")\n",
        "choices": [
            "writeStream, readStream, once",
            "readStream, writeStream, once",
            "writeStream, processingTime = once",
            "writeStream, readStream, once = True",
            "readStream, writeStream, once = True"
        ],
        "answer": "readStream, writeStream, once = True",
        "explanation": "\nThe answer is readStream, writeStream, once = True.\nspark.readStream\n.format(\"delta\")\n.table(\"events_log\")\n.groupBy(\"customerId\")\n.count()\n.writeStream\n.format(\"delta\")\n.outputMode(\"complete\")\n.option(\"checkpointLocation\", \"/tmp/delta/eventsByCustomer/_checkpoints/\")\n.trigger(once = True)\n.table(\"target_table\")\n"
    },
    {
        "question": "\nYou would like to build a streaming process to read from a Kafka queue and write to a Delta table every  minutes, what\nis the correct trigger option\ntrigger(\" minutes\")\nA.\ntrigger(process \" minutes\")\nB.\ntrigger(processingTime = )\nC.\ntrigger(processingTime = \" Minutes\")\nD.\ntrigger()\nE.\nAnswer: D\nExplanation/Reference:\nThe answer is trigger(processingTime = \" Minutes\")\nTriggers:\nUnspecified\nThis is the default. This is equivalent to using processingTime=\"ms\"\nFixed interval micro-batches .trigger(processingTime=\" minutes\")\nThe query will be executed in micro-batches and kicked off at the user-specified intervals\nOne-time micro-batch .trigger(once=True)\nThe query will execute a single micro-batch to process all the available data and then stop on its own\nOne-time micro-batch.trigger .trigger(availableNow=True) -- New feature a better version of (once=True)\nDatabricks supports trigger(availableNow=True) in Databricks Runtime . and above for Delta Lake and Auto Loader\nsources. This functionality combines the batch processing approach of trigger once with the ability to configure batch\nsize, resulting in multiple parallelized batches that give greater control for right-sizing batches and the resultant files.\nhttps://www.dumpsless.com/\n",
        "choices": [
            "trigger(\"15 minutes\")",
            "trigger(process \"15 minutes\")",
            "trigger(processingTime = 15)",
            "trigger(processingTime = \"15 Minutes\")",
            "trigger(15)"
        ],
        "answer": "trigger(processingTime = \"15 Minutes\")",
        "explanation": "\nThe answer is trigger(processingTime = \"15 Minutes\")\nTriggers:\nUnspecified\nThis is the default. This is equivalent to using processingTime=\"500ms\"\nFixed interval micro-batches .trigger(processingTime=\"2 minutes\")\nThe query will be executed in micro-batches and kicked off at the user-specified intervals\nOne-time micro-batch .trigger(once=True)\nThe query will execute a single micro-batch to process all the available data and then stop on its own\nOne-time micro-batch.trigger .trigger(availableNow=True) -- New feature a better version of (once=True)\nDatabricks supports trigger(availableNow=True) in Databricks Runtime 10.2 and above for Delta Lake and Auto Loader\nsources. This functionality combines the batch processing approach of trigger once with the ability to configure batch\nsize, resulting in multiple parallelized batches that give greater control for right-sizing batches and the resultant files.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the following scenarios is the best fit for the AUTO LOADER solution?\n",
        "choices": [
            "Efficiently process new data incrementally from cloud object storage",
            "Incrementally process new streaming from data into delta lake",
            "Incrementally process new data from relational databases like MySQL",
            "Efficiently copy data from data lake location to another data lake location",
            "Efficiently move data incrementally from one delta table to another delta table"
        ],
        "answer": "Efficiently process new data incrementally from cloud object storage",
        "explanation": "\nRefer to more documentation here,\nhttps://docs.microsoft.com/en-us/azure/databricks/ingestion/auto-loader\n"
    },
    {
        "question": "\nYou had AUTO LOADER to process millions of files a day and noticed slowness in load process, so you scaled up the\nDatabricks cluster but realized the performance of the Auto loader is still not improving, what is the best way to resolve\nthis.\n",
        "choices": [
            "AUTO LOADER is not suitable to process millions of files a day",
            "Setup a second AUTO LOADER process to process the data",
            "Increase the maxFilesPerTrigger option to a sufficiently high number",
            "Copy the data from cloud storage to local disk on the cluster for faster access",
            "Merge files to one large file"
        ],
        "answer": "Increase the maxFilesPerTrigger option to a sufficiently high number",
        "explanation": "\nThe default value of maxFilesPerTrigger is 1000 it can be increased to a much higher number but will require a much\nlarger compute to process.\nhttps://www.dumps4less.com/\nhttps://docs.databricks.com/ingestion/auto-loader/options.html\n"
    },
    {
        "question": "\nThe current ELT pipeline is receiving data from the operations team once a day so you had setup an AUTO LOADER\nprocess to run once a day using trigger (Once = True) and scheduled a job to run once a day, operations team recently\nrolled out a new feature that allows them to send data every  min, what changes do you need to make to AUTO\nLOADER to process the data every  min.\n",
        "choices": [
            "Convert AUTO LOADER to structured streaming",
            "Change AUTO LOADER trigger to .trigger(ProcessingTime = \"1 minute\")",
            "Setup a job cluster run the notebook once a minute",
            "Enable stream processing",
            "Change AUTO LOADER trigger to (\"1 minute\")"
        ],
        "answer": "Change AUTO LOADER trigger to .trigger(ProcessingTime = \"1 minute\")",
        "explanation": "164\nThe current ELT pipeline is receiving data from the operations team once a day so you had setup an AUTO LOADER\nprocess to run once a day using trigger (Once = True) and scheduled a job to run once a day, operations team recently\nrolled out a new feature that allows them to send data every 1 min, what changes do you need to make to AUTO\nLOADER to process the data every 1 min.\nConvert AUTO LOADER to structured streaming\nA.\nChange AUTO LOADER trigger to .trigger(ProcessingTime = \"1 minute\")\nB.\nSetup a job cluster run the notebook once a minute\nC.\nEnable stream processing\nD.\nChange AUTO LOADER trigger to (\"1 minute\")\nE.\nAnswer: B\n"
    },
    {
        "question": "\nWhat is the purpose of bronze layer in a Multi hop architecture?\n",
        "choices": [
            "Copy of raw data, easy to query and ingest data for downstream processes",
            "Powers ML applications",
            "Data quality checks, corrupt data quarantined",
            "Contain aggregated data that is to be consumed into Silver",
            "Reduces data storage by compressing the data"
        ],
        "answer": "Copy of raw data, easy to query and ingest data for downstream processes",
        "explanation": "\nMedallion Architecture \u2013 Databricks\nBronze Layer:\n1. Raw copy of ingested data\n2. Replaces traditional data lake\n3. Provides efficient storage and querying of full, unprocessed history of data\n4. No schema is applied at this layer\n"
    },
    {
        "question": "\nWhat is the purpose of the silver layer in a Multi hop architecture?\n",
        "choices": [
            "Replaces a traditional data lake",
            "Efficient storage and querying of full, unprocessed history of data",
            "Eliminates duplicate data, quarantines bad data",
            "Refined views with aggregated data",
            "Optimized query performance for business-critical data"
        ],
        "answer": "Eliminates duplicate data, quarantines bad data",
        "explanation": "\nMedallion Architecture \u2013 Databricks\nSilver Layer:\n1. Reduces data storage complexity, latency, and redundency\n2. Optimizes ETL throughput and analytic query performance\n3. Preserves grain of original data (without aggregation)\n4. Eliminates duplicate records\n5. production schema enforced\n6. Data quality checks, quarantine corrupt data\n"
    },
    {
        "question": "\nWhat is the purpose of gold layer in Multi hop architecture?\n",
        "choices": [
            "Optimizes ETL throughput and analytic query performance",
            "Eliminate duplicate records",
            "Preserves grain of original data, without any aggregations",
            "Data quality checks and schema enforcement",
            "Optimized query performance for business-critical data"
        ],
        "answer": "Optimized query performance for business-critical data",
        "explanation": "\nMedallion Architecture \u2013 Databricks\nGold Layer:\n1. Powers Ml applications, reporting, dashboards, ad hoc analytics\n2. Refined views of data, typically with aggregations\n3. Reduces strain on production systems\n4. Optimizes query performance for business-critical data\n"
    },
    {
        "question": "\nThe Delta Live Tables Pipeline is configured to run in Development mode using the Triggered Pipeline Mode. what is the\nexpected outcome after clicking Start to update the pipeline?\n",
        "choices": [
            "All datasets will be updated once and the pipeline will shut down. The compute resources will be terminated",
            "All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed",
            "All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist after",
            "All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for",
            "All datasets will be updated continuously and the pipeline will not shut down. The compute resources will persist with"
        ],
        "answer": "All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for",
        "explanation": "\nThe answer is All datasets will be updated once and the pipeline will shut down. The compute resources will persist to\nallow for additional testing.\nDLT pipeline supports two modes Development and Production, you can switch between the two based on the stage of\nyour development and deployment lifecycle.\nDevelopment and production modes\nWhen you run your pipeline in development mode, the Delta Live Tables system:\nReuses a cluster to avoid the overhead of restarts.\nDisables pipeline retries so you can immediately detect and fix errors.\nIn production mode, the Delta Live Tables system:\nRestarts the cluster for specific recoverable errors, including memory leaks and stale credentials.\nRetries execution in the event of specific errors, for example, a failure to start a cluster.\nUse the\nbuttons in the Pipelines UI to switch between development and production modes. By default, pipelines run in\ndevelopment mode.\nhttps://www.dumps4less.com/\nSwitching between development and production modes only controls cluster and pipeline execution behavior. Storage\nlocations must be configured as part of pipeline settings and are not affected when switching between modes.\nPlease review additional DLT concepts using below link\nhttps://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#delta-live-tables-concepts\n"
    },
    {
        "question": "\nThe DELT LIVE TABLE Pipeline is configured to run in Production mode using the continuous Pipeline Mode. what is the\nexpected outcome after clicking Start to update the pipeline?\n",
        "choices": [
            "All datasets will be updated once and the pipeline will shut down. The compute resources will be terminated",
            "All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed",
            "All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist after",
            "All datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for",
            "All datasets will be updated continuously and the pipeline will not shut down. The compute resources will persist with"
        ],
        "answer": "All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed",
        "explanation": "\nThe answer is, All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be\ndeployed for the update and terminated when the pipeline is stopped.\nDLT pipeline supports two modes Development and Production, you can switch between the two based on the stage of\nyour development and deployment lifecycle.\nDevelopment and production modes\nDevelopment:\nWhen you run your pipeline in development mode, the Delta Live Tables system:\nReuses a cluster to avoid the overhead of restarts.\nDisables pipeline retries so you can immediately detect and fix errors.\nProduction:\nIn production mode, the Delta Live Tables system:\nRestarts the cluster for specific recoverable errors, including memory leaks and stale credentials.\nRetries execution in the event of specific errors, for example, a failure to start a cluster.\nUse the\nbuttons in the Pipelines UI to switch between development and production modes. By default, pipelines run in\ndevelopment mode.\nSwitching between development and production modes only controls cluster and pipeline execution behavior. Storage\nlocations must be configured as part of pipeline settings and are not affected when switching between modes.\nPlease review additional DLT concepts using the below lin\nhttps://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#delta-live-tables-concepts\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nyou are working to set up two notebooks to run on a schedule second notebook is dependent on first notebook, both\nnotebooks need different types of compute to run in an optimal fashion? What is the best way to setup these notebooks\nas jobs?\n",
        "choices": [
            "Use DELTA LIVE PIPELINES instead of notebook tasks",
            "A Job can only use single cluster, setup job for each notebook and use job dependency to link both jobs together",
            "Each task can use different cluster, add these two notebooks as two tasks in a single job with linear dependency and",
            "Use a single job to setup both notebooks as individual tasks, but use the cluster API to setup the second cluster before",
            "Use a very large cluster to run both the tasks in a single job"
        ],
        "answer": "Each task can use different cluster, add these two notebooks as two tasks in a single job with linear dependency and",
        "explanation": "\nTasks in Jobs support different clusters for each task in the same job.\n"
    },
    {
        "question": "\nYou are tasked to setup a set notebook as a job for six departments and each department can run the task parallelly, the\nnotebook takes an input parameter dept number to process the data by department how do you go about to setup this in\njob?\n",
        "choices": [
            "Use a single notebook as task in the job and use dbutils.notebook.run to run each notebook with parameter in a",
            "A task in the job cannot take an input parameter, create six notebooks with hardcoded dept number and setup six",
            "A task accepts key-value pair parameters, creates six tasks pass department number as parameter foreach task with",
            "A parameter can only be passed at the job level, create six jobs pass department number to each job with linear job",
            "A parameter can only be passed at the job level, create six jobs pass department number to each job with no job"
        ],
        "answer": "A task accepts key-value pair parameters, creates six tasks pass department number as parameter foreach task with",
        "explanation": "\nHere is how you setup\nCreate a single job and six tasks with the same notebook and assign a different parameter for each task ,\nAll tasks are added in a single job and can run parallel either using single shared cluster or with individual clusters\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are asked to setup two tasks, first task runs a notebook to download the from a remote database, second task is a\nDLT pipeline that can process this data, how do you plan to configure this in Jobs UI\n",
        "choices": [
            "Single job cannot have a notebook task and DLT Pipeline task, use two different jobs with linear dependency.",
            "Jobs UI does not support DTL pipeline, setup the first task using jobs UI and setup the DLT to run in continuous mode.",
            "Jobs UI does not support DTL pipeline, setup the first task using jobs UI and setup the DLT to run in trigger mode.",
            "Single job can be used to setup both notebook and DLT pipeline, use two different tasks with linear dependency.",
            "Add first step in the DLT pipeline and run the DLT pipeline as triggered mode in JOBS UI"
        ],
        "answer": "Single job can be used to setup both notebook and DLT pipeline, use two different tasks with linear dependency.",
        "explanation": "\nThe answer is Single job can be used to set up both notebook and DLT pipeline, use two different tasks with linear\ndependency,\nHere is the JOB UI\nCreate a notebook task\nCreate DLT task\nadd notebook task as dependency\nFinal view\nCreate the notebook task\nDLT task\nhttps://www.dumps4less.com/\nFinal view\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are asked to setup the alert to notify every time once the data is loaded into a reporting table, team also asked to\ninclude the number of records in the alert email notification.\n",
        "choices": [
            "Use notebook and python code to run every minute, using python variables to capture send the information in an",
            "Setup an alert but use the default template to notify the message in email\u2019s subject",
            "Setup an alert but use the custom template to notify the message in email\u2019s subject",
            "Use the webhook destination instead so alert message can be customized",
            "Use custom email hook to customize the message"
        ],
        "answer": "Setup an alert but use the custom template to notify the message in email\u2019s subject",
        "explanation": "\nAlerts support custom template supports using variables to customize the default message. setup the query to count the\nnumber rows based on your criteria and use the variable QUERY_RESULT_VALUE\nAlerts | Databricks on AWS\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nOperations team is using a centralized data quality monitoring system, a user can publish data quality metrics through a\nwebhook, you were asked to develop a process to send messages using webhook if there is atleast one duplicate record,\nwhich of following approaches can be taken to integrate with current data quality monitoring system\n",
        "choices": [
            "Use notebook and Jobs to use python to publish DQ metrics",
            "Setup an alert to send an email, use python to parse email, and publish a webhook message",
            "Setup an alert with custom template",
            "Setup an alert with custom Webhook destination",
            "Setup an alert with dynamic template"
        ],
        "answer": "Setup an alert with custom Webhook destination",
        "explanation": "\nAlerts supports multiple destinations, email is the default destinations.\nAlert destinations | Databricks on AWS\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are currently working with application team to setup a SQL Endpoint point, once the team started consuming the SQL\nEndpoint you noticed that during peak hours as the number of users increases you are seeing degradation in the query\nperformance, which of the following steps can be taken to resolve the issue?\n",
        "choices": [
            "They can turn on the Serverless feature for the SQL endpoint.",
            "They can increase the maximum bound of the SQL endpoint\u2019s scaling range.",
            "They can increase the cluster size of the SQL endpoint.",
            "They can turn on the Auto Stop feature for the SQL endpoint.",
            "They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy from \u201cCost"
        ],
        "answer": "They can increase the maximum bound of the SQL endpoint\u2019s scaling range.",
        "explanation": "\nThe answer is, They can increase the maximum bound of the SQL endpoint\u2019s scaling range.\nSQL endpoint scales horizontally(scale-out) and vertical (scale-up), you have to understand when to use what.\nScale-out -> to add more clusters for a SQL endpoint, change max number of clusters scaling range.\nIf you are trying to improve the throughput, being able to run as many queries as possible then having an additional\ncluster(s) will improve the performance.\nScale-up-> Increase the size of the SQL endpoint, change cluster size from x-small to small, to medium, X Large....\nIf you are trying to improve the performance of a single query having additional memory, additional nodes and cpu in the\ncluster will improve the performance.\n"
    },
    {
        "question": "\nData engineering team is using a SQL query to review data quality and monitor the ETL job everyday, which of the\nfollowing approaches can be used to setup a schedule and automate this process?\nThey can schedule the query to run every  day from the Jobs UI\nA.\nThey can schedule the query to refresh every  day from the query\u2019s page in Databricks SQL.\nB.\nThey can schedule the query to run every  hours from the Jobs UI.\nC.\nThey can schedule the query to refresh every  day from the SQL endpoint\u2019s page in Databricks SQL.\nD.\nThey can schedule the query to refresh every  hours from the SQL endpoint\u2019s page in Databricks SQL\nE.\nAnswer: B\nExplanation/Reference:\nIndividual queries can be refreshed on a schedule basis,\nTo set the schedule:\nClick the query info tab.\nClick the link to the right of Refresh Schedule to open a picker with schedule intervals.\nSet the schedule.\nThe picker scrolls and allows you to choose:\nAn interval: - minutes, - hours,  or  days,  or  weeks\nA time. The time selector displays in the picker only when the interval is greater than  day and the day selection is\ngreater than  week. When you schedule a specific time, Databricks SQL takes input in your computer\u2019s timezone and\nconverts it to UTC. If you want a query to run at a certain time in UTC, you must adjust the picker by your local offset. For\nexample, if you want a query to execute at : UTC each day, but your current timezone is PDT (UTC-), you should\nselect : in the picker:\nClick OK.\nYour query will run automatically.\nhttps://www.dumpsless.com/\nIf you experience a scheduled query not executing according to its schedule, you should manually trigger the query to\nmake sure it doesn\u2019t fail. However, you should be aware of the following:\nIf you schedule an interval\u2014for example, \u201cevery  minutes\u201d\u2014the interval is calculated from the last successful\nexecution. If you manually execute a query, the scheduled query will not be executed until the interval has passed.\nIf you schedule a time, Databricks SQL waits for the results to be \u201coutdated\u201d. For example, if you have a query set to\nrefresh every Thursday and you manually execute it on Wednesday, by Thursday the results will still be considered\n\u201cvalid\u201d, so the query wouldn\u2019t be scheduled for a new execution. Thus, for example, when setting a weekly schedule,\ncheck the last query execution time and expect the scheduled query to be executed on the selected day after that\nexecution is a week old. Make sure not to manually execute the query during this time.\nIf a query execution fails, Databricks SQL retries with a back-off algorithm. The more failures the further away the next\nretry will be (and it might be beyond the refresh interval).\nRefer documentation for additional info,\nhttps://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/schedule-query\n",
        "choices": [
            "They can schedule the query to run every 1 day from the Jobs UI",
            "They can schedule the query to refresh every 1 day from the query\u2019s page in Databricks SQL.",
            "They can schedule the query to run every 12 hours from the Jobs UI.",
            "They can schedule the query to refresh every 1 day from the SQL endpoint\u2019s page in Databricks SQL.",
            "They can schedule the query to refresh every 12 hours from the SQL endpoint\u2019s page in Databricks SQL"
        ],
        "answer": "They can schedule the query to refresh every 1 day from the query\u2019s page in Databricks SQL.",
        "explanation": "\nIndividual queries can be refreshed on a schedule basis,\nTo set the schedule:\nClick the query info tab.\nClick the link to the right of Refresh Schedule to open a picker with schedule intervals.\nSet the schedule.\nThe picker scrolls and allows you to choose:\nAn interval: 1-30 minutes, 1-12 hours, 1 or 30 days, 1 or 2 weeks\nA time. The time selector displays in the picker only when the interval is greater than 1 day and the day selection is\ngreater than 1 week. When you schedule a specific time, Databricks SQL takes input in your computer\u2019s timezone and\nconverts it to UTC. If you want a query to run at a certain time in UTC, you must adjust the picker by your local offset. For\nexample, if you want a query to execute at 00:00 UTC each day, but your current timezone is PDT (UTC-7), you should\nselect 17:00 in the picker:\nClick OK.\nYour query will run automatically.\nhttps://www.dumps4less.com/\nIf you experience a scheduled query not executing according to its schedule, you should manually trigger the query to\nmake sure it doesn\u2019t fail. However, you should be aware of the following:\nIf you schedule an interval\u2014for example, \u201cevery 15 minutes\u201d\u2014the interval is calculated from the last successful\nexecution. If you manually execute a query, the scheduled query will not be executed until the interval has passed.\nIf you schedule a time, Databricks SQL waits for the results to be \u201coutdated\u201d. For example, if you have a query set to\nrefresh every Thursday and you manually execute it on Wednesday, by Thursday the results will still be considered\n\u201cvalid\u201d, so the query wouldn\u2019t be scheduled for a new execution. Thus, for example, when setting a weekly schedule,\ncheck the last query execution time and expect the scheduled query to be executed on the selected day after that\nexecution is a week old. Make sure not to manually execute the query during this time.\nIf a query execution fails, Databricks SQL retries with a back-off algorithm. The more failures the further away the next\nretry will be (and it might be beyond the refresh interval).\nRefer documentation for additional info,\nhttps://docs.microsoft.com/en-us/azure/databricks/sql/user/queries/schedule-query\n"
    },
    {
        "question": "\nIn order to use Unity catalog features, which of the following steps needs to be taken on managed/external tables in the\nDatabricks workspace?\n",
        "choices": [
            "Enable unity catalog feature in workspace settings",
            "Migrate/upgrade objects in workspace managed/external tables/view to unity catalog",
            "Upgrade to DBR version 15.0",
            "Copy data from workspace to unity catalog",
            "Upgrade workspace to Unity catalog"
        ],
        "answer": "Migrate/upgrade objects in workspace managed/external tables/view to unity catalog",
        "explanation": "\nUpgrade tables and views to Unity Catalog - Azure Databricks | Microsoft Docs\nManaged table: Upgrade a managed to Unity Catalog\nExternal table: Upgrade an external table to Unity Catalog\n"
    },
    {
        "question": "\nWhat is the top-level object in unity catalog?\n",
        "choices": [
            "Catalog",
            "Table",
            "Workspace",
            "Database",
            "Metastore"
        ],
        "answer": "Metastore",
        "explanation": "\nKey concepts - Azure Databricks | Microsoft Docs\n"
    },
    {
        "question": "\nOne of the team members Steve who has the ability to create views, created a new view called regional_sales_vw on the\nexisting table called sales which is owned by John, and the second team member Kevin who works with regional sales\nmanagers wanted to query the data in regional_sales_vw, so Steve granted the permission to Kevin using command\nGRANT VIEW, USAGE ON regional_sales_vw to kevin@company.com but Kevin is still unable to access the view?\n",
        "choices": [
            "Kevin needs select access on the table sales",
            "Kevin needs owner access on the view regional_sales_vw",
            "Steve is not the owner of the sales table",
            "Kevin is not the owner of the sales table",
            "Table access control is not enabled on the table and view"
        ],
        "answer": "Steve is not the owner of the sales table",
        "explanation": "\nOwnership determines whether or not you can grant privileges on derived objects to other users, since Steve is not the\nowner of the underlying sales table, he can not grant access to the table or data in the table indirectly.\nOnly owner(user or group) can grant access to a object\nhttps://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#a-user-has-select-\nprivileges-on-a-view-of-table-t-but-when-that-user-tries-to-select-from-that-view-they-get-the-error-user-does-not-\nhave-privilege-select-on-table\nhttps://www.dumps4less.com/\nData object privileges - Azure Databricks | Microsoft Doc\n"
    },
    {
        "question": "\nKevin is the owner of the schema sales, Steve wanted to create new table in sales schema called regional_sales so Kevin\ngrants the create table permissions to Steve. Steve creates the new table called regional_sales in sales schema, who is\nthe owner of the table regional_sales\n",
        "choices": [
            "Kevin is the owner of sales schema, all the tables in the schema will be owned by Kevin",
            "Steve is the owner of the table",
            "By default ownership is assigned DBO",
            "By default ownership is assigned to DEFAULT_OWNER",
            "Kevin and Smith both are owners of table"
        ],
        "answer": "Steve is the owner of the table",
        "explanation": "\nA user who creates the object becomes its owner, does not matter who is the owner of the parent object.\n"
    },
    {
        "question": "\nYou were asked to write python code to stop all running streams, which of the following command can be used to get a\nlist of all active streams currently running so we can stop them, fill in the blank.\nfor s in _______________:\ns.stop()\n",
        "choices": [
            "Spark.getActiveStreams()",
            "spark.streams.active",
            "activeStreams()",
            "getActiveStreams()",
            "spark.streams.getActive"
        ],
        "answer": "spark.streams.active",
        "explanation": "181\nYou were asked to write python code to stop all running streams, which of the following command can be used to get a\nlist of all active streams currently running so we can stop them, fill in the blank.\nfor s in _______________:\ns.stop()\nSpark.getActiveStreams()\nA.\nspark.streams.active\nB.\nactiveStreams()\nC.\ngetActiveStreams()\nD.\nspark.streams.getActive\nE.\nAnswer: B\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nAt the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to\ningest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to\nchange overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to\ncommand to load the data, fill in the blanks for successful execution of below code.\nspark.readStream\n.format(\"cloudfiles\")\n.option(\"_______\",\u201dcsv)\n.option(\"_______\", \u2018dbfs:/location/checkpoint/\u2019)\n.load(data_source)\n.writeStream\n.option(\"_______\",\u2019 dbfs:/location/checkpoint/\u2019)\n.option(\"_______\", \"true\")\n.table(table_name))\n",
        "choices": [
            "format, checkpointlocation, schemalocation, overwrite",
            "cloudfiles.format, checkpointlocation, cloudfiles.schemalocation, overwrite",
            "cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema",
            "cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, append",
            "cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, overwrite"
        ],
        "answer": "cloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema",
        "explanation": "182\nAt the end of the inventory process a file gets uploaded to the cloud object storage, you are asked to build a process to\ningest data which of the following method can be used to ingest the data incrementally, schema of the file is expected to\nchange overtime ingestion process should be able to handle these changes automatically. Below is the auto loader to\ncommand to load the data, fill in the blanks for successful execution of below code.\nspark.readStream\n.format(\"cloudfiles\")\n.option(\"_______\",\u201dcsv)\n.option(\"_______\", \u2018dbfs:/location/checkpoint/\u2019)\n.load(data_source)\n.writeStream\n.option(\"_______\",\u2019 dbfs:/location/checkpoint/\u2019)\n.option(\"_______\", \"true\")\n.table(table_name))\nformat, checkpointlocation, schemalocation, overwrite\nA.\ncloudfiles.format, checkpointlocation, cloudfiles.schemalocation, overwrite\nB.\ncloudfiles.format, cloudfiles.schemalocation, checkpointlocation, mergeSchema\nC.\ncloudfiles.format, cloudfiles.schemalocation, checkpointlocation, append\nD.\ncloudfiles.format, cloudfiles.schemalocation, checkpointlocation, overwrite\nE.\nAnswer: C\n"
    },
    {
        "question": "\nWhich of the following scenarios is the best fit for AUTO LOADER?\n",
        "choices": [
            "Efficiently process new data incrementally from cloud object storage",
            "Efficiently move data incrementally from one delta table to another delta table",
            "Incrementally process new data from streaming data sources like Kafka into delta lake",
            "Incrementally process new data from relational databases like MySQL",
            "Efficiently copy data from one data lake location to another data lake location"
        ],
        "answer": "Efficiently process new data incrementally from cloud object storage",
        "explanation": "\nThe answer is, Efficiently process new data incrementally from cloud object storage, AUTO LOADER only supports\nhttps://www.dumps4less.com/\ningesting files stored in a cloud object storage. Auto Loader cannot process streaming data sources like Kafka or Delta\nstreams, use Structured streaming for these data sources.\nAuto Loader and Cloud Storage Integration\nAuto Loader supports a couple of ways to ingest data incrementally\nDirectory listing - List Directory and maintain the state in RocksDB, supports incremental file listing\nFile notification - Uses a trigger+queue to store the file notification which can be later used to retrieve the file, unlike\nDirectory listing File notification can scale up to millions of files per day.\n[OPTIONAL]\nAuto Loader vs COPY INTO?\nAuto Loader\nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional\nsetup. Auto Loader provides a new Structured Streaming source called cloudFiles. Given an input directory path on the\ncloud file storage, the cloudFiles source automatically processes new files as they arrive, with the option of also\nprocessing existing files in that directory.\nWhen to use Auto Loader instead of the COPY INTO?\nYou want to load data from a file location that contains files in the order of millions or higher. Auto Loader can discover\nfiles more efficiently than the COPY INTO SQL command and can split file processing into multiple batches.\nYou do not plan to load subsets of previously uploaded files. With Auto Loader, it can be more difficult to reprocess\nsubsets of files. However, you can use the COPY INTO SQL command to reload subsets of files while an Auto Loader\nstream is simultaneously running.\n"
    },
    {
        "question": "\nYou are asked to setup an AUTO LOADER to process the incoming data, this data arrives in JSON format and get dropped\ninto cloud object storage and you are required to process the data as soon as it arrives in cloud storage, which of the\nfollowing statements is correct\n",
        "choices": [
            "AUTO LOADER is native to DELTA lake it cannot support external cloud object storage",
            "AUTO LOADER has to be triggered from an external process when the file arrives in the cloud storage",
            "AUTO LOADER needs to be converted to a Structured stream process",
            "AUTO LOADER can only process continuous data when stored in DELTA lake",
            "AUTO LOADER can support file notification method so it can process data as it arrives"
        ],
        "answer": "AUTO LOADER can support file notification method so it can process data as it arrives",
        "explanation": "\nAuto Loader supports two modes when ingesting new files from cloud object storage\nDirectory listing: Auto Loader identifies new files by listing the input directory, and uses a directory polling approach.\nFile notification: Auto Loader can automatically set up a notification service and queue service that subscribe to file\nevents from the input directory.\nFile notification is more efficient and can be used to process the data in real-time as data arrives in cloud object storage.\nChoosing between file notification and directory listing modes | Databricks on AWS\n"
    },
    {
        "question": "\nWhat is the main difference between bronze and silver?\n",
        "choices": [
            "Duplicates are removed in bronze, schema is applied in silver",
            "Silver may contain aggregated data",
            "Bronze is raw copy of ingested data, silver contains data with production schema and optimized for ELT/ETL",
            "Bad data is filtered in Bronze, silver is a copy of bronze data"
        ],
        "answer": "Bronze is raw copy of ingested data, silver contains data with production schema and optimized for ELT/ETL",
        "explanation": "\nMedallion Architecture \u2013 Databricks\n"
    },
    {
        "question": "\nWhat is the main difference between silver and gold?\n",
        "choices": [
            "Silver may contain aggregated data",
            "Gold may contain aggregated data",
            "Data quality checks are applied in gold",
            "Silver is a copy of bronze data",
            "God is a copy of silver data"
        ],
        "answer": "Gold may contain aggregated data",
        "explanation": "\nMedallion Architecture \u2013 Databricks.\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhat is the main difference between silver and gold?\n",
        "choices": [
            "Silver optimized to perform ETL, Gold is optimized query performance",
            "Gold is optimized go perform ETL, Silver is optimized for query performance",
            "Silver is copy of Bronze, Gold is a copy of Silver",
            "Silver is stored in Delta Lake, Gold is stored in memory",
            "Silver may contain aggregated data, gold may preserve the granularity of original data"
        ],
        "answer": "Silver optimized to perform ETL, Gold is optimized query performance",
        "explanation": "\nMedallion Architecture \u2013 Databricks\nGold Layer:\nhttps://www.dumps4less.com/\n1. Powers Ml applications, reporting, dashboards, ad hoc analytics\n2. Refined views of data, typically with aggregations\n3. Reduces strain on production systems\n4. Optimizes query performance for business-critical data\n"
    },
    {
        "question": "\nA dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp\nEXPECT (timestamp > '--')\nWhat is the expected behavior when a batch of data containing data that violates these constraints is processed?\n",
        "choices": [
            "Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.",
            "Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.",
            "Records that violate the expectation cause the job to fail.",
            "Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the",
            "Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table."
        ],
        "answer": "Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.",
        "explanation": "\nThe answer is, Records that violate the expectation are added to the target dataset and recorded as invalid in the event\nlog.\nDelta live tables support three types of expectations to fix bad data in DLT pipelines\nhttps://www.dumps4less.com/\nReview below example code to examine these expectations,\n"
    },
    {
        "question": "\nA dataset has been defined using Delta Live Tables and includes an expectations clause: CONSTRAINT valid_timestamp\nEXPECT (timestamp > '--') ON VIOLATION DROP ROW\nWhat is the expected behavior when a batch of data containing data that violates these constraints is processed?\n",
        "choices": [
            "Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.",
            "Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.",
            "Records that violate the expectation cause the job to fail.",
            "Records that violate the expectation are added to the target dataset and flagged as invalid in a field added to the",
            "Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table."
        ],
        "answer": "Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.",
        "explanation": "\nThe answer is Records that violate the expectation are dropped from the target dataset and recorded as invalid in the\nevent log.\nDelta live tables support three types of expectations to fix bad data in DLT pipelines\nReview below example code to examine these expectations,\n"
    },
    {
        "question": "\nYou are asked to debug a databricks job that is taking too long to run on Sunday\u2019s, what are the steps you are going to\ntake to see the step that is taking longer to run?\n",
        "choices": [
            "A notebook activity of job run is only visible when using all-purpose cluster.",
            "Under Workflow UI and jobs select job you want to monitor and select the run, notebook activity can be viewed.",
            "Enable debug mode in the Jobs to see the output activity of a job, output should be available to view.",
            "Once a job is launched, you cannot access the job\u2019s notebook activity.",
            "Use the compute\u2019s spark UI to monitor the job activity."
        ],
        "answer": "Under Workflow UI and jobs select job you want to monitor and select the run, notebook activity can be viewed.",
        "explanation": "\nThe answer is, Under Workflow UI and jobs select job you want to monitor and select the run, notebook activity can be\nviewed.\nYou have the ability to view current active runs or completed runs, once you click the run you can see the\nClick on the run to view the notebook output\n"
    },
    {
        "question": "\nYour colleague was walking you through how a job was setup, but you noticed a warning message that said, \u201cJobs\nrunning on all-purpose cluster are considered all purpose compute\", the colleague was not sure why he was getting the\nwarning message, how do you best explain this warning message?\n",
        "choices": [
            "All-purpose clusters cannot be used for Job clusters, due to performance issues.",
            "All-purpose clusters take longer to start the cluster vs a job cluster",
            "All-purpose clusters are less expensive than the job clusters",
            "All-purpose clusters are more expensive than the job clusters",
            "All-purpose cluster provide interactive messages that can not be viewed in a job"
        ],
        "answer": "All-purpose clusters are more expensive than the job clusters",
        "explanation": "\nWarning message:\nPricing for All-purpose clusters are more expensive than the job clusters\nAWS pricing(Aug 15th 2022)\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYour team has hundreds of jobs running but it is difficult to track cost of each job run, you are asked to provide a\nrecommendation on how to monitor and track cost across various workloads\n",
        "choices": [
            "Create jobs in different workspaces, so we can track the cost easily",
            "Use Tags, during job creation so cost can be easily tracked",
            "Use job logs to monitor and track the costs",
            "Use workspace admin reporting",
            "Use a single cluster for all the jobs, so cost can be easily tracked"
        ],
        "answer": "Use Tags, during job creation so cost can be easily tracked",
        "explanation": "\nThe answer is Use Tags, during job creation so cost can be easily tracked\nReview below link for more details https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-\naws.html\nHere is a view how tags get propagated from pools to clusters and clusters without pools,\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nThe sales team has asked the Data engineering team to develop a dashboard that shows sales performance for all stores,\nbut the sales team would like to use the dashboard but would like to select individual store location, which of the\nfollowing approaches Data Engineering team can use to build this functionality into the dashboard.\n",
        "choices": [
            "Use query Parameters which then allow user to choose any location",
            "Currently dashboards do not support parameters",
            "Use Databricks REST API to create a dashboard for each location",
            "Use SQL UDF function to filter the data based on the location",
            "Use Dynamic views to filter the data based on the location"
        ],
        "answer": "Use query Parameters which then allow user to choose any location",
        "explanation": "\nThe answer is\nDatabricks many types of parameters in the dashboard, a drop-down list can be created based on a query that has a\nunique list of store locations.\nHere is a simple query that takes a parameter for\nSELECT * FROM sales WHERE field IN ( {{ Multi Select Parameter }} )\nOr\nSELECT * FROM sales WHERE field = {{ Single Select Parameter }}\nQuery parameter types\nText\nNumber\nDropdown List\nQuery Based Dropdown List\nhttps://www.dumps4less.com/\nDate and Time\n"
    },
    {
        "question": "\nYou are working on dashboard that takes really lot of time to load in the browser, due to each visualization contains lot of\ndata to populate, which of the following approaches can be taken to address this issue?\n",
        "choices": [
            "Increase size of the SQL endpoint cluster",
            "Increase the scale of maximum range of SQL endpoint cluster",
            "Use Databricks SQL Query filter to limit the amount of data in each visualization",
            "Remove data from Delta Lake",
            "Use Delta cache to store the intermediate results"
        ],
        "answer": "Use Databricks SQL Query filter to limit the amount of data in each visualization",
        "explanation": "\nNote*: The question may sound misleading but these are types of questions the exam tries to ask.\nA query filter lets you interactively reduce the amount of data shown in a visualization, similar to query parameter but\nwith a few key differences. A query filter limits data after it has been loaded into your browser. This makes filters ideal for\nsmaller datasets and environments where query executions are time-consuming, rate-limited, or costly.\nThis query filter is different from than filter that needs to be applied at the data level, this filter is at the visualization\nlevel so you can toggle how much data you want to see.\nSELECT action AS `action::filter`, COUNT(0) AS \"actions count\"\nFROM events\nGROUP BY action\nWhen queries have filters you can also apply filters at the dashboard level. Select the Use Dashboard Level Filters\ncheckbox to apply the filter to all queries.\nDashboard filters\nQuery filters | Databricks on AWS\n"
    },
    {
        "question": "\nOne of the queries in the Dashboard takes a long time to refresh, which of the below steps can be taken to identify the\nroot cause of this issue?\n",
        "choices": [
            "Restart the SQL endpoint",
            "Select the SQL endpoint cluster, spark UI, SQL tab to see the execution plan and time spent in each step",
            "Run optimize and Z ordering",
            "Change the Spot Instance Policy from \u201cCost optimized\u201d to \u201cReliability Optimized.\u201d",
            "Use Query History, to view queries and select query, and check query profile to time spent in each step"
        ],
        "answer": "Use Query History, to view queries and select query, and check query profile to time spent in each step",
        "explanation": "\nThe answer is, Use Query History, to view queries and select query, and check the query profile to see time spent in each\nstep.\nHere is the view of the query profile, for more info use the link\nhttps://docs.microsoft.com/en-us/azure/databricks/sql/admin/query-profile\nAs you can see here Databricks SQL query profile is much different to Spark UI and provides much more clear information\non how time is being spent on different queries.\n"
    },
    {
        "question": "\nA SQL Dashboard was built for the supply chain team to monitor the inventory and product orders, but all of the\ntimestamps displayed on the dashboards are showing in UTC format, so they requested to change the time zone to the\nlocation of New York. How would you approach resolving this issue?\n",
        "choices": [
            "Move the workspace from Central US zone to East US Zone",
            "Change the timestamp on the delta tables to America/New_York format",
            "Change the spark configuration of SQL endpoint to format the timestamp to America/New_York",
            "Under SQL Admin Console, set the SQL configuration parameter time zone to America/New_York",
            "Add SET Timezone = America/New_York on every of the SQL queries in the dashboard."
        ],
        "answer": "Under SQL Admin Console, set the SQL configuration parameter time zone to America/New_York",
        "explanation": "\nThe answer is, Under SQL Admin Console, set the SQL configuration parameter time zone to America/New_York\nHere are steps you can take this to configure, so the entire dashboard is changed without changing individual queries\nConfigure SQL parameters\nTo configure all warehouses with SQL parameters:\nhttps://www.dumps4less.com/\nClick Settings at the bottom of the sidebar and select SQL Admin Console.\nClick the SQL Warehouse Settings tab.\nIn the SQL Configuration Parameters textbox, specify one key-value pair per line. Separate the name of the parameter\nfrom its value using a space. For example, to enable ANSI_MODE:\nSimilarly, we can add a line in the SQL Configuration parameters\ntimezone America/New_York\nSQL configuration parameters | Databricks on AWS\n"
    },
    {
        "question": "\nWhich of the following technique can be used to fine-grained access control to rows and columns based on access?\n",
        "choices": [
            "Use Unity catalog to grant access to rows and columns",
            "Row and column access control lists",
            "Use dynamic view functions",
            "Data access control lists",
            "Dynamic Access control lists with Unity Catalog"
        ],
        "answer": "Use dynamic view functions",
        "explanation": "\nThe answer is, Use dynamic view functions.\nHere is an example that limits access to rows based on the user being part managers group, in the below view if a user is\nnot a part of the manager's group you can only see rows where the total amount is <= 1000000\nDynamic view function to filter rows\nCREATE VIEW sales_redacted AS\nSELECT user_id, country, product, total\nFROM sales_raw\nWHERE CASE WHEN is_member('managers') THEN TRUE ELSE total <= 1000000 END;\nDynamic view function to hide a column\nCREATE VIEW sales_redacted AS\nhttps://www.dumps4less.com/\nSELECT user_id,\nCASE WHEN is_member('auditors') THEN email ELSE 'REDACTED' END AS email,\ncountry,\nproduct,\ntotal\nFROM sales_raw\nPlease review below for more details\nhttps://docs.microsoft.com/en-us/azure/databricks/security/access-control/table-acls/object-privileges#dynamic-view-\nfunctions\n"
    },
    {
        "question": "\nUnity catalog helps you manage the below resources in Databricks at account level\n",
        "choices": [
            "Tables",
            "ML Models",
            "Dashboards",
            "Catalogs",
            "All of the above"
        ],
        "answer": "All of the above",
        "explanation": "198\nUnity catalog helps you manage the below resources in Databricks at account level\nTables\nA.\nML Models\nB.\nDashboards\nC.\nCatalogs\nD.\nAll of the above\nE.\nAnswer: E\n"
    },
    {
        "question": "\nA newly joined team member John Smith in the Marketing team who currently has access read access to sales tables but\ndoes not have access to delete rows from the table, which of the following commands help you accomplish this?\n",
        "choices": [
            "GRANT USAGE ON TABLE table_name TO john.smith@marketing.com",
            "GRANT DELETE ON TABLE table_name TO john.smith@marketing.com",
            "GRANT DELETE TO TABLE table_name ON john.smith@marketing.com",
            "GRANT MODIFY TO TABLE table_name ON john.smith@marketing.com",
            "GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com"
        ],
        "answer": "GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com",
        "explanation": "\nThe answer is GRANT MODIFY ON TABLE table_name TO john.smith@marketing.com\nhttps://www.dumps4less.com/\nBelow are the list privileges that can be granted to a user or a group,\n\u00b7 SELECT: gives reada access to an object.\n\u00b7 CREATE: gives ability to create an object (for example, a table in a schema).\n\u00b7 MODIFY: gives ability to add, delete, and modify data to or from an object.\n\u00b7 USAGE: does not give any abilities, but is an additional requirement to perform any action on a schema object.\n\u00b7 READ_METADATA: gives ability to view an object and its metadata.\n\u00b7 CREATE_NAMED_FUNCTION: gives ability to create a named UDF in an existing catalog or schema.\n\u00b7 MODIFY_CLASSPATH: gives ability to add files to the Spark class path.\n\u00b7 ALL PRIVILEGES: gives all privileges (is translated into all the above privileges\n"
    },
    {
        "question": "\nKevin is the owner of both the sales table and regional_sales_vw view which uses the sales table as underlying source for\nthe data, and Kevin is looking to grant select privilege on the view regional_sales_vw to one of newly joined team\nmembers Steven. Which of the following is a true statement?\n",
        "choices": [
            "Kevin can not grant access to Steven since he does not have security admin privilege",
            "Kevin although is the owner but does not have ALL PRIVILEGES permission",
            "Kevin can grant access to the view, because he is the owner of the view and the underlying table",
            "Kevin can not grant access to Steven since he does have workspace admin privilege",
            "Steve will also require SELECT access on the underlying table"
        ],
        "answer": "Kevin can grant access to the view, because he is the owner of the view and the underlying table",
        "explanation": "\nThe answer is, Kevin can grant access to the view, because he is the owner of the view and the underlying table,\nOwnership determines whether or not you can grant privileges on derived objects to other users, a user who creates a\nschema, table, view, or function becomes its owner. The owner is granted all privileges and can grant privileges to other\nusers\n"
    },
    {
        "question": "\nHow does Lakehouse replace the dependency on using Data lakes and Data warehouses in a Data and Analytics solution?\n",
        "choices": [
            "Open, direct access to data stored in standard data formats.",
            "Supports ACID transactions.",
            "Supports BI and Machine learning workloads",
            "Support for end-to-end streaming and batch workloads",
            "All the above"
        ],
        "answer": "All the above",
        "explanation": "\nLakehouse combines the benefits of a data warehouse and data lakes,\nLakehouse = Data Lake + DataWarehouse\nHere are some of the major benefits of a lakehouse\nLakehouse = Data Lake + DataWarehouse\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nYou are currently working on storing data you receive from different customer surveys this data is highly unstructured\nand changes over time, why Lakehouse is a better choice compared to a Data warehouse?\n",
        "choices": [
            "Lakehouse supports schema enforcement and evolution, traditional data warehouses lack schema evolution.",
            "Lakehouse supports SQL",
            "Lakehouse supports ACID",
            "Lakehouse enforces data integrity",
            "Lakehouse supports primary and foreign keys like a data warehouse"
        ],
        "answer": "Lakehouse supports schema enforcement and evolution, traditional data warehouses lack schema evolution.",
        "explanation": "202\nYou are currently working on storing data you receive from different customer surveys this data is highly unstructured\nand changes over time, why Lakehouse is a better choice compared to a Data warehouse?\nLakehouse supports schema enforcement and evolution, traditional data warehouses lack schema evolution.\nA.\nLakehouse supports SQL\nB.\nLakehouse supports ACID\nC.\nLakehouse enforces data integrity\nD.\nLakehouse supports primary and foreign keys like a data warehouse\nE.\nAnswer: A\n"
    },
    {
        "question": "\nWhich of the following locations hosts the driver and worker nodes of a Databricks-managed cluster?\n",
        "choices": [
            "Data plane",
            "Control plane",
            "Databricks Filesystem",
            "JDBC data source",
            "Databricks web application"
        ],
        "answer": "Data plane",
        "explanation": "\n"
    },
    {
        "question": "\nYou have written a notebook to generate a summary data set for reporting, Notebook was scheduled using the job\ncluster, but you realized it takes  minutes to start the cluster, what feature can be to start the cluster in a timely\nfashion?\n",
        "choices": [
            "Setup an additional job to run ahead of the actual job so the cluster is running second job starts",
            "Use the Databricks cluster pools feature to reduce the startup time",
            "Use Databricks Premium edition instead of Databricks standard edition",
            "Pin the cluster in the cluster UI page so it is always available to the jobs",
            "Disable auto termination so the cluster is always running"
        ],
        "answer": "Use the Databricks cluster pools feature to reduce the startup time",
        "explanation": "\nCluster pools allow us to reserve VM's ahead of time, when a new job cluster is created VM are grabbed from the pool.\nNote: when the VM's are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only\nbilled once VM is allocated to a cluster.\nHere is a demo of how to setup and follow some best practices,\nhttps://www.youtube.com/watch?v=FVtITxOabxg&ab_channel=DatabricksAcademy\n"
    },
    {
        "question": "\nWhich of the following statement is true about Databricks repos?\n",
        "choices": [
            "You can approve the pull request if you are the owner of Databricks repos",
            "A workspace can only have one instance of git integration",
            "Databricks Repos and Notebook versioning are the same features",
            "You cannot create a new branch in Databricks repos",
            "Databricks repos allow you to comment and commit code changes and push them to remote branch"
        ],
        "answer": "Databricks repos allow you to comment and commit code changes and push them to remote branch",
        "explanation": "\nSee below diagram to understand the role Databricks Repos and Git provider plays when building a CI/CD workdlow.\nAll the steps highlighted in yellow can be done Databricks Repo, all the steps highlighted in Gray are done in a git\nprovider like Github or Azure Devops\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the statement is correct about the cluster pools?\n",
        "choices": [
            "Cluster pools allow you to perform load balancing",
            "Cluster pools allow you to create a cluster",
            "Cluster pools allow you to save time when starting a new cluster",
            "Cluster pools are used to share resources among multiple teams",
            "Cluster pools allow you to have all the nodes in the cluster from single physical server rack"
        ],
        "answer": "Cluster pools allow you to save time when starting a new cluster",
        "explanation": "206\nWhich of the statement is correct about the cluster pools?\nCluster pools allow you to perform load balancing\nA.\nCluster pools allow you to create a cluster\nB.\nCluster pools allow you to save time when starting a new cluster\nC.\nCluster pools are used to share resources among multiple teams\nD.\nCluster pools allow you to have all the nodes in the cluster from single physical server rack\nE.\nAnswer: C\n"
    },
    {
        "question": "\nOnce a Custer is deleted below additional actions needs to performed by the administrator\n",
        "choices": [
            "Remove virtual machines but storage and networking are automatically dropped",
            "Drop storage disks but Virtual machines and networking are automatically dropped",
            "Remove networking but Virtual machines and storage disks are automatically dropped",
            "Remove logs",
            "No action needs to be performed. All resources are automatically removed."
        ],
        "answer": "No action needs to be performed. All resources are automatically removed.",
        "explanation": "207\nOnce a Custer is deleted below additional actions needs to performed by the administrator\nRemove virtual machines but storage and networking are automatically dropped\nA.\nDrop storage disks but Virtual machines and networking are automatically dropped\nB.\nhttps://www.dumps4less.com/\nRemove networking but Virtual machines and storage disks are automatically dropped\nC.\nRemove logs\nD.\nNo action needs to be performed. All resources are automatically removed.\nE.\nAnswer: E\n"
    },
    {
        "question": "\nHow does a Delta Lake differ from a traditional data lake?\n",
        "choices": [
            "Delta lake is Datawarehouse service on top of data lake that can provide reliability, security, and performance",
            "Delta lake is a caching layer on top of data lake that can provide reliability, security, and performance",
            "Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and",
            "Delta lake is an open storage format designed to replace flat files with additional capabilities that can provide",
            "Delta lake is proprietary software designed by Databricks that can provide reliability, security, and performance"
        ],
        "answer": "Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and",
        "explanation": "\nWhat is Delta?\nDelta lake is\n\u00b7 Open source\n\u00b7 Builds up on standard data format\n\u00b7 Optimized for cloud object storage\n\u00b7 Built for scalable metadata handling\nDelta lake is not\n\u00b7 Proprietary technology\n\u00b7 Storage format\n\u00b7 Storage medium\n\u00b7 Database service or data warehouse\n"
    },
    {
        "question": "\nDrop a DELTA table\n",
        "choices": [
            "DROP DELTA table_name",
            "DROP TABLE table_name",
            "DROP TABLE table_name FORMAT DELTA",
            "DROP table_name"
        ],
        "answer": "DROP TABLE table_name",
        "explanation": "210\nDrop a DELTA table\nDROP DELTA table_name\nA.\nDROP TABLE table_name\nB.\nDROP TABLE table_name FORMAT DELTA\nC.\nDROP table_name\nD.\nAnswer: B\n"
    },
    {
        "question": "\nDelete records from the transactions Delta table where transactionDate is greater than current timestamp?\n",
        "choices": [
            "DELETE FROM transactions FORMAT DELTA where transactionDate > currenct_timestmap()",
            "DELETE FROM transactions if transctionDate > current_timestamp()",
            "DELETE FROM transactions where transactionDate > current_timestamp()",
            "DELETE FROM transactions where transactionDate > current_timestamp() KEEP_HISTORY",
            "DELET FROM transactions where transactionDate GE current_timestamp()"
        ],
        "answer": "DELETE FROM transactions where transactionDate > current_timestamp()",
        "explanation": "211\nDelete records from the transactions Delta table where transactionDate is greater than current timestamp?\nDELETE FROM transactions FORMAT DELTA where transactionDate > currenct_timestmap()\nA.\nDELETE FROM transactions if transctionDate > current_timestamp()\nB.\nDELETE FROM transactions where transactionDate > current_timestamp()\nC.\nDELETE FROM transactions where transactionDate > current_timestamp() KEEP_HISTORY\nD.\nhttps://www.dumps4less.com/\nDELET FROM transactions where transactionDate GE current_timestamp()\nE.\nAnswer: C\n"
    },
    {
        "question": "\nIdentify one of the below statements that can query a delta table in PySpark Dataframe API\n",
        "choices": [
            "Spark.read.mode(\u201cdelta\u201d).table(\u201ctable_name)",
            "Spark.read.table.delta(\u201ctable_name)",
            "Spark.read.table(\u201ctable_name\u201d)",
            "Spark.read.format(\u201cdelta\u201d).LoadTableAs(\u201ctable_name\u201d)",
            "Spark.read.format(\u201cdelta\u201d).TableAs(\u201ctable_name\u201d)"
        ],
        "answer": "Spark.read.table(\u201ctable_name\u201d)",
        "explanation": "212\nIdentify one of the below statements that can query a delta table in PySpark Dataframe API\nSpark.read.mode(\u201cdelta\u201d).table(\u201ctable_name)\nA.\nSpark.read.table.delta(\u201ctable_name)\nB.\nSpark.read.table(\u201ctable_name\u201d)\nC.\nSpark.read.format(\u201cdelta\u201d).LoadTableAs(\u201ctable_name\u201d)\nD.\nSpark.read.format(\u201cdelta\u201d).TableAs(\u201ctable_name\u201d)\nE.\nAnswer: C\n"
    },
    {
        "question": "\nThe default threshold of VACUUM is  days, internal audit team asked to certain tables to maintain at least  days as\npart of compliance requirement, which of the below setting is needed to implement.\nALTER TABLE table_name set TBLPROPERTIES (delta.deletedFileRetentionDuration= \u2018interval  days\u2019)\nA.\nMODIFY TABLE table_name set TBLPROPERTY (delta.maxRetentionDays = \u2018interval  days\u2019)\nB.\nALTER TABLE table_name set EXENDED TBLPROPERTIES (delta.deletedFileRetentionDuration= \u2018interval  days\u2019)\nC.\nALTER TABLE table_name set EXENDED TBLPROPERTIES (delta.vaccum.duration= \u2018interval  days\u2019)\nD.\nAnswer: A\nExplanation/Reference:\nALTER TABLE table_name SET TBLPROPERTIES ( property_key [ = ] property_val [, ...] )\nTBLPROPERTIES allow you to set key-value pairs\nTable properties and table options (Databricks SQL) | Databricks on AWS\nhttps://www.dumpsless.com/\n",
        "choices": [
            "ALTER TABLE table_name set TBLPROPERTIES (delta.deletedFileRetentionDuration= \u2018interval 365 days\u2019)",
            "MODIFY TABLE table_name set TBLPROPERTY (delta.maxRetentionDays = \u2018interval 365 days\u2019)",
            "ALTER TABLE table_name set EXENDED TBLPROPERTIES (delta.deletedFileRetentionDuration= \u2018interval 365 days\u2019)",
            "ALTER TABLE table_name set EXENDED TBLPROPERTIES (delta.vaccum.duration= \u2018interval 365 days\u2019)"
        ],
        "answer": "ALTER TABLE table_name set TBLPROPERTIES (delta.deletedFileRetentionDuration= \u2018interval 365 days\u2019)",
        "explanation": "\nALTER TABLE table_name SET TBLPROPERTIES ( property_key [ = ] property_val [, ...] )\nTBLPROPERTIES allow you to set key-value pairs\nTable properties and table options (Databricks SQL) | Databricks on AWS\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the following commands can be used to read a delta table in SQL\n",
        "choices": [
            "Spark.sql(\u201cselect * from table_name\u201d)",
            "%sql Select * from table_name",
            "Both A & B",
            "Execute.sql(\u201cselect * from table\u201d)",
            "Delta.sql(\u201cselect * from table\u201d)"
        ],
        "answer": "Both A & B",
        "explanation": "214\nWhich of the following commands can be used to read a delta table in SQL\nSpark.sql(\u201cselect * from table_name\u201d)\nA.\n%sql Select * from table_name\nB.\nBoth A & B\nC.\nExecute.sql(\u201cselect * from table\u201d)\nD.\nDelta.sql(\u201cselect * from table\u201d)\nE.\nAnswer: C\n\n"
    },
    {
        "question": "\nWhich of the following SQL statements can be used to update a transactions table, to set a flag on the table from Y to N\n",
        "choices": [
            "MODIFY transactions SET active_flag = 'N' WHERE active_flag = 'Y'",
            "MERGE transactions SET active_flag = 'N' WHERE active_flag = 'Y'",
            "UPDATE transactions SET active_flag = 'N' WHERE active_flag = 'Y'",
            "REPLACE transactions SET active_flag = 'N' WHERE active_flag = 'Y'"
        ],
        "answer": "UPDATE transactions SET active_flag = 'N' WHERE active_flag = 'Y'",
        "explanation": "\nThe answer is\nUPDATE transactions SET active_flag = 'N' WHERE active_flag = 'Y'\nDelta Lake supports UPDATE statements on the delta table, all of the changes as part of the update are ACID compliant.\n\n"
    },
    {
        "question": "\nWhich of the following array functions takes input column return unique list of values in an array?\n",
        "choices": [
            "COLLECT_LIST",
            "COLLECT_SET",
            "COLLECT_UNION",
            "ARRAY_INTERSECT",
            "ARRAY_UNION"
        ],
        "answer": "COLLECT_SET",
        "explanation": "\n"
    },
    {
        "question": "\nYou are looking to process the data based on two variables, one to check if the department is supply chain or check if\nprocess flag is set to True\n",
        "choices": [
            "if department = \u201csupply chain\u201d | process:",
            "if department == \u201csupply chain\u201d or process = TRUE:",
            "if department == \u201csupply chain\u201d | process == TRUE:",
            "if department == \u201csupply chain\u201d | if process == TRUE:",
            "if department == \u201csupply chain\u201d or process:"
        ],
        "answer": "if department == \u201csupply chain\u201d or process:",
        "explanation": "222\nYou are looking to process the data based on two variables, one to check if the department is supply chain or check if\nprocess flag is set to True\nif department = \u201csupply chain\u201d | process:\nA.\nif department == \u201csupply chain\u201d or process = TRUE:\nB.\nif department == \u201csupply chain\u201d | process == TRUE:\nC.\nif department == \u201csupply chain\u201d | if process == TRUE:\nD.\nif department == \u201csupply chain\u201d or process:\nE.\nAnswer: E\n\n"
    },
    {
        "question": "\nWhich of the following python statements can be used to replace the schema name and table name in the query?\n",
        "choices": [
            "table_name = \"sales\"schema_name = \"bronze\"query = f\"select * from schema_name.table_name\"",
            "table_name = \"sales\"query = \"select * from {schema_name}.{table_name}\"",
            "table_name = \"sales\"query = f\"select * from {schema_name}.{table_name}\"",
            "table_name = \"sales\"query = f\"select * from + schema_name +\".\"+table_name\""
        ],
        "answer": "table_name = \"sales\"query = f\"select * from {schema_name}.{table_name}\"",
        "explanation": "\nThe answer is\ntable_name = \"sales\"\nquery = f\"select * from {schema_name}.{table_name}\"\nIt is always best to use f strings to replace python variables, rather than using string concatenation.\n"
    },
    {
        "question": "\nyou are currently working on creating a spark stream process to read and write in for a one-time micro batch, and also\nrewrite the existing target table, fill in the blanks to complete the below command sucesfully.\nspark.table(\"source_table\")\n.writeStream\nhttps://www.dumpsless.com/\n.option(\"____\", \u201cdbfs:/location/silver\")\n.outputMode(\"____\")\n.trigger(Once=____)\n.table(\"target_table\")\n",
        "choices": [
            "checkpointlocation, complete, True",
            "targetlocation, overwrite, True",
            "checkpointlocation, True, overwrite",
            "checkpointlocation, True, complete",
            "checkpointlocation, overwrite, True"
        ],
        "answer": "checkpointlocation, complete, True",
        "explanation": "225\nyou are currently working on creating a spark stream process to read and write in for a one-time micro batch, and also\nrewrite the existing target table, fill in the blanks to complete the below command sucesfully.\nspark.table(\"source_table\")\n.writeStream\nhttps://www.dumps4less.com/\n.option(\"____\", \u201cdbfs:/location/silver\")\n.outputMode(\"____\")\n.trigger(Once=____)\n.table(\"target_table\")\ncheckpointlocation, complete, True\nA.\ntargetlocation, overwrite, True\nB.\ncheckpointlocation, True, overwrite\nC.\ncheckpointlocation, True, complete\nD.\ncheckpointlocation, overwrite, True\nE.\nAnswer: A\n"
    },
    {
        "question": "\nWhich of the following data workloads will utilize a gold table as its source?\n",
        "choices": [
            "A job that enriches data by parsing its timestamps into a human-readable format",
            "A job that queries aggregated data that already feeds into a dashboard",
            "A job that ingests raw data from a streaming source into the Lakehouse",
            "A job that aggregates cleaned data to create standard summary statistics",
            "A job that cleans data by removing malformatted records"
        ],
        "answer": "A job that queries aggregated data that already feeds into a dashboard",
        "explanation": "\nThe answer is, A job that queries aggregated data that already feeds into a dashboard\nThe gold layer is used to store aggregated data, which are typically used for dashboards and reporting.\nReview the below link for more info\nMedallion Architecture \u2013 Databricks\nGold Layer:\n1. Powers Ml applications, reporting, dashboards, ad hoc analytics\n2. Refined views of data, typically with aggregations\n3. Reduces strain on production systems\n4. Optimizes query performance for business-critical data\nhttps://www.dumps4less.com/\n"
    },
    {
        "question": "\nWhich of the following programming languages can be used to build a Databricks SQL dashboard?\n",
        "choices": [
            "Python",
            "Scala",
            "SQL",
            "R",
            "All of the above"
        ],
        "answer": "SQL",
        "explanation": "\nThe answer is SQL\n"
    },
    {
        "question": "\nWhat are two different modes of DELTA LIVE TABLE Pipelines\n",
        "choices": [
            "Triggered, Incremental",
            "Once, Continuous",
            "Triggered, Continuous",
            "Once, Incremental",
            "Continuous, Incremental"
        ],
        "answer": "Triggered, Continuous",
        "explanation": "\nThe answer is Triggered, Continuous\nhttps://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-concepts#--\ncontinuous-and-triggered-pipelines\nTriggered pipelines update each table with whatever data is currently available and then stop the cluster running the\npipeline. Delta Live Tables automatically analyzes the dependencies between your tables and starts by computing those\nthat read from external sources. Tables within the pipeline are updated after their dependent data sources have been\nupdated.\nContinuous pipelines update tables continuously as input data changes. Once an update is started, it continues to run\nhttps://www.dumps4less.com/\nuntil manually stopped. Continuous pipelines require an always-running cluster but ensure that downstream consumers\nhave the most up-to-date data.\n"
    },
    {
        "question": "\nYou are working on a process to load external CSV files into a Delta by leveraging the COPY INTO command, but after\nrunning the command for the second time no data was loaded into the table name, why is that?\nCOPY INTO table_name\nFROM 'dbfs:/mnt/raw/*.csv'\nFILEFORMAT = CSV\n",
        "choices": [
            "COPY INTO only works one time data load",
            "Run REFRESH TABLE sales before running COPY INTO",
            "COPY INTO did not detect new files after the last load",
            "Use incremental = TRUE option to load new files",
            "COPY INTO does not support incremental load, use AUTO LOADER"
        ],
        "answer": "COPY INTO did not detect new files after the last load",
        "explanation": "\nThe answer is COPY INTO did not detect new files after the last load,\nCOPY INTO keeps track of files that were successfully loaded into the table, the next time when the COPY INTO runs it\nskips them.\nFYI, you can change this behavior by using COPY_OPTIONS 'force'= 'true', when this option is enabled all files in the\npath/pattern are loaded.\nCOPY INTO table_identifier\nFROM [ file_location | (SELECT identifier_list FROM file_location) ]\nFILEFORMAT = data_source\n[FILES = [file_name, ... | PATTERN = 'regex_pattern']\n[FORMAT_OPTIONS ('data_source_reader_option' = 'value', ...)]\n[COPY_OPTIONS 'force' = ('false'|'true')]\n"
    },
    {
        "question": "\nThe sales team has asked the Data engineering team to develop a dashboard that shows sales performance for all stores,\nbut the sales team would like to use the dashboard but would like to select individual store location, which of the\nfollowing approaches Data Engineering team can use to build this functionality into the dashboard.\n",
        "choices": [
            "Use query Parameters which then allow user to choose any location",
            "Currently dashboards do not support parameters",
            "Use Databricks REST API to create a dashboard for each location",
            "Use SQL UDF function to filter the data based on the location",
            "Use Dynamic views to filter the data based on the location"
        ],
        "answer": "Use query Parameters which then allow user to choose any location",
        "explanation": "\nThe answer is\nDatabricks many types of parameters in the dashboard, a drop-down list can be created based on a query that has a\nunique list of store locations.\nHere is a simple query that takes a parameter for\nSELECT * FROM sales WHERE field IN ( {{ Multi Select Parameter }} )\nOr\nSELECT * FROM sales WHERE field = {{ Single Select Parameter }}\nQuery parameter types\nText\nNumber\nDropdown List\nQuery Based Dropdown List\nDate and Time"
    }
]